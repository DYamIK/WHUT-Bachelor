
@article{liu_libero_2023,
	title = {{LIBERO}: benchmarking knowledge transfer for lifelong robot learning},
	volume = {36},
	shorttitle = {Libero},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/8c3c666820ea055a77726d66fc7d447f-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2025-03-28},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Bo and Zhu, Yifeng and Gao, Chongkai and Feng, Yihao and Liu, Qiang and Zhu, Yuke and Stone, Peter},
	month = dec,
	year = {2023},
	keywords = {ccfInfo: CCF-A NeurIPS, citationNumber: 5},
	pages = {44776--44791},
}

@misc{zhang_anyattack_2024,
	title = {{AnyAttack}: targeted adversarial attacks on vision-language models toward any images},
	shorttitle = {{AnyAttack}},
	url = {http://arxiv.org/abs/2410.05346},
	doi = {10.48550/arXiv.2410.05346},
	abstract = {Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. Current attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we propose AnyAttack, a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowing any image to serve as a target for the attack. Our framework employs the pre-training and fine-tuning paradigm, with the adversarial noise generator pre-trained on the large-scale LAION-400M dataset. This large-scale pre-training endows our method with powerful transferability across a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack. Additionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google Gemini, Claude Sonnet, Microsoft Copilot and OpenAI GPT. These results reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.},
	language = {en},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Zhang, Jiaming and Ye, Junhong and Ma, Xingjun and Li, Yige and Yang, Yunfan and Sang, Jitao and Yeung, Dit-Yan},
	month = dec,
	year = {2024},
	note = {arXiv:2410.05346 [cs]
TLDR: æœ¬æ–‡æå‡ºäº†AnyAttackï¼Œä¸€ç§è‡ªç›‘ç£æ¡†æ¶ï¼Œæ— éœ€æ ‡ç­¾ç›‘ç£å³å¯ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆå®šå‘å¯¹æŠ—å›¾åƒï¼Œä½¿å¾—ä»»ä½•å›¾åƒéƒ½èƒ½ä½œä¸ºæ”»å‡»ç›®æ ‡ã€‚},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, ccfInfo: Not Found, citationNumber: 0, â­â­â­â­},
}

@misc{noauthor_zchoiawesome-embodied-robotics-and-agent_nodate,
	title = {zchoi/awesome-embodied-robotics-and-agent: è¿™æ˜¯ä¸€ä¸ªå…³äºâ€œå…·èº« {AI} æˆ–æœºå™¨äººç»“åˆå¤§è¯­è¨€æ¨¡å‹â€ç ”ç©¶çš„ç²¾é€‰åˆ—è¡¨ã€‚å…³æ³¨æ­¤ä»“åº“ä»¥è·å–æœ€æ–°æ›´æ–°ï¼ğŸ”¥ --- zchoi/awesome-embodied-robotics-and-agent: this is a curated list of "embodied {AI} or robot with large language models" research. {Watch} this repository for the latest updates! ğŸ”¥},
	url = {https://github.com/zchoi/Awesome-Embodied-Robotics-and-Agent?tab=readme-ov-file#vision-language-action-model},
	language = {en},
	urldate = {2025-03-25},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
}

@misc{d35de3b7-a2f9-4351-8059-866d26cb39e9,
	title = {Exploring the adversarial vulnerabilities of vision-language-action models in robotics},
	url = {http://arxiv.org/abs/2411.13587},
	doi = {10.48550/arXiv.2411.13587},
	abstract = {Recently in robotics, Vision-Language-Action (VLA) models have emerged as a transformative approach, enabling robots to execute complex tasks by integrating visual and linguistic inputs within an end-to-end learning framework. While VLA models offer significant capabilities, they also introduce new attack surfaces, making them vulnerable to adversarial attacks. With these vulnerabilities largely unexplored, this paper systematically quantifies the robustness of VLA-based robotic systems. Recognizing the unique demands of robotic execution, our attack objectives target the inherent spatial and functional characteristics of robotic systems. In particular, we introduce two untargeted attack objectives that leverage spatial foundations to destabilize robotic actions, and a targeted attack objective that manipulates the robotic trajectory. Additionally, we design an adversarial patch generation approach that places a small, colorful patch within the camera's view, effectively executing the attack in both digital and physical environments. Our evaluation reveals a marked degradation in task success rates, with up to a 100{\textbackslash}\% reduction across a suite of simulated robotic tasks, highlighting critical security gaps in current VLA architectures. By unveiling these vulnerabilities and proposing actionable evaluation metrics, we advance both the understanding and enhancement of safety for VLA-based robotic systems, underscoring the necessity for continuously developing robust defense strategies prior to physical-world deployments.},
	language = {en},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {Wang, Taowen and Han, Cheng and Liang, James Chenhao and Yang, Wenhao and Liu, Dongfang and Zhang, Luna Xinyu and Wang, Qifan and Luo, Jiebo and Tang, Ruixiang},
	month = mar,
	year = {2025},
	note = {arXiv:2411.13587 [cs]
TLDR: This paper introduces two untargeted attack objectives that leverage spatial foundations to destabilize robotic actions, and a targeted attack objective that manipulates the robotic trajectory, and designs an adversarial patch generation approach that places a small, colorful patch within the camera's view.
titleTranslation: æ¢ç´¢æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„å¯¹æŠ—æ€§æ¼æ´
TLDR: æœ¬å·¥ä½œå¼•å…¥äº†ä¸€ç§æ— ç›®æ ‡çš„ä½ç½®æ„ŸçŸ¥æ”»å‡»ç›®æ ‡ï¼Œåˆ©ç”¨ç©ºé—´åŸºç¡€æ¥ç ´åæœºå™¨äººåŠ¨ä½œçš„ç¨³å®šæ€§ï¼Œä»¥åŠä¸€ç§æœ‰ç›®æ ‡çš„æ”»å‡»ç›®æ ‡ï¼Œç”¨äºæ“çºµæœºå™¨äººè½¨è¿¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å¯¹æŠ—æ€§è¡¥ä¸ç”Ÿæˆæ–¹æ³•ï¼Œåœ¨ç›¸æœºè§†é‡å†…æ”¾ç½®ä¸€ä¸ªå°çš„å½©è‰²è¡¥ä¸ã€‚},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, ccfInfo: Not Found, citationNumber: 0, notion, â­â­â­â­â­},
}

@inproceedings{hu_adversarial_2022,
	address = {New Orleans, LA, USA},
	title = {Adversarial texture for fooling person detectors in the physical world},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6946-3},
	url = {https://ieeexplore.ieee.org/document/9879058/},
	doi = {10.1109/CVPR52688.2022.01295},
	abstract = {Nowadays, cameras equipped with AI systems can capture and analyze images to detect people automatically. However, the AI system can make mistakes when receiving deliberately designed patterns in the real world, i.e., physical adversarial examples. Prior works have shown that it is possible to print adversarial patches on clothes to evade DNN-based person detectors. However, these adversarial examples could have catastrophic drops in the attack success rate when the viewing angle (i.e., the cameraâ€™s angle towards the object) changes. To perform a multi-angle attack, we propose Adversarial Texture (AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people wearing such clothes can hide from person detectors from different viewing angles. We propose a generative method, named Toroidal-Cropping-based Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive structures. We printed several pieces of cloth with AdvTexure and then made T-shirts, skirts, and dresses in the physical world. Experiments showed that these clothes could fool person detectors in the physical world.},
	language = {en},
	urldate = {2025-03-21},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hu, Zhanhao and Huang, Siyuan and Zhu, Xiaopei and Sun, Fuchun and Zhang, Bo and Hu, Xiaolin},
	month = jun,
	year = {2022},
	note = {TLDR: Adversarial Texture is proposed, which can cover clothes with arbitrary shapes so that people wearing such clothes can hide from person detectors from different viewing angles and a generative method, named Toroidal-Cropping-based Expandable Generative Attack (TC-EGA), is proposed to craft AdvTexture with repetitive structures.},
	keywords = {ccfInfo: CCF-A CVPR, citationNumber: 102},
	pages = {13297--13306},
}

@article{wei_physical_2024,
	title = {Physical adversarial attack meets computer vision: a decade survey},
	volume = {46},
	issn = {1939-3539},
	shorttitle = {Physical adversarial attack meets computer vision},
	url = {https://ieeexplore.ieee.org/abstract/document/10602786},
	doi = {10.1109/TPAMI.2024.3430860},
	abstract = {Despite the impressive achievements of Deep Neural Networks (DNNs) in computer vision, their vulnerability to adversarial attacks remains a critical concern. Extensive research has demonstrated that incorporating sophisticated perturbations into input images can lead to a catastrophic degradation in DNNsâ€™ performance. This perplexing phenomenon not only exists in the digital space but also in the physical world. Consequently, it becomes imperative to evaluate the security of DNNs-based systems to ensure their safe deployment in real-world scenarios, particularly in security-sensitive applications. To facilitate a profound understanding of this topic, this paper presents a comprehensive overview of physical adversarial attacks. First, we distill four general steps for launching physical adversarial attacks. Building upon this foundation, we uncover the pervasive role of artifacts carrying adversarial perturbations in the physical world. These artifacts influence each step. To denote them, we introduce a new term: adversarial medium. Then, we take the first step to systematically evaluate the performance of physical adversarial attacks, taking the adversarial medium as a first attempt. Our proposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness, Stealthiness, Robustness, Practicability, Aesthetics, and Economics. We also provide comparative results across task categories, together with insightful observations and suggestions for future research directions.},
	language = {en},
	number = {12},
	urldate = {2025-01-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wei, Hui and Tang, Hao and Jia, Xuemei and Wang, Zhixiang and Yu, Hanxun and Li, Zhubo and Satoh, Shinâ€™ichi and Van Gool, Luc and Wang, Zheng},
	month = dec,
	year = {2024},
	note = {GSCC: 0000047 
Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence
TLDR: A comprehensive overview of physical adversarial attacks is presented and a proposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness, Stealthiness, Robustness, Practicability, Aesthetics, and Economics.
titleTranslation: ç‰©ç†å¯¹æŠ—æ”»å‡»ä¸è®¡ç®—æœºè§†è§‰ï¼šåå¹´è°ƒæŸ¥
abstractTranslation: å°½ç®¡æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†ä»¤äººç©ç›®çš„æˆå°±ï¼Œä½†å®ƒä»¬å¯¹å¯¹æŠ—æ”»å‡»çš„è„†å¼±æ€§ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„å…³æ³¨ç‚¹ã€‚å¤§é‡ç ”ç©¶è¡¨æ˜ï¼Œå‘è¾“å…¥å›¾åƒä¸­æ·»åŠ å¤æ‚çš„æ‰°åŠ¨å¯èƒ½å¯¼è‡´ DNN æ€§èƒ½çš„ç¾éš¾æ€§ä¸‹é™ã€‚è¿™ä¸€ä»¤äººå›°æƒ‘çš„ç°è±¡ä¸ä»…å­˜åœ¨äºæ•°å­—ç©ºé—´ä¸­ï¼Œä¹ŸåŒæ ·å­˜åœ¨äºç‰©ç†ä¸–ç•Œä¸­ã€‚å› æ­¤ï¼Œè¯„ä¼°åŸºäº DNN çš„ç³»ç»Ÿçš„å®‰å…¨æ€§å˜å¾—è‡³å…³é‡è¦ï¼Œä»¥ç¡®ä¿å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„å®‰å…¨éƒ¨ç½²ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨æ•æ„Ÿçš„åº”ç”¨ä¸­ã€‚ä¸ºäº†æ–¹ä¾¿å¯¹è¿™ä¸€ä¸»é¢˜çš„æ·±å…¥ç†è§£ï¼Œæœ¬æ–‡æä¾›äº†ç‰©ç†å¯¹æŠ—æ”»å‡»çš„å…¨é¢æ¦‚è¿°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æç‚¼å‡ºå››ä¸ªå‘èµ·ç‰©ç†å¯¹æŠ—æ”»å‡»çš„é€šç”¨æ­¥éª¤ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ­ç¤ºäº†æºå¸¦å¯¹æŠ—æ‰°åŠ¨çš„ç‰©ç†ä¸–ç•Œä¸­ä¼ªå½±çš„æ™®éä½œç”¨ã€‚è¿™äº›ä¼ªå½±å½±å“ç€æ¯ä¸€ä¸ªæ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æœ¯è¯­ï¼šå¯¹æŠ—ä»‹è´¨ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿ˆå‡ºäº†ç³»ç»Ÿè¯„ä¼°ç‰©ç†å¯¹æŠ—æ”»å‡»æ€§èƒ½çš„ç¬¬ä¸€æ­¥ï¼Œå°†å¯¹æŠ—ä»‹è´¨ä½œä¸ºé¦–æ¬¡å°è¯•ã€‚æˆ‘ä»¬æå‡ºçš„è¯„ä¼°æŒ‡æ ‡ hiPAA ç”±å…­ä¸ªæ–¹é¢ç»„æˆï¼šæœ‰æ•ˆæ€§ã€éšè”½æ€§ã€é²æ£’æ€§ã€å®ç”¨æ€§ã€ç¾å­¦å’Œç»æµæ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†è·¨ä»»åŠ¡ç±»åˆ«çš„æ¯”è¾ƒç»“æœï¼Œä»¥åŠå¯¹æœªæ¥ç ”ç©¶æ–¹å‘çš„æ·±åˆ»è§‚å¯Ÿå’Œå»ºè®®ã€‚},
	keywords = {Adversarial attack, Biological system modeling, Computer vision, Data models, Perturbation methods, Predictive models, Surveys, Task analysis, adversarial medium, ccfInfo: Net Error: 0, citationNumber: 5, computer vision, physical world, survey, â­â­â­â­â­},
	pages = {9797--9817},
}

@inproceedings{wiyatno_physical_2019,
	address = {Seoul, Korea (South)},
	title = {Physical adversarial textures that fool visual object tracking},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-4803-8},
	url = {https://ieeexplore.ieee.org/document/9009841},
	doi = {10.1109/ICCV.2019.00492},
	abstract = {We present a method for creating inconspicuous-looking textures that, when displayed as posters in the physical world, cause visual object tracking systems to become confused. As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade. This adversarial attack evaluates several optimization strategies for fooling seldom-targeted regression models: non-targeted, targeted, and a newly-coined family of guided adversarial losses. Also, while we use the Expectation Over Transformation (EOT) algorithm to generate physical adversaries that fool tracking models when imaged under diverse conditions, we compare the impacts of different scene variables to find practical attack setups with high resulting adversarial strength and convergence speed. We further showcase that textures optimized using simulated scenes can confuse real-world tracking systems for cameras and robots.},
	language = {en},
	urldate = {2025-03-21},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wiyatno, Rey and Xu, Anqi},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504
TLDR: While the Expectation Over Transformation (EOT) algorithm is used to generate physical adversaries that fool tracking models when imaged under diverse conditions, the impacts of different scene variables are compared to find practical attack setups with high resulting adversarial strength and convergence speed.},
	keywords = {Cameras, Convergence, Object tracking, Target tracking, Task analysis, Visualization},
	pages = {4821--4830},
}

@inproceedings{wang_dual_2021,
	title = {Dual attention suppression attack: generate adversarial camouflage in physical world},
	shorttitle = {Dual attention suppression attack},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Dual_Attention_Suppression_Attack_Generate_Adversarial_Camouflage_in_Physical_World_CVPR_2021_paper.html},
	language = {en},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {Wang, Jiakai and Liu, Aishan and Yin, Zixin and Liu, Shunchang and Tang, Shiyu and Liu, Xianglong},
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ccfInfo: CCF-A CVPR, citationNumber: 114},
	pages = {8565--8574},
}

@inproceedings{duan_adversarial_2020,
	title = {Adversarial camouflage: hiding physical-world attacks with natural styles},
	shorttitle = {Adversarial camouflage},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Duan_Adversarial_Camouflage_Hiding_Physical-World_Attacks_With_Natural_Styles_CVPR_2020_paper.html},
	language = {en},
	urldate = {2025-03-21},
	author = {Duan, Ranjie and Ma, Xingjun and Wang, Yisen and Bailey, James and Qin, A. K. and Yang, Yun},
	year = {2020},
	note = {Version Number: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ccfInfo: CCF-A CVPR, citationNumber: 232, â­â­â­â­},
	pages = {1000--1008},
}

@inproceedings{suryanto_dta_2022,
	title = {{DTA}: physical camouflage attacks using differentiable transformation network},
	shorttitle = {Dta},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Suryanto_DTA_Physical_Camouflage_Attacks_Using_Differentiable_Transformation_Network_CVPR_2022_paper.html},
	language = {en},
	urldate = {2025-03-21},
	author = {Suryanto, Naufal and Kim, Yongsu and Kang, Hyoeun and Larasati, Harashta Tatimma and Yun, Youngyeo and Le, Thi-Thu-Huong and Yang, Hunmin and Oh, Se-Yoon and Kim, Howon},
	year = {2022},
	keywords = {ccfInfo: CCF-A CVPR, citationNumber: 36, â­â­â­â­},
	pages = {15305--15314},
}

@inproceedings{hu_physically_2023,
	title = {Physically realizable natural-looking clothing textures evade person detectors via {3D} modeling},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Physically_Realizable_Natural-Looking_Clothing_Textures_Evade_Person_Detectors_via_3D_CVPR_2023_paper.html},
	language = {en},
	urldate = {2025-03-21},
	author = {Hu, Zhanhao and Chu, Wenda and Zhu, Xiaopei and Zhang, Hui and Zhang, Bo and Hu, Xiaolin},
	year = {2023},
	keywords = {citationNumber: 13},
	pages = {16975--16984},
}

@inproceedings{zhou_rauca_2024,
	title = {{RAUCA}: a novel physical adversarial attack on vehicle detectors via robust and accurate camouflage generation},
	shorttitle = {Rauca},
	url = {https://proceedings.mlr.press/v235/zhou24n.html},
	abstract = {Adversarial camouflage is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance. One promising approach involves using differentiable neural renderers to facilitate adversarial camouflage optimization through gradient back-propagation. However, existing methods often struggle to capture environmental characteristics during the rendering process or produce adversarial textures that can precisely map to the target vehicle, resulting in suboptimal attack performance. Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios. To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA. The core of RAUCA is a novel neural rendering component, Neural Renderer Plus (NRP), which can accurately project vehicle textures and render images with environmental characteristics such as lighting and weather. In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the NRP to enhance the attack robustness. Experimental results on six popular object detectors show that RAUCA consistently outperforms existing methods in both simulation and real-world settings.},
	language = {en},
	urldate = {2025-03-21},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Jiawei and Lyu, Linye and He, Daojing and Li, Yu},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498
shortConferenceName: ICML},
	keywords = {ccfInfo: CCF-A ICML, citationNumber: 0},
	pages = {62076--62087},
}

@inproceedings{li_adversarial_2019,
	title = {Adversarial camera stickers: a physical camera-based attack on deep learning systems},
	shorttitle = {Adversarial camera stickers},
	url = {https://proceedings.mlr.press/v97/li19j.html},
	abstract = {Recent work has documented the susceptibility of deep learning systems to adversarial examples, but most such attacks directly manipulate the digital input to a classifier. Although a smaller line of work considers physical adversarial attacks, in all cases these involve manipulating the object of interest, e.g., putting a physical sticker on an object to misclassify it, or manufacturing an object specifically intended to be misclassified. In this work, we consider an alternative question: is it possible to fool deep classifiers, over all perceived objects of a certain type, by physically manipulating the camera itself? We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different (targeted) class. To accomplish this, we propose an iterative procedure for both updating the attack perturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is physically realizable). For example, we show that we can achieve physically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6\% of the time. This presents a new class of physically-realizable threat models to consider in the context of adversarially robust machine learning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54},
	language = {en},
	urldate = {2025-03-21},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Juncheng and Schmidt, Frank and Kolter, Zico},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498
shortConferenceName: ICML},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, ccfInfo: CCF-A ICML, citationNumber: 153},
	pages = {3896--3904},
}

@article{kim_openvla_nodate,
	title = {{OpenVLA}: {An} {Open}-{Source} {Vision}-{Language}-{Action} {Model}},
	language = {en},
	author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
	note = {GSCC: 0000264 2025-03-15T09:58:02.760Z},
	keywords = {ccfInfo: CCF-None CORR, citationNumber: 0},
}

@misc{kim_openvla_2024,
	title = {{OpenVLA}: {An} {Open}-{Source} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{OpenVLA}},
	url = {http://arxiv.org/abs/2406.09246},
	doi = {10.48550/arXiv.2406.09246},
	abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
	month = sep,
	year = {2024},
	note = {GSCC: 0000264 2025-03-15T09:47:26.723Z 
TLDR: OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations, is introduced and it is shown that it can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities.
titleTranslation: OpenVLAï¼šä¸€ç§å¼€æºçš„è§†è§‰è¯­è¨€è¡Œä¸ºæ¨¡å‹
abstractTranslation: åœ¨äº’è”ç½‘è§„æ¨¡çš„è§†è§‰è¯­è¨€æ•°æ®å’Œå„ç§æœºå™¨äººæ¼”ç¤ºçš„åŸºç¡€ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„å¤§å‹ç­–ç•¥æœ‰å¯èƒ½æ”¹å˜æˆ‘ä»¬æ•™æˆæœºå™¨äººæ–°æŠ€èƒ½çš„æ–¹å¼ï¼šæˆ‘ä»¬å¯ä»¥å¾®è°ƒè¿™ç§è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ ï¼ˆVLAï¼‰ æ¨¡å‹ï¼Œä»¥è·å¾—ç¨³å¥ã€å¯æ¨å¹¿çš„è§†è§‰è¿åŠ¨æ§åˆ¶ç­–ç•¥ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ–°è¡Œä¸ºã€‚ç„¶è€Œï¼Œåœ¨æœºå™¨äººæŠ€æœ¯ä¸­å¹¿æ³›é‡‡ç”¨ VLA ä¸€ç›´å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸º 1ï¼‰ ç°æœ‰çš„ VLA åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å°é—­çš„ï¼Œä¸å‘å…¬ä¼—å¼€æ”¾ï¼Œä»¥åŠ 2ï¼‰ ä¹‹å‰çš„å·¥ä½œæœªèƒ½æ¢ç´¢ä¸ºæ–°ä»»åŠ¡æœ‰æ•ˆå¾®è°ƒ VLA çš„æ–¹æ³•ï¼Œè¿™æ˜¯é‡‡ç”¨çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº† OpenVLAï¼Œè¿™æ˜¯ä¸€ç§ 7B å‚æ•°å¼€æº VLAï¼Œç»è¿‡ 970k å„ç§çœŸå®ä¸–ç•Œæœºå™¨äººæ¼”ç¤ºçš„è®­ç»ƒã€‚OpenVLA åŸºäº Llama 2 è¯­è¨€æ¨¡å‹æ„å»ºï¼Œå¹¶ç»“åˆäº†èåˆ DINOv2 å’Œ SigLIP çš„é¢„è®­ç»ƒç‰¹å¾çš„å¯è§†åŒ–ç¼–ç å™¨ã€‚ä½œä¸ºå¢åŠ çš„æ•°æ®å¤šæ ·æ€§å’Œæ–°æ¨¡å‹ç»„ä»¶çš„äº§ç‰©ï¼ŒOpenVLA åœ¨é€šæ‰æ“ä½œæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç»“æœï¼Œåœ¨ 29 ä¸ªä»»åŠ¡å’Œå¤šä¸ªæœºå™¨äººå®æ–½ä¾‹ä¸­ï¼Œç»å¯¹ä»»åŠ¡æˆåŠŸç‡æ¯” RT-2-X ï¼ˆ55Bï¼‰ ç­‰å°é—­æ¨¡å‹é«˜å‡º 16.5\%ï¼Œå‚æ•°å‡å°‘äº† 7 å€ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°é’ˆå¯¹æ–°è®¾ç½®å¾®è°ƒ OpenVLAï¼Œåœ¨æ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å¤šä»»åŠ¡ç¯å¢ƒä¸­å…·æœ‰ç‰¹åˆ«å¼ºçš„æ³›åŒ–ç»“æœå’Œå¼ºå¤§çš„è¯­è¨€åŸºç¡€èƒ½åŠ›ï¼Œå¹¶ä¸”æ¯” Diffusion Policy ç­‰ä»å¤´å¼€å§‹çš„è¡¨è¾¾æ¨¡ä»¿å­¦ä¹ æ–¹æ³•é«˜å‡º 20.4\%ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è®¡ç®—æ•ˆç‡;ä½œä¸ºå•ç‹¬çš„è´¡çŒ®ï¼Œæˆ‘ä»¬è¡¨æ˜ OpenVLA å¯ä»¥é€šè¿‡ç°ä»£ä½ç§©è‡ªé€‚åº”æ–¹æ³•åœ¨æ¶ˆè´¹ç±» GPU ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡é‡åŒ–é«˜æ•ˆæœåŠ¡ï¼Œè€Œä¸ä¼šå½±å“ä¸‹æ¸¸æˆåŠŸç‡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹æ£€æŸ¥ç‚¹ã€å¾®è°ƒç¬”è®°æœ¬å’Œå†…ç½®æ”¯æŒåœ¨ Open X-Embodiment æ•°æ®é›†ä¸Šå¤§è§„æ¨¡è®­ç»ƒ VLA çš„ PyTorch ä»£ç åº“ã€‚},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, ccfInfo: CCF-None CORR, citationNumber: Error: 500, notion, â­â­â­â­},
}

@article{chi_visuomotor_nodate,
	title = {Visuomotor policy learning via action diffusion},
	abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robotâ€™s visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.},
	language = {en},
	author = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
	keywords = {ccfInfo: Not Found, citationNumber: Error: 500},
}

@misc{zhen_3d-vla_2024,
	title = {{3D}-{VLA}: a {3D} vision-language-action generative world model},
	shorttitle = {3d-vla},
	url = {http://arxiv.org/abs/2403.09631},
	doi = {10.48550/arXiv.2403.09631},
	abstract = {Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09631 [cs]
TLDR: 3D-VLA is proposed by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model and significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments.},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Robotics (cs.RO), ccfInfo: Net Error: 0, citationNumber: 3, citationNumber: Error: 500},
}

@misc{li_towards_2024,
	title = {Towards generalist robot policies: what matters in building vision-language-action models},
	shorttitle = {Towards generalist robot policies},
	url = {http://arxiv.org/abs/2412.14058},
	doi = {10.48550/arXiv.2412.14058},
	abstract = {Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Li, Xinghang and Li, Peiyan and Liu, Minghuan and Wang, Dong and Liu, Jirong and Kang, Bingyi and Ma, Xiao and Kong, Tao and Zhang, Hanbo and Liu, Huaping},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14058 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, citationNumber: Error: 500},
}

@misc{tschannen_siglip_2025,
	title = {{SigLIP} 2: multilingual vision-language encoders with improved semantic understanding, localization, and dense features},
	shorttitle = {{SigLIP} 2},
	url = {http://arxiv.org/abs/2502.14786},
	doi = {10.48550/arXiv.2502.14786},
	abstract = {We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Tschannen, Michael and Gritsenko, Alexey and Wang, Xiao and Naeem, Muhammad Ferjad and Alabdulmohsin, Ibrahim and Parthasarathy, Nikhil and Evans, Talfan and Beyer, Lucas and Xia, Ye and Mustafa, Basil and HÃ©naff, Olivier and Harmsen, Jeremiah and Steiner, Andreas and Zhai, Xiaohua},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14786 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, citationNumber: Error: 500},
}

@misc{oquab_dinov2_2024,
	title = {{DINOv2}: learning robust visual features without supervision},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, TimothÃ©e and Moutakanni, ThÃ©o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, HervÃ© and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]
TLDR: This work revisits existing approaches and combines different techniques to scale the pretraining in terms of data and model size, and proposes an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, citationNumber: Error: 500},
}

@misc{touvron_llama_2023,
	title = {Llama 2: open foundation and fine-tuned chat models},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, citationNumber: Error: 500},
}

@article{zhang_modality-invariant_2022,
	title = {Modality-invariant asymmetric networks for cross-modal hashing},
	volume = {35},
	url = {https://ieeexplore.ieee.org/abstract/document/9689994/},
	language = {en},
	number = {5},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Zheng and Luo, Haoyang and Zhu, Lei and Lu, Guangming and Shen, Heng Tao},
	year = {2022},
	note = {Publisher: IEEE},
	keywords = {ccfInfo: CCF-A  TKDE, citationNumber: 52},
	pages = {5091--5104},
}

@inproceedings{tan_bit-aware_2022,
	address = {Madrid Spain},
	title = {Bit-aware semantic transformer hashing for multi-modal retrieval},
	isbn = {978-1-4503-8732-3},
	url = {https://dl.acm.org/doi/10.1145/3477495.3531947},
	doi = {10.1145/3477495.3531947},
	abstract = {Multi-modal hashing learns binary hash codes with extremely low storage cost and high retrieval speed. It can support efficient multimodal retrieval well. However, most existing methods still suffer from three important problems: 1) Limited semantic representation capability with shallow learning. 2) Mandatory feature-level multimodal fusion ignores heterogeneous multi-modal semantic gaps. 3) Direct coarse pairwise semantic preserving cannot effectively capture the fine-grained semantic correlations. For solving these problems, in this paper, we propose a Bit-aware Semantic Transformer Hashing (BSTH) framework to excavate bit-wise semantic concepts and simultaneously align the heterogeneous modalities for multi-modal hash learning on the concept-level. Specifically, the bitwise implicit semantic concepts are learned with the transformer in a self-attention manner, which can achieve implicit semantic alignment on the fine-grained concept-level and reduce the heterogeneous modality gaps. Then, the concept-level multi-modal fusion is performed to enhance the semantic representation capability of each implicit concept and the fused concept representations are further encoded to the corresponding hash bits via bit-wise hash functions. Further, to supervise the bit-aware transformer module, a label prototype learning module is developed to learn prototype embeddings for all categories that capture the explicit semantic correlations on the category-level by considering the co-occurrence priors. Experiments on three widely tested multi-modal retrieval datasets demonstrate the superiority of the proposed method from various aspects.},
	language = {en},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the 45th {International} {ACM} {Sigir} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Tan, Wentao and Zhu, Lei and Guan, Weili and Li, Jingjing and Cheng, Zhiyong},
	month = jul,
	year = {2022},
	note = {GSCC: 0000030 2025-02-27T12:40:00.602Z 
TLDR: A Bit-aware Semantic Transformer Hashing (BSTH) framework to excavate bit-wise semantic concepts and simultaneously align the heterogeneous modalities for multi-modal hash learning on the concept-level is proposed.},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 14},
	pages = {982--991},
}

@inproceedings{xu_mmpoi_2024,
	address = {Singapore Singapore},
	title = {{MMPOI}: a multi-modal content-aware framework for {POI} recommendations},
	isbn = {979-8-4007-0171-9},
	shorttitle = {Mmpoi},
	url = {https://dl.acm.org/doi/10.1145/3589334.3645449},
	doi = {10.1145/3589334.3645449},
	abstract = {The Point-of-Interest (POI) recommendation system, designed to recommend potential future visits of users based on their checkin sequences, faces the challenge of data scarcity. This challenge primarily stems from the data sparsity issue, namely users interact with only a small number of POIs. Most existing studies attempt to solve this problem by focusing on POI check-in sequences, without considering the substantial multi-modal content information (e.g. textual and image data) commonly associated with POIs. In this paper, we propose a novel multi-modal content-aware framework for POI recommendation (MMPOI). Our approach addresses the issue of data sparsity by incorporating multi-modal content information about POIs from a new perspective. Specifically, MMPOI leverages pre-trained models for inter-modal conversion and employs a unified pre-trained model to extract modal-specific features from each modality, effectively bridging the semantic gap between different modalities. We propose to build a Multi-Modal Trajectory Flow Graph (MTFG) which combines the multi-modal semantic structure with check-in sequences. Moreover, we design an adaptive multi-task Transformer that models usersâ€™ multi-modal movement patterns and integrates them for the next POI recommendation tasks. Extensive experiments on four real-world datasets demonstrate that MMPOI outperforms state-of-the-art POI recommendation methods. To facilitate reproducibility, we have released both the code and the multi-modal POI recommendation datasets we collect1.},
	language = {en},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {ACM},
	author = {Xu, Yang and Cong, Gao and Zhu, Lei and Cui, Lizhen},
	month = may,
	year = {2024},
	note = {GSCC: 0000004 2025-02-27T12:39:19.453Z 
TLDR: This paper proposes a novel multi-modal content-aware framework for POI recommendation (MMPOI), which leverages pre-trained models for inter-modal conversion and employs a unified pre-trained model to extract modal-specific features from each modality, effectively bridging the semantic gap between different modalities.},
	keywords = {ccfInfo: CCF-A WWW, ccfInfo: Net Error: 0, citationNumber: 0},
	pages = {3454--3463},
}

@article{an_cognitive_2022,
	title = {Cognitive multi-modal consistent hashing with flexible semantic transformation},
	volume = {59},
	url = {https://doi.org/10.1016/j.ipm.2021.102743},
	doi = {10.1016/J.IPM.2021.102743},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {Inf. Process. Manag.},
	author = {An, Junfeng and Luo, Haoyang and Zhang, Zheng and Zhu, Lei and Lu, Guangming},
	year = {2022},
	note = {GSCC: 0000020 2025-02-27T12:40:08.450Z
TLDR: This article proposes a discriminative multi-modal hashing framework named Cognitive Multi-Modal Consistent Hashing (CMCH), which can progressively pursue the structure consensus over heterogeneous multi- modal data and simultaneously explore the informative transformed semantics, and constructs a parameter-free collaborative multi- Modal fusion module to incorporate and excavate the underlying common components from multi-source data.},
	pages = {102743},
}

@article{peng_binary_2022,
	title = {Binary multi-modal matrix factorization for fast item cold-start recommendation},
	volume = {507},
	url = {https://doi.org/10.1016/j.neucom.2022.08.013},
	doi = {10.1016/J.NEUCOM.2022.08.013},
	language = {en},
	urldate = {2025-02-27},
	journal = {Neurocomputing},
	author = {Peng, Chengmei and Zhu, Lei and Xu, Yang and Li, Yaping and Guo, Lei},
	year = {2022},
	note = {GSCC: 0000010 2025-02-27T12:42:16.530Z
TLDR: This paper proposes an efï¬cient consensus multi-modal mapping to transform the heterogeneous multi-modal features to the uniï¬ed factors by exploiting the complementarity of multiple modalities, and develops an effective Discrete Coordinate Descent approach to tackle the formulated discrete hash optimization problem directly.},
	pages = {145--156},
}

@article{huang_who_2022,
	title = {Who is gambling? {Finding} cryptocurrency gamblers using multi-modal retrieval methods},
	volume = {11},
	issn = {2192-6611, 2192-662X},
	shorttitle = {Who is gambling?},
	url = {https://doi.org/10.1007/s13735-022-00264-3},
	doi = {10.1007/S13735-022-00264-3},
	language = {en},
	number = {4},
	urldate = {2025-02-27},
	journal = {Int. J. Multim. Inf. Retr.},
	author = {Huang, Zhengjie and Liu, Zhenguang and Chen, Jianhai and He, Qinming and Wu, Shuang and Zhu, Lei and Wang, Meng},
	year = {2022},
	note = {GSCC: 0000007 2025-02-27T12:42:32.903Z 
TLDR: A tool to automatically detect the smart contracts and addresses involved in gambling by scrutinizing the smart contract code and address transaction records is proposed, and a novel LightGBM model with memory components, which possesses the ability to learn from its own misclassifications, is presented.},
	pages = {539--551},
}

@inproceedings{su_task-adversarial_2023,
	address = {Ottawa ON Canada},
	title = {Task-adversarial adaptation for multi-modal recommendation},
	isbn = {979-8-4007-0108-5},
	url = {https://dl.acm.org/doi/10.1145/3581783.3612391},
	doi = {10.1145/3581783.3612391},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}, {Mm} 2023, {Ottawa}, {On}, {Canada}, 29 {October} 2023- 3 {November} 2023},
	publisher = {ACM},
	author = {Su, Hongzu and Li, Jingjing and Li, Fengling and Zhu, Lei and Lu, Ke and Yang, Yang},
	editor = {El-Saddik, Abdulmotaleb and Mei, Tao and Cucchiara, Rita and Bertini, Marco and Vallejo, Diana Patricia Tobon and Atrey, Pradeep K. and Hossain, M. Shamim},
	year = {2023},
	note = {GSCC: 0000000 2025-02-27T12:39:37.023Z 
TLDR: A Task-Adversarial Adaptation (TAA) framework, which is able to align data distributions and reduce resource consumption at the same time, and formulate the proposed approach as a plug-and-play module to accelerate the model training and improve the performance of mainstream multi-modal multi-task recommendation systems.},
	keywords = {ccfInfo: CCF-A ACM MM, citationNumber: 0},
	pages = {6530--6538},
}

@inproceedings{qiao_mutual-enhanced_2023,
	title = {Mutual-enhanced incongruity learning network for multi-modal sarcasm detection},
	volume = {37},
	url = {https://doi.org/10.1609/aaai.v37i8.26138},
	doi = {10.1609/AAAI.V37I8.26138},
	language = {en},
	urldate = {2025-02-27},
	booktitle = {Thirty-seventh {Aaai} {Conference} on {Artificial} {Intelligence}, {Aaai} 2023, {Thirty}-fifth {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence}, {Iaai} 2023, {Thirteenth} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {Eaai} 2023, {Washington}, {Dc}, {Usa}, {February} 7-14, 2023},
	publisher = {AAAI Press},
	author = {Qiao, Yang and Jing, Liqiang and Song, Xuemeng and Chen, Xiaolin and Zhu, Lei and Nie, Liqiang},
	editor = {Williams, Brian and Chen, Yiling and Neville, Jennifer},
	year = {2023},
	note = {GSCC: 0000040 2025-02-27T12:39:53.748Z 
TLDR: A mutual enhancement module is introduced to take advantage of the underlying consistency between the two modules to boost the performance of the Mutual-enhanced Incongruity Learning Network for multi-modal sarcasm detection, named MILNet.},
	keywords = {ccfInfo: CCF-A AAAI, citationNumber: 19},
	pages = {9507--9515},
}

@article{tan_partial_2023,
	title = {Partial multi-modal hashing via neighbor-aware completion learning},
	volume = {25},
	url = {https://ieeexplore.ieee.org/abstract/document/10021860/},
	doi = {10.1109/TMM.2023.3238308},
	language = {en},
	urldate = {2025-03-05},
	journal = {IEEE Trans. Multim.},
	author = {Tan, Wentao and Zhu, Lei and Li, Jingjing and Zhang, Zheng and Zhang, Huaxiang},
	year = {2023},
	note = {GSCC: 0000019 2025-02-27T12:39:34.781Z 
TLDR: This work proposes a flexible deep partial multi-modal hash learning framework, named Neighbor-aware Completion Hashing (NCH), which jointly performs the cross- modal completion learning for incomplete multi-Modal data and the multi- Modality-missing hash codes.},
	pages = {8499--8510},
}

@article{zheng_one_2023,
	title = {One for more: structured multi-modal hashing for multiple multimedia retrieval tasks},
	volume = {233},
	shorttitle = {One for more},
	url = {https://doi.org/10.1016/j.eswa.2023.120913},
	doi = {10.1016/J.ESWA.2023.120913},
	language = {en},
	urldate = {2025-02-27},
	journal = {Expert Syst. Appl.},
	author = {Zheng, Chaoqun and Li, Fengling and Zhu, Lei and Zhang, Zheng and Lu, Wenpeng},
	year = {2023},
	note = {GSCC: 0000006 2025-02-27T12:42:39.194Z},
	keywords = {ccfInfo: CCF-C ESWA, citationNumber: 1},
	pages = {120913},
}

@article{zhu_multi-modal_2024,
	title = {Multi-modal hashing for efficient multimedia retrieval: a survey},
	volume = {36},
	shorttitle = {Multi-modal hashing for efficient multimedia retrieval},
	url = {https://ieeexplore.ieee.org/abstract/document/10144360/},
	doi = {10.1109/TKDE.2023.3282921},
	language = {en},
	number = {1},
	urldate = {2025-03-05},
	journal = {IEEE Trans. Knowl. Data Eng.},
	author = {Zhu, Lei and Zheng, Chaoqun and Guan, Weili and Li, Jingjing and Yang, Yang and Shen, Heng Tao},
	year = {2024},
	note = {GSCC: 0000065 2025-02-27T12:39:02.242Z 
TLDR: This paper systematically review the existing learning to hash methods for efficient multimedia retrieval, categorizing them according to the multimedia retrieval tasks, the specific multi-modal semantic modeling techniques, and hash learning strategies, and presents the performance comparison results.},
	keywords = {ccfInfo: CCF-A  TKDE, ccfInfo: Net Error: 0, citationNumber: 16},
	pages = {239--260},
}

@article{zheng_lcemh_2024,
	title = {{LCEMH}: label correlation enhanced multi-modal hashing for efficient multi-modal retrieval},
	volume = {659},
	shorttitle = {Lcemh},
	url = {https://doi.org/10.1016/j.ins.2023.120064},
	doi = {10.1016/J.INS.2023.120064},
	language = {en},
	urldate = {2025-02-27},
	journal = {Inf. Sci.},
	author = {Zheng, Chaoqun and Zhu, Lei and Zhang, Zheng and Duan, Wenjun and Lu, Wenpeng},
	year = {2024},
	note = {GSCC: 0000005 2025-02-27T12:39:11.594Z},
	keywords = {ccfInfo: CCF-B, citationNumber: 1},
	pages = {120064},
}

@article{zhu_efficient_2022,
	title = {Efficient multi-modal hashing with online query adaption for multimedia retrieval},
	volume = {40},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/3477180},
	doi = {10.1145/3477180},
	abstract = {Multi-modal hashing supports efficient multimedia retrieval well. However, existing methods still suffer from two problems: (1) Fixed multi-modal fusion. They collaborate the multi-modal features with fixed weights for hash learning, which cannot adaptively capture the variations of online streaming multimedia contents. (2) Binary optimization challenge. To generate binary hash codes, existing methods adopt either two-step relaxed optimization that causes significant quantization errors or direct discrete optimization that consumes considerable computation and storage cost. To address these problems, we first propose a Supervised Multi-modal Hashing with Online Query-adaption method. A self-weighted fusion strategy is designed to adaptively preserve the multi-modal features into hash codes by exploiting their complementarity. Besides, the hash codes are efficiently learned with the supervision of pair-wise semantic labels to enhance their discriminative capability while avoiding the challenging symmetric similarity matrix factorization. Further, we propose an efficient Unsupervised Multi-modal Hashing with Online Query-adaption method with an adaptive multi-modal quantization strategy. The hash codes are directly learned without the reliance on the specific objective formulations. Finally, in both methods, we design a parameter-free online hashing module to adaptively capture query variations at the online retrieval stage. Experiments validate the superiority of our proposed methods.},
	language = {en},
	number = {2},
	urldate = {2025-02-27},
	journal = {ACM Transactions on Information Systems},
	author = {Zhu, Lei and Zheng, Chaoqun and Lu, Xu and Cheng, Zhiyong and Nie, Liqiang and Zhang, Huaxiang},
	month = apr,
	year = {2022},
	note = {TLDR: A Supervised Multi-modal Hashing with Online Query-adaption method designed to adaptively preserve the multi-modal features into hash codes by exploiting their complementarity and a parameter-free online hashing module to adaptively capture query variations at the online retrieval stage is proposed.},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 13},
	pages = {1--36},
}

@article{cui_online_2024,
	title = {Online query expansion hashing for efficient image retrieval},
	volume = {34},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1051-8215, 1558-2205},
	url = {https://ieeexplore.ieee.org/document/10185594/},
	doi = {10.1109/TCSVT.2023.3296412},
	abstract = {Unsupervised hashing has the desirable advantages of label independence, high storage, and retrieval efficiency, which is suitable for scalable image retrieval. Most existing methods focus on enhancing the image hashing model training process at the offline stage. However, little attention has been paid to the query content analysis by them at the online retrieval stage. They still suffer from important query semantic shortages, and thus limit the online retrieval performance, which is the ultimate objective of the image retrieval system. In this paper, we propose an Online Query Expansion Hashing (OQEH) for efficient image retrieval, by adaptively enhancing the discriminative capability of query hash codes in an expansion manner at the online retrieval stage. Specifically, we first design a self-expansion network to learn semantically invariant feature representations from images and their visual augmentations. Then, we conduct neighborhoodexpansion to search similar samples for each image from a query expansion set with the semantically invariant features and design a Transformer architecture to adaptively transfer the semantics of neighbor samples to their corresponding images. With the support of semantically invariant features, query expansion set, and adaptive semantic transfer, the representation capability of query hash codes can be enhanced at the online retrieval stage. Experimental results demonstrate that the proposed OQEH method achieves superior retrieval accuracy and comparable retrieval efficiency compared with the state-of-the-art methods. Particularly, on MS COCO dataset, OQEH can obtain about 6\% performance improvement compared with the state-of-theart results. The source codes of our method are available at: https://github.com/christinecui/OQEH.},
	language = {en},
	number = {3},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Cui, Hui and Li, Fengling and Zhu, Lei and Li, Jingjing and Zhang, Zheng},
	month = mar,
	year = {2024},
	note = {TLDR: This paper designs a self-expansion network to learn semantically invariant feature representations from images and their visual augmentations and designs a Transformer architecture to adaptively transfer the semantics of neighbor samples to their corresponding images.},
	keywords = {ccfInfo: CCF-B TCSVT, ccfInfo: Net Error: 0, citationNumber: 0},
	pages = {1941--1953},
}

@inproceedings{lu_online_2019,
	title = {Online multi-modal hashing with dynamic query-adaption},
	doi = {10.1145/3331184.3331217},
	language = {en},
	booktitle = {Proceedings of the 42nd {International} {ACM} {Sigir} {Conference} on {Research} and {Development} in {Information} {Retrieval}, {Sigir} 2019, {Paris}, {France}, {July} 21-25, 2019},
	publisher = {ACM},
	author = {Lu, Xu and Zhu, Lei and Cheng, Zhiyong and Nie, Liqiang and Zhang, Huaxiang},
	editor = {Piwowarski, Benjamin and Chevalier, Max and Gaussier, Ã‰ric and Maarek, Yoelle and Nie, Jian-Yun and Scholer, Falk},
	year = {2019},
	note = {TLDR: A self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity, while avoiding the challenging symmetric similarity matrix factorization.},
	pages = {715--724},
}

@article{wang_targeted_2023,
	title = {Targeted adversarial attack against deep cross-modal hashing retrieval},
	volume = {33},
	url = {https://ieeexplore.ieee.org/abstract/document/10090409/},
	doi = {10.1109/TCSVT.2023.3263054},
	language = {en},
	number = {10},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Wang, Tianshi and Zhu, Lei and Zhang, Zheng and Zhang, Huaxiang and Han, Junwei},
	year = {2023},
	note = {Publisher: IEEE},
	keywords = {ccfInfo: CCF-B TCSVT, citationNumber: 0},
	pages = {6159--6172},
}

@article{zhu_efficient_2023,
	title = {Efficient query-based black-box attack against cross-modal hashing retrieval},
	volume = {41},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/3559758},
	doi = {10.1145/3559758},
	abstract = {Deep cross-modal hashing retrieval models inherit the vulnerability of deep neural networks. They are vulnerable to adversarial attacks, especially for the form of subtle perturbations to the inputs. Although many adversarial attack methods have been proposed to handle the robustness of hashing retrieval models, they still suffer from two problems: (1) Most of them are based on the white-box settings, which is usually unrealistic in practical application. (2) Iterative optimization for the generation of adversarial examples in them results in heavy computation. To address these problems, we propose an Efficient Query-based Black-Box Attack (EQB
              2
              A) against deep cross-modal hashing retrieval, which can efficiently generate adversarial examples for the black-box attack. Specifically, by sending a few query requests to the attacked retrieval system, the cross-modal retrieval model stealing is performed based on the neighbor relationship between the retrieved results and the query, thus obtaining the knockoffs to substitute the attacked system. A multi-modal knockoffs-driven adversarial generation is proposed to achieve efficient adversarial example generation. While the entire network training converges, EQB
              2
              A can efficiently generate adversarial examples by forward-propagation with only given benign images. Experiments show that EQB
              2
              A achieves superior attacking performance under the black-box setting.},
	language = {en},
	number = {3},
	urldate = {2025-02-27},
	journal = {ACM Transactions on Information Systems},
	author = {Zhu, Lei and Wang, Tianshi and Li, Jingjing and Zhang, Zheng and Shen, Jialie and Wang, Xinhua},
	month = jul,
	year = {2023},
	note = {TLDR: A multi-modal knockoffs-driven adversarial generation is proposed to achieve efficient adversarial example generation and experiments show that EQB2A achieves superior attacking performance under the black-box setting.},
	keywords = {ccfInfo: CCF-A  TOIS, citationNumber: 10},
	pages = {1--25},
}

@article{wang_invisible_2024,
	title = {Invisible {Black}-{Box} {Backdoor} {Attack} against {Deep} {Cross}-{Modal} {Hashing} {Retrieval}},
	volume = {42},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/3650205},
	doi = {10.1145/3650205},
	abstract = {Deep cross-modal hashing has promoted the field of multi-modal retrieval due to its excellent efficiency and storage, but its vulnerability to backdoor attacks is rarely studied. Notably, current deep cross-modal hashing methods inevitably require large-scale training data, resulting in poisoned samples with imperceptible triggers that can easily be camouflaged into the training data to bury backdoors in the victim model. Nevertheless, existing backdoor attacks focus on the uni-modal vision domain, while the multi-modal gap and hash quantization weaken their attack performance. In addressing the aforementioned challenges, we undertake an invisible black-box backdoor attack against deep cross-modal hashing retrieval in this article. To the best of our knowledge, this is the first attempt in this research field. Specifically, we develop a flexible trigger generator to generate the attackerâ€™s specified triggers, which learns the sample semantics of the non-poisoned modality to bridge the cross-modal attack gap. Then, we devise an input-aware injection network, which embeds the generated triggers into benign samples in the form of sample-specific stealth and realizes cross-modal semantic interaction between triggers and poisoned samples. Owing to the knowledge-agnostic of victim models, we enable any cross-modal hashing knockoff to facilitate the black-box backdoor attack and alleviate the attack weakening of hash quantization. Moreover, we propose a confusing perturbation and mask strategy to induce the high-performance victim models to focus on imperceptible triggers in poisoned samples. Extensive experiments on benchmark datasets demonstrate that our method has a state-of-the-art attack performance against deep cross-modal hashing retrieval. Besides, we investigate the influences of transferable attacks, few-shot poisoning, multi-modal poisoning, perceptibility, and potential defenses on backdoor attacks. Our codes and datasets are available at https://github.com/tswang0116/IB3A.},
	language = {en},
	number = {4},
	urldate = {2025-02-27},
	journal = {ACM Transactions on Information Systems},
	author = {Wang, Tianshi and Li, Fengling and Zhu, Lei and Li, Jingjing and Zhang, Zheng and Shen, Heng Tao},
	month = jul,
	year = {2024},
	note = {TLDR: A flexible trigger generator to generate the attackerâ€™s specified triggers, which learns the sample semantics of the non-poisoned modality to bridge the cross-modal attack gap, and an input-aware injection network, which embeds the generated triggers into benign samples in the form of sample-specific stealth and realizes cross-modal semantic interaction between triggers and poisoned samples.},
	keywords = {ccfInfo: CCF-A  TOIS, citationNumber: 0},
	pages = {1--27},
}

@misc{wu_unleashing_2023,
	title = {Unleashing large-scale video generative pre-training for visual robot manipulation},
	url = {http://arxiv.org/abs/2312.13139},
	doi = {10.48550/arXiv.2312.13139},
	abstract = {Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9\% to 94.9\%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3\% to 85.4\%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Wu, Hongtao and Jing, Ya and Cheang, Chilam and Chen, Guangzeng and Xu, Jiafeng and Li, Xinghang and Liu, Minghuan and Li, Hang and Kong, Tao},
	month = dec,
	year = {2023},
	note = {arXiv:2312.13139 [cs]
TLDR: The introduction of GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation, which outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{karamcheti_language-driven_2023,
	title = {Language-driven representation learning for robotics},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2302.12766},
	doi = {10.48550/ARXIV.2302.12766},
	abstract = {Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems \${\textbackslash}unicode\{x2013\}\$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.},
	language = {en},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Karamcheti, Siddharth and Nair, Suraj and Chen, Annie S. and Kollar, Thomas and Finn, Chelsea and Sadigh, Dorsa and Liang, Percy},
	year = {2023},
	note = {Version Number: 1
TLDR: Voltron is introduced, a framework for language-driven representation learning from human videos and associated captions that outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Robotics (cs.RO), ccfInfo: CCF-None RSS, ccfInfo: Net Error: 0, citationNumber: 112, citationNumber: Error: 500},
}

@inproceedings{khandelwal_simple_2022,
	title = {Simple but effective: {CLIP} embeddings for embodied {AI}},
	shorttitle = {Simple but effective},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.html},
	language = {en},
	urldate = {2025-03-15},
	author = {Khandelwal, Apoorv and Weihs, Luca and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
	year = {2022},
	keywords = {ccfInfo: CCF-A CVPR, citationNumber: Error: 500},
	pages = {14829--14838},
}

@misc{belkhale_hydra_2023,
	title = {{HYDRA}: hybrid robot actions for imitation learning},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Hydra},
	url = {https://arxiv.org/abs/2306.17237},
	doi = {10.48550/ARXIV.2306.17237},
	abstract = {Imitation Learning (IL) is a sample efficient paradigm for robot learning using expert demonstrations. However, policies learned through IL suffer from state distribution shift at test time, due to compounding errors in action prediction which lead to previously unseen states. Choosing an action representation for the policy that minimizes this distribution shift is critical in imitation learning. Prior work propose using temporal action abstractions to reduce compounding errors, but they often sacrifice policy dexterity or require domain-specific knowledge. To address these trade-offs, we introduce HYDRA, a method that leverages a hybrid action space with two levels of action abstractions: sparse high-level waypoints and dense low-level actions. HYDRA dynamically switches between action abstractions at test time to enable both coarse and fine-grained control of a robot. In addition, HYDRA employs action relabeling to increase the consistency of actions in the dataset, further reducing distribution shift. HYDRA outperforms prior imitation learning methods by 30-40\% on seven challenging simulation and real world environments, involving long-horizon tasks in the real world like making coffee and toasting bread. Videos are found on our website: https://tinyurl.com/3mc6793z},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
	year = {2023},
	note = {Version Number: 2
TLDR: HYDRA is introduced, a method that leverages a hybrid action space with two levels of action abstractions: sparse high-level waypoints and dense low-level actions that outperforms prior imitation learning methods by 30-40\% on seven challenging simulation and real world environments.},
	keywords = {FOS: Computer and information sciences, Robotics (cs.RO), ccfInfo: CCF-None CORL, citationNumber: Error: 500},
}

@misc{liu_rdt-1b_2025,
	title = {{RDT}-{1B}: a diffusion foundation model for bimanual manipulation},
	shorttitle = {{RDT}-{1B}},
	url = {http://arxiv.org/abs/2410.07864},
	doi = {10.48550/arXiv.2410.07864},
	abstract = {Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1{\textasciitilde}5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun},
	month = mar,
	year = {2025},
	note = {arXiv:2410.07864 [cs]
TLDR: å¼•å…¥äº†ä¸€ç§ç‰©ç†å¯è§£é‡Šçš„ç»Ÿä¸€åŠ¨ä½œç©ºé—´ï¼Œå®ƒèƒ½å¤Ÿç»Ÿä¸€å„ç§æœºå™¨äººçš„åŠ¨ä½œè¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™åŸå§‹åŠ¨ä½œçš„ç‰©ç†æ„ä¹‰ï¼Œä»è€Œä¿ƒè¿›å¯è¿ç§»ç‰©ç†çŸ¥è¯†çš„å­¦ä¹ ã€‚},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, ccfInfo: Net Error: 0, citationNumber: 0, å…·èº«æ™ºèƒ½, æ™ºèƒ½å†³ç­–, è‡ªä¸»æ— äººç³»ç»Ÿ},
}

@misc{khazatsky_droid_2024,
	title = {{DROID}: a large-scale {In}-the-wild robot manipulation dataset},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Droid},
	url = {https://arxiv.org/abs/2403.12945},
	doi = {10.48550/ARXIV.2403.12945},
	abstract = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and Fagan, Peter David and Hejna, Joey and Itkina, Masha and Lepert, Marion and Ma, Yecheng Jason and Miller, Patrick Tree and Wu, Jimmy and Belkhale, Suneel and Dass, Shivin and Ha, Huy and Jain, Arhan and Lee, Abraham and Lee, Youngwoon and Memmel, Marius and Park, Sungjae and Radosavovic, Ilija and Wang, Kaiyuan and Zhan, Albert and Black, Kevin and Chi, Cheng and Hatch, Kyle Beltran and Lin, Shan and Lu, Jingpei and Mercat, Jean and Rehman, Abdul and Sanketi, Pannag R and Sharma, Archit and Simpson, Cody and Vuong, Quan and Walke, Homer Rich and Wulfe, Blake and Xiao, Ted and Yang, Jonathan Heewon and Yavary, Arefeh and Zhao, Tony Z. and Agia, Christopher and Baijal, Rohan and Castro, Mateo Guaman and Chen, Daphne and Chen, Qiuyu and Chung, Trinity and Drake, Jaimyn and Foster, Ethan Paul and Gao, Jensen and Herrera, David Antonio and Heo, Minho and Hsu, Kyle and Hu, Jiaheng and Jackson, Donovon and Le, Charlotte and Li, Yunshuang and Lin, Kevin and Lin, Roy and Ma, Zehan and Maddukuri, Abhiram and Mirchandani, Suvir and Morton, Daniel and Nguyen, Tony and O'Neill, Abigail and Scalise, Rosario and Seale, Derick and Son, Victor and Tian, Stephen and Tran, Emi and Wang, Andrew E. and Wu, Yilin and Xie, Annie and Yang, Jingyun and Yin, Patrick and Zhang, Yunchu and Bastani, Osbert and Berseth, Glen and Bohg, Jeannette and Goldberg, Ken and Gupta, Abhinav and Gupta, Abhishek and Jayaraman, Dinesh and Lim, Joseph J and Malik, Jitendra and MartÃ­n-MartÃ­n, Roberto and Ramamoorthy, Subramanian and Sadigh, Dorsa and Song, Shuran and Wu, Jiajun and Yip, Michael C. and Zhu, Yuke and Kollar, Thomas and Levine, Sergey and Finn, Chelsea},
	year = {2024},
	note = {Version Number: 1
TLDR: This work introduces DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months.},
	keywords = {FOS: Computer and information sciences, Robotics (cs.RO), ccfInfo: CCF-None CORR},
}

@misc{reuss_multimodal_2024,
	title = {Multimodal diffusion transformer: learning versatile behavior from multimodal goals},
	shorttitle = {Multimodal diffusion transformer},
	url = {http://arxiv.org/abs/2407.05996},
	doi = {10.48550/arXiv.2407.05996},
	abstract = {This work introduces the Multimodal Diffusion Transformer (MDT), a novel diffusion policy framework, that excels at learning versatile behavior from multimodal goal specifications with few language annotations. MDT leverages a diffusion-based multimodal transformer backbone and two self-supervised auxiliary objectives to master long-horizon manipulation tasks based on multimodal goals. The vast majority of imitation learning methods only learn from individual goal modalities, e.g. either language or goal images. However, existing large-scale imitation learning datasets are only partially labeled with language annotations, which prohibits current methods from learning language conditioned behavior from these datasets. MDT addresses this challenge by introducing a latent goal-conditioned state representation that is simultaneously trained on multimodal goal instructions. This state representation aligns image and language based goal embeddings and encodes sufficient information to predict future states. The representation is trained via two self-supervised auxiliary objectives, enhancing the performance of the presented transformer backbone. MDT shows exceptional performance on 164 tasks provided by the challenging CALVIN and LIBERO benchmarks, including a LIBERO version that contains less than \$2{\textbackslash}\%\$ language annotations. Furthermore, MDT establishes a new record on the CALVIN manipulation challenge, demonstrating an absolute performance improvement of \$15{\textbackslash}\%\$ over prior state-of-the-art methods that require large-scale pretraining and contain \$10{\textbackslash}times\$ more learnable parameters. MDT shows its ability to solve long-horizon manipulation from sparsely annotated data in both simulated and real-world environments. Demonstrations and Code are available at https://intuitive-robots.github.io/mdt\_policy/.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Reuss, Moritz and YaÄŸmurlu, Ã–mer ErdinÃ§ and Wenzel, Fabian and Lioutikov, Rudolf},
	month = jul,
	year = {2024},
	note = {arXiv:2407.05996 [cs]
TLDR: å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼ˆMDTï¼‰è¢«æå‡ºï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ‰©æ•£ç­–ç•¥æ¡†æ¶ï¼Œæ“…é•¿ä»å¤šæ¨¡æ€ç›®æ ‡è§„èŒƒä¸­å­¦ä¹ å¤šæ ·åŒ–è¡Œä¸ºï¼Œä¸”ä»…éœ€å°‘é‡è¯­è¨€æ³¨é‡Šã€‚},
	keywords = {Computer Science - Robotics},
}

@article{lv_land_2022,
	title = {Land cover change detection with heterogeneous remote sensing images: review, progress, and perspective},
	volume = {110},
	issn = {1558-2256},
	shorttitle = {Land cover change detection with heterogeneous remote sensing images},
	url = {https://ieeexplore.ieee.org/abstract/document/9955391},
	doi = {10.1109/JPROC.2022.3219376},
	abstract = {With the fast development of remote sensing platforms and sensors technology, change detection with heterogeneous remote sensing images (Hete-CD) has become an attractive topic in recent years and plays a vital role in land cover change detection for responding to natural disaster emergencies when homogeneous images are unavailable. Although Hete-CD has been developed for about three decades, and various related methods have been developed and applied successfully in practice, a systematic and comprehensive review of the current achievements regarding Hete-CD remains lacking. Therefore, in this article, we first present an overview of Hete-CD in terms of the related literature. Second, the major techniques of Hete-CD are reviewed in terms of publicly available datasets, the taxonomy of major techniques, results, performance, and quantitative evaluation. Then, some classical methods are selected for comparison and discussion. Finally, based on the discussion and literature review, challenges, opportunities, and future directions for Hete-CD are concluded. The review aims to provide a â€œone-stop-shopâ€ understanding of the problems with the categories of existing approaches, open opportunities and challenges, and potential future directions for Hete-CD.},
	language = {en},
	number = {12},
	urldate = {2025-03-13},
	journal = {Proceedings of the IEEE},
	author = {Lv, ZhiYong and Huang, HaiTao and Li, Xinghua and Zhao, MingHua and Benediktsson, JÃ³n Atli and Sun, WeiWei and Falco, Nicola},
	month = dec,
	year = {2022},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Adaptive optics, Cross-modal change detection, Earthquakes, Heterogeneous networks, Optical imaging, Optical sensors, Remote sensing, Terrain factors, cross-sensor change detection, different resolution change detection, heterogeneous remote-sensing change detection, multimodality change detection, multiresolution change detection},
	pages = {1976--1991},
}

@misc{li_gr-mg_2024,
	title = {{GR}-{MG}: leveraging partially annotated data via multi-modal goal-conditioned policy},
	shorttitle = {Gr-mg},
	url = {http://arxiv.org/abs/2408.14368},
	doi = {10.48550/arXiv.2408.14368},
	abstract = {The robotics community has consistently aimed to achieve generalizable robot manipulation with flexible natural language instructions. One primary challenge is that obtaining robot trajectories fully annotated with both actions and texts is time-consuming and labor-intensive. However, partially-annotated data, such as human activity videos without action labels and robot trajectories without text labels, are much easier to collect. Can we leverage these data to enhance the generalization capabilities of robots? In this paper, we propose GR-MG, a novel method which supports conditioning on a text instruction and a goal image. During training, GR-MG samples goal images from trajectories and conditions on both the text and the goal image or solely on the image when text is not available. During inference, where only the text is provided, GR-MG generates the goal image via a diffusion-based image-editing model and conditions on both the text and the generated image. This approach enables GR-MG to leverage large amounts of partially-annotated data while still using languages to flexibly specify tasks. To generate accurate goal images, we propose a novel progress-guided goal image generation model which injects task progress information into the generation process. In simulation experiments, GR-MG improves the average number of tasks completed in a row of 5 from 3.35 to 4.04. In real-robot experiments, GR-MG is able to perform 58 different tasks and improves the success rate from 68.7{\textbackslash}\% to 78.1{\textbackslash}\% and 44.4{\textbackslash}\% to 60.6{\textbackslash}\% in simple and generalization settings, respectively. It also outperforms comparing baseline methods in few-shot learning of novel skills. Video demos, code, and checkpoints are available on the project page: https://gr-mg.github.io/.},
	language = {en},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Li, Peiyan and Wu, Hongtao and Huang, Yan and Cheang, Chilam and Wang, Liang and Kong, Tao},
	month = dec,
	year = {2024},
	note = {arXiv:2408.14368 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{tan_segment_2024,
	title = {Segment change model ({SCM}) for unsupervised change detection in {VHR} remote sensing images: a case study of buildings},
	shorttitle = {Segment change model ({SCM}) for unsupervised change detection in {VHR} remote sensing images},
	url = {http://arxiv.org/abs/2312.16410},
	doi = {10.1109/IGARSS53475.2024.10642429},
	abstract = {The field of Remote Sensing (RS) widely employs Change Detection (CD) on very-high-resolution (VHR) images. A majority of extant deep-learning-based methods hinge on annotated samples to complete the CD process. Recently, the emergence of Vision Foundation Model (VFM) enables zero-shot predictions in particular vision tasks. In this work, we propose an unsupervised CD method named Segment Change Model (SCM), built upon the Segment Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP). Our method recalibrates features extracted at different scales and integrates them in a top-down manner to enhance discriminative change edges. We further design an innovative Piecewise Semantic Attention (PSA) scheme, which can offer semantic representation without training, thereby minimize pseudo change phenomenon. Through conducting experiments on two public datasets, the proposed SCM increases the mIoU from 46.09\% to 53.67\% on the LEVIR-CD dataset, and from 47.56\% to 52.14\% on the WHU-CD dataset. Our codes are available at https://github.com/StephenApX/UCD-SCM.},
	language = {en},
	urldate = {2025-03-12},
	booktitle = {Igarss 2024 - 2024 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Tan, Xiaoliang and Chen, Guanzhou and Wang, Tong and Wang, Jiaqi and Zhang, Xiaodong},
	month = jul,
	year = {2024},
	note = {arXiv:2312.16410 [cs]
TLDR: This work proposes an unsupervised CD method named Segment Change Model (SCM), built upon the Segment Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP) and designs an innovative Piecewise Semantic Attention (PSA) scheme, which can offer semantic representation without training, thereby minimize pseudo change phenomenon.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {8577--8580},
}

@misc{chen_time_2023,
	title = {Time travelling pixels: bitemporal features integration with foundation model for remote sensing image change detection},
	shorttitle = {Time travelling pixels},
	url = {http://arxiv.org/abs/2312.16202},
	doi = {10.48550/arXiv.2312.16202},
	abstract = {Change detection, a prominent research area in remote sensing, is pivotal in observing and analyzing surface transformations. Despite significant advancements achieved through deep learning-based methods, executing high-precision change detection in spatio-temporally complex remote sensing scenarios still presents a substantial challenge. The recent emergence of foundation models, with their powerful universality and generalization capabilities, offers potential solutions. However, bridging the gap of data and tasks remains a significant obstacle. In this paper, we introduce Time Travelling Pixels (TTP), a novel approach that integrates the latent knowledge of the SAM foundation model into change detection. This method effectively addresses the domain shift in general knowledge transfer and the challenge of expressing homogeneous and heterogeneous characteristics of multi-temporal images. The state-of-the-art results obtained on the LEVIR-CD underscore the efficacy of the TTP. The Code is available at {\textbackslash}url\{https://kychen.me/TTP\}.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Chen, Keyan and Liu, Chengyang and Li, Wenyuan and Liu, Zili and Chen, Hao and Zhang, Haotian and Zou, Zhengxia and Shi, Zhenwei},
	month = dec,
	year = {2023},
	note = {arXiv:2312.16202 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{fang_changer_2023,
	title = {Changer: feature interaction is what you need for change detection},
	volume = {61},
	issn = {1558-0644},
	shorttitle = {Changer},
	url = {https://ieeexplore.ieee.org/abstract/document/10129139},
	doi = {10.1109/TGRS.2023.3277496},
	abstract = {Change detection is an important tool for long-term Earth observation missions. It takes bi-temporal images as input and predicts â€œwhereâ€ the change has occurred. Different from other dense prediction tasks, a meaningful consideration for change detection is the interaction between bi-temporal features. With this motivation, in this article we propose a novel general change detection architecture, MetaChanger, which includes a series of alternative interaction layers in the feature extractor. To verify the effectiveness of MetaChanger, we propose two derived models, ChangerAD and ChangerEx with simple interaction strategies: aggregation-distribution (AD) and feature â€œexchange.â€ AD is abstracted from some complex interaction methods, and feature â€œexchangeâ€ is a completely parameter and computation-free operation by exchanging bi-temporal features. In addition, for better alignment of bi-temporal features, we propose a flow-based dual-alignment fusion (FDAF) module which allows interactive alignment and feature fusion. Crucially, we observe Changer series models achieve competitive performance on different scale change detection datasets. Further, our proposed ChangerAD and ChangerEx could serve as a starting baseline for future MetaChanger design. Code and weights are made available at https://github.com/likyoo/open-cd.},
	language = {en},
	urldate = {2025-03-11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Fang, Sheng and Li, Kaiyu and Li, Zhe},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing
TLDR: A novel general change detection architecture, MetaChanger, which includes a series of alternative interaction layers in the feature extractor, and a flow-based dual-alignment fusion (FDAF) module which allows interactive alignment and feature fusion.},
	keywords = {Change detection, Decoding, Feature extraction, Image segmentation, Indexes, Semantics, Task analysis, Transformers, ccfInfo: CCF-B TGARS, citationNumber: 25, deep neural network, feature interaction, high-resolution remote sensing (RS) image},
	pages = {1--11},
}

@inproceedings{bao_one_2023,
	title = {One transformer fits all distributions in multi-modal diffusion at scale},
	url = {https://proceedings.mlr.press/v202/bao23a.html},
	abstract = {This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is â€“ learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model â€“ perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation).},
	language = {en},
	urldate = {2025-03-11},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bao, Fan and Nie, Shen and Xue, Kaiwen and Li, Chongxuan and Pu, Shi and Wang, Yaole and Yue, Gang and Cao, Yue and Su, Hang and Zhu, Jun},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498
shortConferenceName: ICML},
	keywords = {ccfInfo: CCF-A ICML, citationNumber: 119},
	pages = {1692--1717},
}

@misc{agia_unpacking_2024,
	title = {Unpacking failure modes of generative policies: runtime monitoring of consistency and progress},
	shorttitle = {Unpacking failure modes of generative policies},
	url = {http://arxiv.org/abs/2410.04640},
	doi = {10.48550/arXiv.2410.04640},
	abstract = {Robot behavior policies trained via imitation learning are prone to failure under conditions that deviate from their training data. Thus, algorithms that monitor learned policies at test time and provide early warnings of failure are necessary to facilitate scalable deployment. We propose Sentinel, a runtime monitoring framework that splits the detection of failures into two complementary categories: 1) Erratic failures, which we detect using statistical measures of temporal action consistency, and 2) task progression failures, where we use Vision Language Models (VLMs) to detect when the policy confidently and consistently takes actions that do not solve the task. Our approach has two key strengths. First, because learned policies exhibit diverse failure modes, combining complementary detectors leads to significantly higher accuracy at failure detection. Second, using a statistical temporal action consistency measure ensures that we quickly detect when multimodal, generative policies exhibit erratic behavior at negligible computational cost. In contrast, we only use VLMs to detect failure modes that are less time-sensitive. We demonstrate our approach in the context of diffusion policies trained on robotic mobile manipulation domains in both simulation and the real world. By unifying temporal consistency detection and VLM runtime monitoring, Sentinel detects 18\% more failures than using either of the two detectors alone and significantly outperforms baselines, thus highlighting the importance of assigning specialized detectors to complementary categories of failure. Qualitative results are made available at https://sites.google.com/stanford.edu/sentinel.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Agia, Christopher and Sinha, Rohan and Yang, Jingyun and Cao, Zi-ang and Antonova, Rika and Pavone, Marco and Bohg, Jeannette},
	month = oct,
	year = {2024},
	note = {arXiv:2410.04640 [cs]
TLDR: Sentinel is a runtime monitoring framework that splits the detection of failures into two complementary categories: Erratic failures, which the authors detect using statistical measures of temporal action consistency, and task progression failures, where they use Vision Language Models (VLMs) to detect when the policy confidently and consistently takes actions that do not solve the task.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{li_stade-cdnet_2024,
	title = {{STADE}-{CDNet}: spatialâ€“temporal attention with difference enhancement-based network for remote sensing image change detection},
	volume = {62},
	issn = {1558-0644},
	shorttitle = {{STADE}-{CDNet}},
	url = {https://ieeexplore.ieee.org/abstract/document/10440364},
	doi = {10.1109/TGRS.2024.3367948},
	abstract = {High-resolution remote sensing (RS) image change detection (CD) focuses on ground surface changes. It has wide applications, including territorial spatial planning, urban region detection, and military operations. However, class imbalance and pseudochanges are caused by the unchanged areas far outnumbering the changed areas and lighting changes. To address these problems, we propose spatialâ€“temporal attention with a difference enhancement-based network (STADE-CDNet). In STADE-CDNet, a CD difference enhancement module (CDDM) is proposed to extract important features from the difference map to detect changed regions. This module enhances the network with differential feature attributes through the training layer, improving the networkâ€™s learning ability and reducing the imbalance problem. A temporal memory module (TMM) is designed to extract temporal and spatial information. Inspired by the self-attention mechanism of the transformer, we propose a transformer and TMM (TTMM). Four encoding layers are designed to detect the semantic information from high to low levels of the multitemporal image pairs. The fusion and parallelism of multivariate data are achieved through collaborative modeling of deep learning and CD, compensating for the need for excessive human intervention in traditional algorithms. We evaluate our approach in two different datasets [LEVIR building CD (LEVIR-CD) and deeply supervised image fusion network for change detection (DSIFN-CD)]. Promising quantitative and qualitative results show that the STADE-CDNet can improve accuracy. In particular, the proposed CDDM significantly reduces false positive detection, with F1 scores at least 1.97\% and 2.1\% higher than other methods in the case of the LEVIR-CD and DSIFN-CD datasets, respectively. Our code is available at https://github.com/LiLisaZhi/STADE-CDNet.},
	language = {en},
	urldate = {2025-03-11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Li, Zhi and Cao, Siying and Deng, Jiakun and Wu, Fengyi and Wang, Ruilan and Luo, Junhai and Peng, Zhenming},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing
TLDR: Promising quantitative and qualitative results show that the STADE-CDNet can improve accuracy, and the proposed CDDM significantly reduces false positive detection, with \$F1\$ scores at least 1.97\% higher than other methods in the case of the LEVIR-CD and DSIFN-CD datasets.},
	keywords = {Change detection (CD) difference enhancement, Data mining, Deep learning, Feature extraction, Memory modules, Remote sensing, Semantics, Transformers, multitemporal image pairs, temporal memory, transformer},
	pages = {1--17},
}

@article{li_semicd-vl_2024,
	title = {{SemiCD}-{VL}: visual-language model guidance makes better semi-supervised change detector},
	shorttitle = {{SemiCD}-{VL}},
	url = {https://ieeexplore.ieee.org/abstract/document/10781418/},
	language = {en},
	urldate = {2025-03-11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Li, Kaiyu and Cao, Xiangyong and Deng, Yupeng and Song, Jiayi and Liu, Junmin and Meng, Deyu and Wang, Zhi},
	year = {2024},
	note = {Publisher: IEEE},
	keywords = {ccfInfo: Not Found, citationNumber: 0},
}

@misc{_likyoosemicd-vl_2025,
	title = {likyoo/{SemiCD}-{VL}},
	copyright = {Apache-2.0},
	url = {https://github.com/likyoo/SemiCD-VL},
	abstract = {The pytorch implementation for "SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised Change Detector"},
	urldate = {2025-03-11},
	author = {æå¼€å®‡},
	month = mar,
	year = {2025},
	note = {original-date: 2024-04-11T03:52:41Z},
}

@misc{xing_towards_2025,
	title = {Towards robust and secure embodied {AI}: a survey on vulnerabilities and attacks},
	shorttitle = {Towards robust and secure embodied {AI}},
	url = {http://arxiv.org/abs/2502.13175},
	doi = {10.48550/arXiv.2502.13175},
	abstract = {Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Xing, Wenpeng and Li, Minghao and Li, Mohan and Han, Meng},
	month = feb,
	year = {2025},
	note = {arXiv:2502.13175 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Robotics, ccfInfo: Not Found, citationNumber: 0},
}

@article{zuo_privacy-enhanced_2024,
	title = {Privacy-{Enhanced} {Prototype}-{Based} {Federated} {Cross}-{Modal} {Hashing} for {Cross}-{Modal} {Retrieval}},
	volume = {20},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3674507},
	doi = {10.1145/3674507},
	abstract = {Cross-modal hashing is widely used for efficient similarity searches, improving data processing efficiency, and reducing storage costs. Existing cross-modal hashing methods primarily focus on centralized training scenarios, where fixed-scale and fixed-category multi-modal data is collected beforehand. However, these methods often face challenges associated with the potential risk of privacy breaches and high data communication costs during data transmission in real-world multimedia retrieval tasks. To tackle these challenges, in this article, we propose an efficient
              privacy-enhanced prototype-based federated cross-modal hashing
              (PEPFCH). In PEPFCH, we integrate local and global prototypes in order to effectively capture the distinctive traits of individual clients, while also harnessing the collective intelligence of the entire federated learning system. Moreover, to ensure the security of prototype information and prevent its disclosure during the aggregation process, we use a prototype encryption transmission mechanism to encrypt the prototype information before transmission, making it challenging for attackers to gain access to sensitive data. Additionally, to facilitate personalized federated learning and alleviate the issue of parametric catastrophic forgetting, we establish the image and text hyper-networks for each client and adopt a hyper-network extension strategy to selectively preserve and update previously acquired knowledge when acquiring new concepts or categories. Comprehensive experiments highlight the efficiency and superiority of our proposed method. To enhance research and accessibility, we have publicly released our source codes at:
              https://github.com/vindahi/PEPFCH
              .},
	language = {en},
	number = {9},
	urldate = {2025-03-05},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	author = {Zuo, Ruifan and Zheng, Chaoqun and Li, Fengling and Zhu, Lei and Zhang, Zheng},
	month = sep,
	year = {2024},
	note = {TLDR: In PEPFCH, local and global prototypes are integrated in order to effectively capture the distinctive traits of individual clients, while also harnessing the collective intelligence of the entire federated learning system.},
	keywords = {ccfInfo: Not Found, citationNumber: 0},
	pages = {1--19},
}

@article{shi_incomplete_2024,
	title = {Incomplete {Cross}-{Modal} {Retrieval} with {Deep} {Correlation} {Transfer}},
	volume = {20},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3637442},
	doi = {10.1145/3637442},
	abstract = {Most cross-modal retrieval methods assume the multi-modal training data is complete and has a one-to-one correspondence. However, in the real world, multi-modal data generally suffers from missing modality information due to the uncertainty of data collection and storage processes, which limits the practical application of existing cross-modal retrieval methods. Although some solutions have been proposed to generate the missing modality data using a single pseudo sample, this may lead to incomplete semantic restoration and sub-optimal retrieval results due to the limited semantic information it provides. To address this challenge, this article proposes an Incomplete Cross-Modal Retrieval with Deep Correlation Transfer (ICMR-DCT) method that can robustly model incomplete multi-modal data and dynamically capture the adjacency semantic correlation for cross-modal retrieval. Specifically, we construct intra-modal graph attention-based auto-encoder to learn modality-invariant representations by performing semantic reconstruction through intra-modality adjacency correlation mining. Then, we design dual cross-modal alignment constraints to project multi-modal representations into a common semantic space, thus bridging the heterogeneous modality gap and enhancing the discriminability of the common representation. We further introduce semantic preservation to enhance adjacency semantic information and achieve cross-modal semantic correlation. Moreover, we propose a nearest-neighbor weighting integration strategy with cross-modal correlation transfer to generate the missing modality data according to inter-modality mapping relations and adjacency correlations between each sample and its neighbors, which improves the robustness of our method against incomplete multi-modal training data. Extensive experiments on three widely tested benchmark datasets demonstrate the superior performance of our method in cross-modal retrieval tasks under both complete and incomplete retrieval scenarios. Our used datasets and source codes are available at
              https://github.com/shidan0122/DCT.git
              .},
	language = {en},
	number = {5},
	urldate = {2025-03-05},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	author = {Shi, Dan and Zhu, Lei and Li, Jingjing and Dong, Guohua and Zhang, Huaxiang},
	month = may,
	year = {2024},
	note = {TLDR: This article proposes an Incomplete Cross-Modal Retrieval with Deep Correlation Transfer (ICMR-DCT) method that can robustly model incomplete multi-modal data and dynamically capture the adjacency semantic correlation for cross-modal retrieval.},
	keywords = {ccfInfo: CCF-B TOMCCAP, citationNumber: 0},
	pages = {1--21},
}

@inproceedings{cui_webly_2022,
	address = {Lisboa Portugal},
	title = {Webly supervised image hashing with lightweight semantic transfer network},
	isbn = {978-1-4503-9203-7},
	url = {https://dl.acm.org/doi/10.1145/3503161.3548342},
	doi = {10.1145/3503161.3548342},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Cui, Hui and Zhu, Lei and Li, Jingjing and Zhang, Zheng and Guan, Weili},
	month = oct,
	year = {2022},
	note = {TLDR: A Webly Supervised Image Hashing with a well-designed lightweight network that enhances the semantics of unsupervised image hashing with the weak supervision from freely available web images, and simultaneously avoids involving over-abundant parameters in the deep network architecture.},
	pages = {3451--3460},
}

@article{wei_fummer_2024,
	title = {{FUMMER}: {A} fine-grained self-supervised momentum distillation framework for multimodal recommendation},
	volume = {61},
	shorttitle = {{FUMMER}},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457324001365},
	number = {5},
	urldate = {2025-03-05},
	journal = {Information Processing \& Management},
	author = {Wei, Yibiao and Xu, Yang and Zhu, Lei and Ma, Jingwei and Huang, Jiangping},
	year = {2024},
	note = {Publisher: Elsevier},
	keywords = {ccfInfo: CCF-B IPM, citationNumber: 0},
	pages = {103776},
}

@article{zhang_deep_2023,
	title = {Deep collaborative graph hashing for discriminative image retrieval},
	volume = {139},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323001620},
	urldate = {2025-03-05},
	journal = {Pattern Recognition},
	author = {Zhang, Zheng and Wang, Jianning and Zhu, Lei and Luo, Yadan and Lu, Guangming},
	year = {2023},
	note = {Publisher: Elsevier},
	keywords = {ccfInfo: CCF-B, citationNumber: 9},
	pages = {109462},
}

@incollection{zhu_context-aware_2024,
	address = {Cham},
	title = {Context-aware hashing},
	isbn = {978-3-031-37290-2 978-3-031-37291-9},
	url = {https://link.springer.com/10.1007/978-3-031-37291-9_2},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Multi-modal {Hash} {Learning}},
	publisher = {Springer International Publishing},
	author = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	collaborator = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	year = {2024},
	doi = {10.1007/978-3-031-37291-9_2},
	note = {Series Title: Synthesis Lectures on Information Concepts, Retrieval, and Services},
	keywords = {ccfInfo: Not Found, citationNumber: 0},
	pages = {7--44},
}

@incollection{zhu_multi-modal_2024-1,
	address = {Cham},
	title = {Multi-modal discrete collaborative filtering},
	isbn = {978-3-031-37290-2 978-3-031-37291-9},
	url = {https://link.springer.com/10.1007/978-3-031-37291-9_5},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Multi-modal {Hash} {Learning}},
	publisher = {Springer International Publishing},
	author = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	collaborator = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	year = {2024},
	doi = {10.1007/978-3-031-37291-9_5},
	note = {Series Title: Synthesis Lectures on Information Concepts, Retrieval, and Services},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
	pages = {145--195},
}

@article{wang_multi-level_2023,
	title = {Multi-level adversarial attention cross-modal hashing},
	volume = {117},
	url = {https://www.sciencedirect.com/science/article/pii/S0923596523000991},
	language = {en},
	urldate = {2025-03-05},
	journal = {Signal Processing: Image Communication},
	author = {Wang, Benhui and Zhang, Huaxiang and Zhu, Lei and Nie, Liqiang and Liu, Li},
	year = {2023},
	note = {Publisher: Elsevier},
	keywords = {ccfInfo: CCF-C SPIC, citationNumber: 1},
	pages = {117017},
}

@incollection{zhu_cross-modal_2024,
	address = {Cham},
	title = {Cross-modal hashing},
	isbn = {978-3-031-37290-2 978-3-031-37291-9},
	url = {https://link.springer.com/10.1007/978-3-031-37291-9_3},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Multi-modal {Hash} {Learning}},
	publisher = {Springer International Publishing},
	author = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	collaborator = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	year = {2024},
	doi = {10.1007/978-3-031-37291-9_3},
	note = {Series Title: Synthesis Lectures on Information Concepts, Retrieval, and Services},
	keywords = {ccfInfo: Not Found},
	pages = {45--89},
}

@incollection{zhu_composite_2024,
	address = {Cham},
	title = {Composite multi-modal hashing},
	isbn = {978-3-031-37290-2 978-3-031-37291-9},
	url = {https://link.springer.com/10.1007/978-3-031-37291-9_4},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Multi-modal {Hash} {Learning}},
	publisher = {Springer International Publishing},
	author = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	collaborator = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	year = {2024},
	doi = {10.1007/978-3-031-37291-9_4},
	note = {Series Title: Synthesis Lectures on Information Concepts, Retrieval, and Services},
	keywords = {ccfInfo: Not Found, citationNumber: 0},
	pages = {91--144},
}

@article{xia_when_2023,
	title = {When {CLIP} meets cross-modal hashing retrieval: {A} new strong baseline},
	volume = {100},
	shorttitle = {When {CLIP} meets cross-modal hashing retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253523002841},
	urldate = {2025-03-05},
	journal = {Information Fusion},
	author = {Xia, Xinyu and Dong, Guohua and Li, Fengling and Zhu, Lei and Ying, Xiaomin},
	year = {2023},
	note = {Publisher: Elsevier},
	keywords = {ccfInfo: CCF-None INFFUS, citationNumber: 8},
	pages = {101968},
}

@article{li_cross-domain_2024,
	title = {Cross-domain transfer hashing for efficient cross-modal retrieval},
	url = {https://ieeexplore.ieee.org/abstract/document/10463060/},
	language = {en},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Li, Fengling and Wang, Bowen and Zhu, Lei and Li, Jingjing and Zhang, Zheng and Chang, Xiaojun},
	year = {2024},
	note = {Publisher: IEEE},
	keywords = {ccfInfo: CCF-B TCSVT, citationNumber: 0},
}

@article{jin_based_2024,
	title = {Based on spatial and temporal implicit semantic relational inference for cross-modal retrieval},
	url = {https://ieeexplore.ieee.org/abstract/document/10551848/},
	language = {en},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Jin, Ming and Hu, Wenbo and Zhu, Lei and Wang, Xiang and Hong, Richang},
	year = {2024},
	note = {Publisher: IEEE},
	keywords = {ccfInfo: Not Found, citationNumber: 0},
}

@article{wei_multi-level_2024,
	title = {Multi-level cross-modal contrastive learning for review-aware recommendation},
	volume = {247},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417424002069},
	urldate = {2025-03-05},
	journal = {Expert Systems with Applications},
	author = {Wei, Yibiao and Xu, Yang and Zhu, Lei and Ma, Jingwei and Peng, Chengmei},
	year = {2024},
	note = {Publisher: Elsevier},
	keywords = {ccfInfo: CCF-C ESWA, citationNumber: 0},
	pages = {123341},
}

@article{xue_attention_2025,
	title = {Attention {Guidance} by {Cross}-{Domain} {Supervision} {Signals} for {Scene} {Text} {Recognition}},
	url = {https://ieeexplore.ieee.org/abstract/document/10838318/},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Image Processing},
	author = {Xue, Fanfu and Sun, Jiande and Xue, Yaqi and Wu, Qiang and Zhu, Lei and Chang, Xiaojun and Cheung, Sen-ching},
	year = {2025},
	note = {Publisher: IEEE},
	keywords = {ccfInfo: Not Found, citationNumber: 0},
}

@inproceedings{su_soil_2024,
	address = {Melbourne VIC Australia},
	title = {{SOIL}: contrastive second-order interest learning for multimodal recommendation},
	isbn = {979-8-4007-0686-8},
	shorttitle = {Soil},
	url = {https://dl.acm.org/doi/10.1145/3664647.3681207},
	doi = {10.1145/3664647.3681207},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Su, Hongzu and Li, Jingjing and Li, Fengling and Lu, Ke and Zhu, Lei},
	month = oct,
	year = {2024},
	keywords = {ccfInfo: CCF-A ACM MM, citationNumber: 0},
	pages = {5838--5846},
}

@article{wang_cross-modal_2025,
	title = {Cross-modal retrieval: a systematic review of methods and future directions},
	shorttitle = {Cross-modal retrieval},
	url = {https://ieeexplore.ieee.org/abstract/document/10843094/},
	language = {en},
	urldate = {2025-03-05},
	journal = {Proceedings of the IEEE},
	author = {Wang, Tianshi and Li, Fengling and Zhu, Lei and Li, Jingjing and Zhang, Zheng and Shen, Heng Tao},
	year = {2025},
	note = {Publisher: IEEE},
	keywords = {ccfInfo: CCF-None CORR, citationNumber: 2},
}

@article{zhang_deep_2022,
	title = {Deep discrete hashing for label distribution learning},
	volume = {29},
	url = {https://ieeexplore.ieee.org/abstract/document/9732636/},
	language = {en},
	urldate = {2025-03-05},
	journal = {IEEE Signal Processing Letters},
	author = {Zhang, Zhen and Zhu, Lei and Li, Yaping and Xu, Yang},
	year = {2022},
	note = {Publisher: IEEE},
	pages = {832--836},
}

@article{jin_coarse--fine_2022,
	title = {Coarse-to-fine dual-level attention for video-text cross modal retrieval},
	volume = {242},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122001320},
	language = {en},
	urldate = {2025-03-05},
	journal = {Knowledge-Based Systems},
	author = {Jin, Ming and Zhang, Huaxiang and Zhu, Lei and Sun, Jiande and Liu, Li},
	year = {2022},
	note = {Publisher: Elsevier},
	pages = {108354},
}

@article{tan_teacher-student_2022,
	title = {Teacher-student learning: efficient hierarchical message aggregation hashing for cross-modal retrieval},
	volume = {25},
	shorttitle = {Teacher-student learning},
	url = {https://ieeexplore.ieee.org/abstract/document/9782694/},
	language = {en},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Multimedia},
	author = {Tan, Wentao and Zhu, Lei and Li, Jingjing and Zhang, Huaxiang and Han, Junwei},
	year = {2022},
	note = {Publisher: IEEE},
	pages = {4520--4532},
}

@article{jin_video_2022,
	title = {Video sampled frame category aggregation and consistent representation for cross-modal retrieval},
	volume = {33},
	url = {https://ieeexplore.ieee.org/abstract/document/9878130/},
	language = {en},
	number = {2},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Jin, Ming and Zhang, Huaxiang and Zhu, Lei and Sun, Jiande and Liu, Li},
	year = {2022},
	note = {Publisher: IEEE},
	pages = {909--919},
}

@article{zheng_efficient_2022,
	title = {Efficient semi-supervised multimodal hashing with importance differentiation regression},
	volume = {31},
	url = {https://ieeexplore.ieee.org/abstract/document/9880554/},
	language = {en},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Image Processing},
	author = {Zheng, Chaoqun and Zhu, Lei and Zhang, Zheng and Li, Jingjing and Yu, Xiaomei},
	year = {2022},
	note = {Publisher: IEEE},
	pages = {5881--5892},
}

@article{zhang_discriminative_2022,
	title = {Discriminative visual similarity search with semantically cycle-consistent hashing networks},
	volume = {18},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3532519},
	doi = {10.1145/3532519},
	abstract = {Deep hashing has great potential in large-scale visual similarity search due to its preferable efficiency in storage and computation. Technically, deep hashing for visual similarity search inherits the powerful representation capability of deep neural networks, and it encodes visual features into compact binary codes by preserving representative semantic visual features. Works in this field mainly focus on building the relationship between the visual and objective hash spaces, while they seldom study the triadic cross-domain semantic knowledge transfer among visual, semantic, and hashing spaces, leading to a serious semantic ignorance problem during space transformation. In this article, we propose a novel deep tripartite semantically interactive hashing framework, dubbed Semantically Cycle-consistent Hashing Networks (SCHNs), for discriminative hash code learning. Particularly, we construct a flexible semantic space and a transitive latent space, in conjunction with the visual space, to jointly deduce the privileged discriminative hash space. Specifically, a new semantic space is conceived to strengthen the flexibility and completeness of categories in the semantic feature inference phase. At the same time, a transitive latent space is formulated to explore and uncover the shared semantic interactivity embedded in visual and semantic features. Moreover, to further ensure semantic consistency across multiple spaces, we propose to build a cyclic adversarial learning module to preserve and keep their semantic concurrence during space transformation. Notably, our SCHN, for the first time, establishes the cyclic principle of deep semantic-preserving hashing by adaptive semantic parsing across different spaces in a single-modal visual similarity search. In addition, the entire learning framework is jointly optimized in an end-to-end manner. Extensive experiments performed on diverse large-scale datasets evidence the superiority of our method against other state-of-the-art deep hashing algorithms. The source codes of this article are available at https://github.com/JalinWang/SCHN.},
	language = {en},
	number = {2s},
	urldate = {2025-03-05},
	journal = {ACM Transactions on Multimedia Computing Communications and Applications},
	author = {Zhang, Zheng and Wang, Jianning and Zhu, Lei and Lu, Guangming},
	month = jun,
	year = {2022},
	pages = {1--21},
}

@article{zhu_work_2022,
	title = {Work together: correlation-identity reconstruction hashing for unsupervised cross-modal retrieval},
	volume = {35},
	shorttitle = {Work together},
	url = {https://ieeexplore.ieee.org/abstract/document/9933831/},
	language = {en},
	number = {9},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhu, Lei and Wu, Xize and Li, Jingjing and Zhang, Zheng and Guan, Weili and Shen, Heng Tao},
	year = {2022},
	note = {Publisher: IEEE},
	pages = {8838--8851},
}

@article{shi_unsupervised_2023,
	title = {Unsupervised adaptive feature selection with binary hashing},
	volume = {32},
	url = {https://ieeexplore.ieee.org/abstract/document/10014652/},
	language = {en},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Image Processing},
	author = {Shi, Dan and Zhu, Lei and Li, Jingjing and Zhang, Zheng and Chang, Xiaojun},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {838--853},
}

@book{zhu_multi-modal_2024-2,
	address = {Cham},
	series = {Synthesis {Lectures} on {Information} {Concepts}, {Retrieval}, and {Services}},
	title = {Multi-modal hash learning: efficient multimedia retrieval and recommendations},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-37290-2 978-3-031-37291-9},
	shorttitle = {Multi-modal hash learning},
	url = {https://link.springer.com/10.1007/978-3-031-37291-9},
	language = {en},
	urldate = {2025-03-05},
	publisher = {Springer International Publishing},
	author = {Zhu, Lei and Li, Jingjing and Guan, Weili},
	year = {2024},
	doi = {10.1007/978-3-031-37291-9},
}

@misc{brohan_rt-1_2023,
	title = {{RT}-1: robotics transformer for real-world control at scale},
	shorttitle = {Rt-1},
	url = {http://arxiv.org/abs/2212.06817},
	doi = {10.48550/arXiv.2212.06817},
	abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io},
	language = {en},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
	month = aug,
	year = {2023},
	note = {arXiv:2212.06817 [cs]
TLDR: This paper presents a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties and verify the conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, ccfInfo: CCF-None RSS, citationNumber: 587},
}

@article{li_divergence-agnostic_2022,
	title = {Divergence-agnostic unsupervised domain adaptation by adversarial attacks},
	volume = {44},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9528987/},
	doi = {10.1109/TPAMI.2021.3109287},
	abstract = {Conventional machine learning algorithms suffer the problem that the model trained on existing data fails to generalize well to the data sampled from other distributions. To tackle this issue, unsupervised domain adaptation (UDA) transfers the knowledge learned from a well-labeled source domain to a different but related target domain where labeled data is unavailable. The majority of existing UDA methods assume that data from the source domain and the target domain are available and complete during training. Thus, the divergence between the two domains can be formulated and minimized. In this paper, we consider a more practical yet challenging UDA setting where either the source domain data or the target domain data are unknown. Conventional UDA methods would fail this setting since the domain divergence is agnostic due to the absence of the source data or the target data. Technically, we investigate UDA from a novel viewâ€”adversarial attackâ€”and tackle the divergence-agnostic adaptive learning problem in a uniï¬ed framework. Speciï¬cally, we ï¬rst report the motivation of our approach by investigating the inherent relationship between UDA and adversarial attacks. Then we elaborately design adversarial examples to attack the training model and harness these adversarial examples. We argue that the generalization ability of the model would be signiï¬cantly improved if it can defend against our attack, so as to improve the performance on the target domain. Theoretically, we analyze the generalization bound for our method based on domain adaptation theories. Extensive experimental results on multiple UDA benchmarks under conventional, source-absent and target-absent UDA settings verify that our method is able to achieve a favorable performance compared with previous ones. Notably, this work extends the scope of both domain adaptation and adversarial attack, and expected to inspire more ideas in the community.},
	language = {en},
	number = {11},
	urldate = {2025-02-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Li, Jingjing and Du, Zhekai and Zhu, Lei and Ding, Zhengming and Lu, Ke and Shen, Heng Tao},
	month = nov,
	year = {2022},
	note = {TLDR: The generalization ability of the model would be significantly improved if it can defend against the authors' attack, so as to improve the performance on the target domain, and theoretically, the generalization bound for the method is analyzed based on domain adaptation theories.},
	keywords = {citationNumber: 95},
	pages = {8196--8211},
}

@inproceedings{lu_flexible_2019,
	address = {Nice France},
	title = {Flexible online multi-modal hashing for large-scale multimedia retrieval},
	isbn = {978-1-4503-6889-6},
	url = {https://dl.acm.org/doi/10.1145/3343031.3350999},
	doi = {10.1145/3343031.3350999},
	abstract = {Multi-modal hashing fuses multi-modal features at both offline training and online query stage for compact binary hash learning. It has aroused extensive attention in research filed of efficient large-scale multimedia retrieval. However, existing methods adopt batch-based learning scheme or unsupervised learning paradigm. They cannot efficiently handle the very common online streaming multi-modal data (for batch-learning methods), or learn the hash codes suffering from limited discriminative capability and less flexibility for varied streaming data (for existing online multi-modal hashing methods). In this paper, we develop a supervised Flexible Online Multi-modal Hashing (FOMH) method to adaptively fuse heterogeneous modalities and flexibly learn the discriminative hash code for the newly coming data, even if part of the modalities is missing. Specifically, instead of adopting the fixed weights, the modalities weights in FOMH are automatically learned with the proposed flexible multi-modal binary projection to timely capture the variations of streaming samples. Further, we design an efficient asymmetric online supervised hashing strategy to enhance the discriminative capability of the hash codes, while avoiding the challenging symmetric semantic matrix decomposition and storage cost. Moreover, to support fast hash updating and avoid the propagation of binary quantization errors in online learning process, we propose to directly update the hash codes with an efficient discrete online optimization. Experiments on several public multimedia retrieval datasets validate the superiority of the proposed method from various aspects.},
	language = {en},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Lu, Xu and Zhu, Lei and Cheng, Zhiyong and Li, Jingjing and Nie, Xiushan and Zhang, Huaxiang},
	month = oct,
	year = {2019},
	note = {GSCC: 0000133 2025-02-27T12:41:53.166Z 
TLDR: This paper develops a supervised Flexible Online Multi-modal Hashing method to adaptively fuse heterogeneous modalities and flexibly learn the discriminative hash code for the newly coming data, even if part of the modalities is missing.},
	keywords = {ccfInfo: CCF-A ACM MM, citationNumber: Not Found},
	pages = {1129--1137},
}

@article{zhu_v_2015,
	title = {v},
	volume = {17},
	doi = {10.1109/TMM.2015.2431496},
	language = {en},
	number = {7},
	journal = {IEEE Trans. Multim.},
	author = {Zhu, Lei and Shen, Jialie and Jin, Hai and Xie, Liang and Zheng, Ran},
	year = {2015},
	note = {GSCC: 0000059 2025-02-27T12:42:05.305Z
TLDR: A novel and effective feature representation, called hierarchical multi-modal exemplar (HMME) feature, is proposed to characterize landmark images and demonstrates the superior performance of the proposed approach over several state-of-the-art techniques.},
	pages = {981--993},
}

@article{zheng_fast_2020,
	title = {Fast discrete collaborative multi-modal hashing for large-scale multimedia retrieval},
	volume = {32},
	doi = {10.1109/TKDE.2019.2913388},
	language = {en},
	number = {11},
	journal = {IEEE Trans. Knowl. Data Eng.},
	author = {Zheng, Chaoqun and Zhu, Lei and Lu, Xu and Li, Jingjing and Cheng, Zhiyong and Zhang, Hanwang},
	year = {2020},
	note = {TLDR: Experiments on several public multimedia retrieval datasets demonstrate the superiority of the proposed approach compared with state-of-the-art hashing techniques, in terms of both model learning efficiency and retrieval accuracy.},
	pages = {2171--2184},
}

@article{wu_multi-modal_2021,
	title = {Multi-modal discrete tensor decomposition hashing for efficient multimedia retrieval},
	volume = {465},
	url = {https://doi.org/10.1016/j.neucom.2021.08.125},
	doi = {10.1016/J.NEUCOM.2021.08.125},
	language = {en-US},
	urldate = {2025-02-27},
	journal = {Neurocomputing},
	author = {Wu, Xize and Zhu, Lei and Xie, Liang and Zhang, Zheng and Zhang, Huaxiang},
	year = {2021},
	note = {GSCC: 0000004 2025-02-27T12:42:26.656Z
TLDR: An efficient unsupervised Multi-modal Discrete Tensor Decomposition Hashing (MDTDH) approach is proposed to solve the above problems and obtains superior performance than state-of-the-art un Supervised baselines, and even defeats several supervised baselines.},
	pages = {1--14},
}

@inproceedings{xie_dynamic_2017,
	title = {Dynamic {Multi}-{View} {Hashing} for {Online} {Image} {Retrieval}},
	url = {https://doi.org/10.24963/ijcai.2017/437},
	doi = {10.24963/IJCAI.2017/437},
	language = {en-US},
	urldate = {2025-02-27},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI} 2017, {Melbourne}, {Australia}, {August} 19-25, 2017},
	publisher = {ijcai.org},
	author = {Xie, Liang and Shen, Jialie and Han, Jungong and Zhu, Lei and Shao, Ling},
	editor = {Sierra, Carles},
	year = {2017},
	note = {TLDR: This paper proposes dynamic multi-view hashing (DMVH), which can adaptively augment hash codes according to dynamic changes of image, and demonstrates superior performance of DWVH over several state-of-the-art hashing methods.},
	pages = {3133--3139},
}

@inproceedings{wang_shapley_2024,
	title = {Shapley ensemble adversarial attack},
	doi = {10.1109/ICME57554.2024.10687833},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Multimedia} and {Expo}, {Icme} 2024, {Niagara} {Falls}, {On}, {Canada}, {July} 15-19, 2024},
	publisher = {IEEE},
	author = {Wang, Zheng and Tang, Bowen and Bin, Yi and Zhu, Lei and Wang, Guoqing and Yang, Yang},
	year = {2024},
	note = {TLDR: A novel Shapley Ensemble Adversarial Attack (dubbed SEAA) is proposed â€“ an effective algorithm that allocates weights based on Shapley values of the surrogates to accurately evaluate the contribution of a surrogate by reweighing its importance at each iteration, thus avoiding local optimality and steering the generation of adversarial examples.},
	pages = {1--6},
}

@article{qu_aamt_2024,
	title = {{AAMT}: {Adversarial} {Attack}-{Driven} {Mutual} {Teaching} for {Source}-{Free} {Domain}-{Adaptive} {Person} {Reidentification}},
	volume = {26},
	shorttitle = {{AAMT}},
	doi = {10.1109/TMM.2024.3379096},
	journal = {IEEE Trans. Multim.},
	author = {Qu, Xiaofeng and Zhang, Huaxiang and Zhu, Lei and Nie, Liqiang and Liu, Li},
	year = {2024},
	note = {TLDR: This work introduces an adversarial attacks-driven mutual teaching (AAMT) framework as an innovative and applicable source-free DA person ReID scheme, and designs a contrastive learning loss to enlarge the differences between the training pairs and further mitigate the mutual convergence issue.},
	pages = {8255--8267},
}

@article{xu_multi-modal_2023,
	title = {Multi-modal discrete collaborative filtering for efficient cold-start recommendation},
	volume = {35},
	doi = {10.1109/TKDE.2021.3079581},
	language = {en},
	number = {1},
	journal = {IEEE Trans. Knowl. Data Eng.},
	author = {Xu, Yang and Zhu, Lei and Cheng, Zhiyong and Li, Jingjing and Zhang, Zheng and Zhang, Huaxiang},
	year = {2023},
	note = {GSCC: 0000042 2025-02-27T12:39:29.666Z
TLDR: A Multi-modal Discrete Collaborative Filtering (MDCF) for efficient cold-start recommendation is proposed, which map the multi- modal features of users and items to a consensus Hamming space based on the matrix factorization framework to support large-scale recommendation.},
	pages = {741--755},
}

@article{zhu_flexible_2020,
	title = {Flexible multi-modal hashing for scalable multimedia retrieval},
	volume = {11},
	doi = {10.1145/3365841},
	language = {en},
	number = {2},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Zhu, Lei and Lu, Xu and Cheng, Zhiyong and Li, Jingjing and Zhang, Huaxiang},
	year = {2020},
	note = {GSCC: 0000064 2025-02-27T12:40:16.036Z
TLDR: A novel Flexible Multi-modal Hashing method that learns multiple modality-specific hash codes and multi- modality collaborative hash codes simultaneously within a single model to address the problem of binarize queries when only one or part of modalities are provided.},
	pages = {14:1--14:20},
}

@inproceedings{xie_multi-task_2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-task multi-modal semantic hashing for web image retrieval with limited supervision},
	volume = {10132},
	url = {https://doi.org/10.1007/978-3-319-51811-4\_38},
	doi = {10.1007/978-3-319-51811-4_38},
	language = {en},
	urldate = {2025-02-27},
	booktitle = {Multimedia {Modeling} - 23rd {International} {Conference}, {Mmm} 2017, {Reykjavik}, {Iceland}, {January} 4-6, 2017, {Proceedings}, {Part} {I}},
	publisher = {Springer},
	author = {Xie, Liang and Zhu, Lei and Cheng, Zhiyong},
	editor = {Amsaleg, Laurent and GuÃ°mundsson, Gylfi ÃÃ³r and Gurrin, Cathal and JÃ³nsson, BjÃ¶rn ÃÃ³r and Satoh, Shin'ichi},
	year = {2017},
	note = {GSCC: 0000001 2025-02-27T12:42:06.841Z 
TLDR: Multi-Task Multi-modal Semantic Hashing (MTMSH) is proposed to index large scale social image data collection with limited supervision and improves search accuracy via improving more semantic information from two aspects.},
	pages = {465--477},
}

@article{zhu_discrete_2017,
	title = {Discrete multi-modal hashing with canonical views for robust mobile landmark search},
	volume = {abs/1707.04047},
	url = {http://arxiv.org/abs/1707.04047},
	language = {en},
	urldate = {2025-02-27},
	journal = {Corr},
	author = {Zhu, Lei and Huang, Zi and Liu, Xiaobai and He, Xiangnan and Song, Jingkuan and Zhou, Xiaofang},
	year = {2017},
	note = {GSCC: 0000131 2025-02-27T12:41:58.802Z 
arXiv: 1707.04047},
	keywords = {linter/error},
}

@article{zheng_efficient_2020,
	title = {Efficient parameter-free adaptive multi-modal hashing},
	volume = {27},
	doi = {10.1109/LSP.2020.3008335},
	language = {en},
	journal = {IEEE Signal Process. Lett.},
	author = {Zheng, Chaoqun and Zhu, Lei and Zhang, Shusen and Zhang, Huaxiang},
	year = {2020},
	note = {GSCC: 0000026 2025-02-27T12:40:22.659Z 
TLDR: This letter proposes an unsupervised Efficient Parameter-free Adaptive Multi-modal Hashing (EPAMH) model to adaptively capture the modality variations and preserve the discriminative semantics of multi- modal features into the binary hash codes.},
	pages = {1270--1274},
}

@inproceedings{lu_graph_2021,
	title = {Graph convolutional multi-modal hashing for flexible multimedia retrieval},
	doi = {10.1145/3474085.3475598},
	language = {en},
	booktitle = {Mm '21: {ACM} {Multimedia} {Conference}, {Virtual} {Event}, {China}, {October} 20 - 24, 2021},
	publisher = {ACM},
	author = {Lu, Xu and Zhu, Lei and Liu, Li and Nie, Liqiang and Zhang, Huaxiang},
	editor = {Shen, Heng Tao and Zhuang, Yueting and Smith, John R. and Yang, Yang and CÃ©sar, Pablo and Metze, Florian and Prabhakaran, Balakrishnan},
	year = {2021},
	note = {GSCC: 0000052 2025-02-27T12:40:11.190Z 
TLDR: This paper proposes a Flexible Graph Convolutional Multi-modal Hashing method that adopts GCNs with linear complexity to preserve both the modality-individual and modalities-fused structural similarity for discriminative hash learning.},
	pages = {1414--1422},
}

@misc{hu_video_2024,
	title = {Video prediction policy: a generalist robot policy with predictive visual representations},
	shorttitle = {Video prediction policy},
	url = {http://arxiv.org/abs/2412.14803},
	doi = {10.48550/arXiv.2412.14803},
	abstract = {Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1{\textbackslash}\% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8{\textbackslash}\% increase in success rates for complex real-world dexterous manipulation tasks.},
	language = {en},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Hu, Yucheng and Guo, Yanjiang and Wang, Pengchao and Chen, Xiaoyu and Wang, Yen-Jen and Zhang, Jianke and Sreenath, Koushil and Lu, Chaochao and Chen, Jianyu},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14803 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{xie_online_2016,
	title = {Online cross-modal hashing for web image retrieval},
	url = {https://doi.org/10.1609/aaai.v30i1.9982},
	doi = {10.1609/AAAI.V30I1.9982},
	language = {en},
	urldate = {2025-02-27},
	booktitle = {Proceedings of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}, {February} 12-17, 2016, {Phoenix}, {Arizona}, {USA}},
	publisher = {AAAI Press},
	author = {Xie, Liang and Shen, Jialie and Zhu, Lei},
	editor = {Schuurmans, Dale and Wellman, Michael P.},
	year = {2016},
	note = {TLDR: This paper proposes Online Cross-modal Hashing (OCMH) which can effectively address the above two problems by learning the shared latent codes (SLC) of hash codes and dynamic transfer matrix, and demonstrates the effectiveness and efficiency of OCMH for online cross- modal web image retrieval.},
	pages = {294--300},
}

@misc{wen_tinyvla_2024,
	title = {{TinyVLA}: towards fast, data-efficient vision-language-action models for robotic manipulation},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{TinyVLA}},
	url = {https://arxiv.org/abs/2409.12514},
	doi = {10.48550/ARXIV.2409.12514},
	abstract = {Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that {\textbackslash}methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.},
	language = {en},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and Tang, Jian},
	year = {2024},
	note = {Version Number: 4
TLDR: This paper introduces a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: faster inference speeds, and improved data efficiency, eliminating the need for pre-training stage.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Robotics (cs.RO), ccfInfo: Net Error: 0, citationNumber: 0},
}

@inproceedings{su_cross-domain_2023,
	title = {Cross-domain adaptative learning for online advertisement customer lifetime value prediction},
	url = {https://doi.org/10.1609/aaai.v37i4.25583},
	doi = {10.1609/AAAI.V37I4.25583},
	language = {en},
	urldate = {2025-02-27},
	booktitle = {Thirty-seventh {Aaai} {Conference} on {Artificial} {Intelligence}, {Aaai} 2023, {Thirty}-fifth {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence}, {Iaai} 2023, {Thirteenth} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {Eaai} 2023, {Washington}, {Dc}, {Usa}, {February} 7-14, 2023},
	publisher = {AAAI Press},
	author = {Su, Hongzu and Du, Zhekai and Li, Jingjing and Zhu, Lei and Lu, Ke},
	editor = {Williams, Brian and Chen, Yiling and Neville, Jennifer},
	year = {2023},
	note = {TLDR: A novel cross-domain adaptative framework (CDAF) to leverage consumption data from different domains and design a dual-predictor schema which not only enhances domain-invariant information in the semantic space but also preserves domain-specific information for accurate target prediction.},
	pages = {4605--4613},
}

@article{zhu_exploring_2018,
	title = {Exploring {Auxiliary} {Context}: {Discrete} {Semantic} {Transfer} {Hashing} for {Scalable} {Image} {Retrieval}},
	volume = {29},
	issn = {2162-2388},
	shorttitle = {Exploring {Auxiliary} {Context}},
	url = {https://ieeexplore.ieee.org/abstract/document/8291840},
	doi = {10.1109/TNNLS.2018.2797248},
	abstract = {Unsupervised hashing can desirably support scalable content-based image retrieval for its appealing advantages of semantic label independence, memory, and search efficiency. However, the learned hash codes are embedded with limited discriminative semantics due to the intrinsic limitation of image representation. To address the problem, in this paper, we propose a novel hashing approach, dubbed as discrete semantic transfer hashing (DSTH). The key idea is to directly augment the semantics of discrete image hash codes by exploring auxiliary contextual modalities. To this end, a unified hashing framework is formulated to simultaneously preserve visual similarities of images and perform semantic transfer from contextual modalities. Furthermore, to guarantee direct semantic transfer and avoid information loss, we explicitly impose the discrete constraint, bit-uncorrelation constraint, and bit-balance constraint on hash codes. A novel and effective discrete optimization method based on augmented Lagrangian multiplier is developed to iteratively solve the optimization problem. The whole learning process has linear computation complexity and desirable scalability. Experiments on three benchmark data sets demonstrate the superiority of DSTH compared with several state-of-the-art approaches.},
	number = {11},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhu, Lei and Huang, Zi and Li, Zhihui and Xie, Liang and Shen, Heng Tao},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems
TLDR: This paper proposes a novel hashing approach, dubbed as discrete semantic transfer hashing (DSTH), to directly augment the semantics of discrete image hash codes by exploring auxiliary contextual modalities and guarantees direct semantic transfer and avoid information loss.},
	keywords = {Content-based image retrieval, Image retrieval, Optimization, Scalability, Semantics, Training, Transforms, Visualization, ccfInfo: CCF-B TNNLS, citationNumber: 187, discrete optimization, semantic transfer, unsupervised hashing, visual similarities},
	pages = {5264--5276},
}

@article{cui_scalable_2020,
	title = {Scalable deep hashing for large-scale social image retrieval},
	volume = {29},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/abstract/document/8839750},
	doi = {10.1109/TIP.2019.2940693},
	abstract = {Recent years have witnessed the wide application of hashing for large-scale image retrieval, because of its high computation efficiency and low storage cost. Particularly, benefiting from current advances in deep learning, supervised deep hashing methods have greatly boosted the retrieval performance, under the strong supervision of large amounts of manually annotated semantic labels. However, their performance is highly dependent upon the supervised labels, which significantly limits the scalability. In contrast, unsupervised deep hashing without label dependence enjoys the advantages of well scalability. Nevertheless, due to the relaxed hash optimization, and more importantly, the lack of semantic guidance, existing methods suffer from limited retrieval performance. In this paper, we propose a SCAlable Deep Hashing (SCADH) to learn enhanced hash codes for social image retrieval. We formulate a unified scalable deep hash learning framework which explores the weak but free supervision of discriminative user tags that are commonly accompanied with social images. It jointly learns image representations and hash functions with deep neural networks, and simultaneously enhances the discriminative capability of image hash codes with the refined semantics from the accompanied social tags. Further, instead of simple relaxed hash optimization, we propose a discrete hash optimization method based on Augmented Lagrangian Multiplier to directly solve the hash codes and avoid the binary quantization information loss. Experiments on two standard social image datasets demonstrate the superiority of the proposed approach compared with state-of-the-art shallow and deep hashing techniques.},
	language = {en},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Image Processing},
	author = {Cui, Hui and Zhu, Lei and Li, Jingjing and Yang, Yang and Nie, Liqiang},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Image Processing
TLDR: This paper proposes a unified scalable deep hash learning framework which explores the weak but free supervision of discriminative user tags that are commonly accompanied with social images and proposes a discrete hash optimization method based on Augmented Lagrangian Multiplier to directly solve the hash codes and avoid the binary quantization information loss.},
	keywords = {Hash functions, Image retrieval, Optimization, Quantization (signal), Scalability, Semantics, Unsupervised deep hashing, Visualization, augmented Lagrangian multiplier, ccfInfo: CCF-A TIP, citationNumber: 146, discrete optimization, social tags},
	pages = {1271--1284},
}

@article{dong_unsupervised_2021,
	title = {Unsupervised deep {K}-means hashing for efficient image retrieval and clustering},
	volume = {31},
	issn = {1558-2205},
	url = {https://ieeexplore.ieee.org/abstract/document/9248040},
	doi = {10.1109/TCSVT.2020.3035775},
	abstract = {Recent studies show that hashing technology can achieve efficient similarity searching and many works have been done on supervised deep hash learning. However, under unsupervised scenarios, there are several issues to be solved when learning hashing codes based on visual features for image retrieval and clustering. In this article, we propose a simple but effective Unsupervised Deep K-means Hashing (UDKH) method to simultaneously alleviate the problems of image retrieval and clustering within a single learning framework. UDKH progressively improves the quality of cluster labels and binary hash codes by minimizing pair-wise supervision loss and optimizing the binary K-means to generate discriminative hash codes under the supervision of the learned cluster labels for effective image retrieval. Since the learned hash codes are discriminative, UDKH also improves the image clustering accuracy. Experiments on test datasets demonstrate its effectiveness for image retrieval and clustering.},
	language = {en},
	number = {8},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Dong, Xiao and Liu, Li and Zhu, Lei and Cheng, Zhiyong and Zhang, Huaxiang},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology
TLDR: A simple but effective Unsupervised Deep K-means Hashing (UDKH) method to simultaneously alleviate the problems of image retrieval and clustering within a single learning framework.},
	keywords = {Binary codes, Correlation, Hash functions, Image retrieval, Quantization (signal), Semantics, Visualization, ccfInfo: CCF-B TCSVT, citationNumber: 30, clustering, deep learning, unsupervised hashing},
	pages = {3266--3277},
}

@article{zhu_dual-level_2021,
	title = {Dual-level semantic transfer deep hashing for efficient social image retrieval},
	volume = {31},
	issn = {1558-2205},
	url = {https://ieeexplore.ieee.org/abstract/document/9115089},
	doi = {10.1109/TCSVT.2020.3001583},
	abstract = {Social network stores and disseminates a tremendous amount of user shared images. Deep hashing is an efficient indexing technique to support large-scale social image retrieval, due to its deep representation capability, fast retrieval speed and low storage cost. Particularly, unsupervised deep hashing has well scalability as it does not require any manually labelled data for training. However, owing to the lacking of label guidance, existing methods suffer from severe semantic shortage when optimizing a large amount of deep neural network parameters. Differently, in this paper, we propose a Dual-level Semantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a unified deep hash learning framework. Our model targets at learning the semantically enhanced deep hash codes by specially exploiting the user-generated tags associated with the social images. Specifically, we design a complementary dual-level semantic transfer mechanism to efficiently discover the potential semantics of tags and seamlessly transfer them into binary hash codes. On the one hand, instance-level semantics are directly preserved into hash codes from the associated tags with adverse noise removing. Besides, an image-concept hypergraph is constructed for indirectly transferring the latent high-order semantic correlations of images and tags into hash codes. Moreover, the hash codes are obtained simultaneously with the deep representation learning by the discrete hash optimization strategy. Extensive experiments on two public social image retrieval datasets validate the superior performance of our method compared with state-of-the-art hashing methods. The source codes of our method can be obtained at https://github.com/research2020-1/DSTDH},
	language = {en},
	number = {4},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Zhu, Lei and Cui, Hui and Cheng, Zhiyong and Li, Jingjing and Zhang, Zheng},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology
TLDR: This paper designs a complementary dual-level semantic transfer mechanism to efficiently discover the potential semantics of tags and seamlessly transfer them into binary hash codes and achieves superior performance compared with state-of-the-art hashing methods.},
	keywords = {Hash functions, Image retrieval, Neural networks, Optimization, Quantization (signal), Semantics, Social image retrieval, Visualization, ccfInfo: CCF-B TCSVT, citationNumber: 26, fast discrete optimization, unsupervised deep hashing},
	pages = {1478--1489},
}

@article{cui_adversarial_2024,
	title = {Adversarial source generation for source-free domain adaptation},
	volume = {34},
	issn = {1558-2205},
	url = {https://ieeexplore.ieee.org/document/10335732},
	doi = {10.1109/TCSVT.2023.3337796},
	abstract = {Unsupervised domain adaptation aims to transfer the knowledge learned from a labeled source domain to an unlabeled target domain with different data distributions. However, in practice, source samples are not always available due to privacy protection and storage resource limitations. To address this concern, Source-Free Domain Adaptation (SFDA) has recently attracted growing research attention, as it only needs a pre-trained source model without direct access to source data. In this paper, we propose a novel Adversarial SOurce GEneration (ASOGE) method for SFDA, which introduces an additional generative module to produce synthetic labeled source samples and uses them to facilitate cross-domain adaptation. Unlike early studies that train the generator independently and perform the adaptation only after the generator is finished, ASOGE integrates the generation and adaptation stages within a collaborative framework by making them play an adversarial game. In the generation stage, the labeled source samples are not produced blindly; instead, they are hard-to-align samples that provide knowledge more worth learning for the adaptation stage. To achieve a fine-grained domain alignment, a class-aware discrepancy between source and target domains is measured via contrastive learning. Extensive experiments on benchmark datasets demonstrate the effectiveness of ASOGE compared to the state-of-the-art methods.},
	language = {en},
	number = {6},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Cui, Chaoran and Meng, Fan'an and Zhang, Chunyun and Liu, Ziyi and Zhu, Lei and Gong, Shuai and Lin, Xue},
	month = jun,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology
TLDR: A novel Adversarial SOurce GEneration (ASOGE) method is proposed, which introduces an additional generative module to produce synthetic labeled source samples and uses them to facilitate cross-domain adaptation.},
	keywords = {Adaptation models, Collaboration, Feature extraction, Games, Generators, Measurement, Source-free domain adaptation, Training, adversarial training, class-aware domain discrepancy, contrastive learning},
	pages = {4887--4898},
}

@misc{li_robonurse-vla_2024,
	title = {{RoboNurse}-{VLA}: robotic scrub nurse system based on vision-language-action model},
	shorttitle = {{RoboNurse}-{VLA}},
	url = {http://arxiv.org/abs/2409.19590},
	doi = {10.48550/arXiv.2409.19590},
	abstract = {In modern healthcare, the demand for autonomous robotic assistants has grown significantly, particularly in the operating room, where surgical tasks require precision and reliability. Robotic scrub nurses have emerged as a promising solution to improve efficiency and reduce human error during surgery. However, challenges remain in terms of accurately grasping and handing over surgical instruments, especially when dealing with complex or difficult objects in dynamic environments. In this work, we introduce a novel robotic scrub nurse system, RoboNurse-VLA, built on a Vision-Language-Action (VLA) model by integrating the Segment Anything Model 2 (SAM 2) and the Llama 2 language model. The proposed RoboNurse-VLA system enables highly precise grasping and handover of surgical instruments in real-time based on voice commands from the surgeon. Leveraging state-of-the-art vision and language models, the system can address key challenges for object detection, pose optimization, and the handling of complex and difficult-to-grasp instruments. Through extensive evaluations, RoboNurse-VLA demonstrates superior performance compared to existing models, achieving high success rates in surgical instrument handovers, even with unseen tools and challenging items. This work presents a significant step forward in autonomous surgical assistance, showcasing the potential of integrating VLA models for real-world medical applications. More details can be found at https://robonurse-vla.github.io.},
	language = {en},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Li, Shunlei and Wang, Jin and Dai, Rui and Ma, Wanyu and Ng, Wing Yin and Hu, Yingbai and Li, Zheng},
	month = sep,
	year = {2024},
	note = {arXiv:2409.19590 [cs]
TLDR: This work introduces a novel robotic scrub nurse system, RoboNurse-VLA, built on a Vision-Language-Action (VLA) model by integrating the Segment Anything Model 2 (SAM 2) and the Llama 2 language model.},
	keywords = {Computer Science - Robotics},
}

@misc{wen_diffusion-vla_2024,
	title = {Diffusion-{VLA}: {Scaling} {Robot} {Foundation} {Models} via {Unified} {Diffusion} and {Autoregression}},
	shorttitle = {Diffusion-{VLA}},
	url = {http://arxiv.org/abs/2412.03293},
	doi = {10.48550/arXiv.2412.03293},
	abstract = {In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy. Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations. Subsequently, a diffusion model is attached to generate robust action outputs. To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process. The whole framework is simple and flexible, making it easy to deploy and upgrade. We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA. Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training. We observe that the reasoning module makes the model interpretable. It allows observers to understand the model thought process and identify potential causes of policy failures. Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7{\textbackslash}\% accuracy on 102 previously unseen objects. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability. Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Wen, Junjie and Zhu, Minjie and Zhu, Yichen and Tang, Zhibin and Li, Jinming and Zhou, Zhongyi and Li, Chengmeng and Liu, Xiaoyu and Peng, Yaxin and Shen, Chaomin and Feng, Feifei},
	month = dec,
	year = {2024},
	note = {arXiv:2412.03293 [cs]
TLDR: This paper presents DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy, and introduces a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{yang_mma-diffusion_2024,
	address = {Seattle, WA, USA},
	title = {{MMA}-diffusion: {MultiModal} attack on diffusion models},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5300-6},
	shorttitle = {{MMA}-diffusion},
	url = {https://ieeexplore.ieee.org/document/10657937/},
	doi = {10.1109/CVPR52733.2024.00739},
	abstract = {In recent years, Text-to-Image (T2I) models have seen remarkable advancements, gaining widespread adoption. However, this progress has inadvertently opened avenues for potential misuse, particularly in generating inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces MMA-Diffusion, a framework that presents a signiï¬cant and realistic threat to the security of T2I models by effectively circumventing current defensive measures in both open-source models and commercial online services. Unlike previous approaches, MMA-Diffusion leverages both textual and visual modalities to bypass safeguards like prompt ï¬lters and post-hoc safety checkers, thus exposing and highlighting the vulnerabilities in existing defense mechanisms. Our codes are available at https: //github.com/cure-lab/MMA-Diffusion.},
	language = {en},
	urldate = {2025-02-19},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Yijun and Gao, Ruiyuan and Wang, Xiaosen and Ho, Tsung-Yi and Xu, Nan and Xu, Qiang},
	month = jun,
	year = {2024},
	note = {TLDR: This work introduces MMA-Diffusion, a framework that presents a significant and realistic threat to the security of T2I models by effectively circumventing current defensive measures in both open-source models and commercial online services.},
	keywords = {ccfInfo: CCF-A CVPR, citationNumber: 7},
	pages = {7737--7746},
}

@article{guo_efficient_2024,
	title = {Efficient generation of targeted and transferable adversarial examples for vision-language models via diffusion models},
	issn = {1556-6021},
	url = {https://ieeexplore.ieee.org/abstract/document/10812818},
	doi = {10.1109/TIFS.2024.3518072},
	abstract = {Adversarial attacks, particularly targeted transfer-based attacks, can be used to assess the adversarial robustness of large visual-language models (VLMs), allowing for a more thorough examination of potential security flaws before deployment. However, previous transfer-based adversarial attacks incur high costs due to high iteration counts and complex method structure. Furthermore, due to the unnaturalness of adversarial semantics, the generated adversarial examples have low transferability. These issues limit the utility of existing methods for assessing robustness. To address these issues, we propose AdvDiffVLM, which uses diffusion models to generate natural, unrestricted and targeted adversarial examples via score matching. Specifically, AdvDiffVLM uses Adaptive Ensemble Gradient Estimation (AEGE) to modify the score during the diffusion modelâ€™s reverse generation process, ensuring that the produced adversarial examples have natural adversarial targeted semantics, which improves their transferability. Simultaneously, to improve the quality of adversarial examples, we use the GradCAM-guided Mask Generation (GCMG) to disperse adversarial semantics throughout the image rather than concentrating them in a single area. Finally, AdvDiffVLM embeds more target semantics into adversarial examples after multiple iterations. Experimental results show that our method generates adversarial examples 5x to 10x faster than state-of-the-art (SOTA) transfer-based adversarial attacks while maintaining higher quality adversarial examples. Furthermore, compared to previous transfer-based adversarial attacks, the adversarial examples generated by our method have better transferability. Notably, AdvDiffVLM can successfully attack a variety of commercial VLMs in a black-box environment, including GPT-4V. The code is available at https://github.com/gq-max/AdvDiffVLM.},
	language = {en},
	urldate = {2025-01-07},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Guo, Qi and Pang, Shanmin and Jia, Xiaojun and Liu, Yang and Guo, Qing},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security
TLDR: AdvDiffVLM is proposed, which uses diffusion models to generate natural, unrestricted and targeted adversarial examples via score matching, and can successfully attack a variety of commercial VLMs in a black-box environment, including GPT-4V.
titleTranslation: è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç›®æ ‡åŒ–å’Œå¯ä¼ é€’å¯¹æŠ—æ ·æœ¬çš„æœ‰æ•ˆç”Ÿæˆï¼šæ‰©æ•£æ¨¡å‹
abstractTranslation: å¯¹æŠ—æ€§æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è½¬ç§»çš„æ”»å‡»ï¼Œå¯ä»¥ç”¨æ¥è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¯¹æŠ—ç¨³å¥æ€§ï¼Œåœ¨éƒ¨ç½²ä¹‹å‰å¯æ›´å½»åº•åœ°æ£€æŸ¥æ½œåœ¨çš„å®‰å…¨æ¼æ´ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„åŸºäºè½¬ç§»çš„å¯¹æŠ—æ€§æ”»å‡»ç”±äºè¿­ä»£æ¬¡æ•°å¤šä¸”æ–¹æ³•ç»“æ„å¤æ‚è€Œé€ æˆæˆæœ¬é«˜ã€‚æ­¤å¤–ï¼Œç”±äºå¯¹æŠ—æ€§è¯­ä¹‰çš„ä¸è‡ªç„¶æ€§ï¼Œç”Ÿæˆçš„å¯¹æŠ—æ€§ç¤ºä¾‹çš„å¯è½¬ç§»æ€§è¾ƒä½ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†ç°æœ‰æ–¹æ³•è¯„ä¼°ç¨³å¥æ€§çš„æ•ˆç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdvDiffVLMï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ‰©æ•£æ¨¡å‹é€šè¿‡è¯„åˆ†åŒ¹é…ç”Ÿæˆè‡ªç„¶ã€æ— é™åˆ¶å’Œæœ‰é’ˆå¯¹æ€§çš„å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚å…·ä½“è€Œè¨€ï¼ŒAdvDiffVLMä½¿ç”¨è‡ªé€‚åº”é›†æˆæ¢¯åº¦ä¼°è®¡ï¼ˆAEGEï¼‰æ¥åœ¨æ‰©æ•£æ¨¡å‹çš„åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿®æ”¹è¯„åˆ†ï¼Œç¡®ä¿ç”Ÿæˆçš„å¯¹æŠ—æ€§ç¤ºä¾‹å…·æœ‰è‡ªç„¶çš„å¯¹æŠ—æ€§æœ‰é’ˆå¯¹æ€§çš„è¯­ä¹‰ï¼Œä»è€Œæé«˜å…¶å¯è½¬ç§»æ€§ã€‚åŒæ—¶ï¼Œä¸ºäº†æ”¹å–„å¯¹æŠ—æ€§ç¤ºä¾‹çš„è´¨é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨GradCAMå¼•å¯¼æ©è†œç”Ÿæˆï¼ˆGCMGï¼‰å°†å¯¹æŠ—æ€§è¯­ä¹‰åˆ†æ•£åˆ°å›¾åƒä¸­çš„å„ä¸ªåŒºåŸŸï¼Œè€Œä¸æ˜¯é›†ä¸­åœ¨ä¸€ä¸ªåŒºåŸŸã€‚æœ€åï¼ŒAdvDiffVLMåœ¨å¤šæ¬¡è¿­ä»£åå°†æ›´å¤šç›®æ ‡è¯­ä¹‰åµŒå…¥å¯¹æŠ—æ€§ç¤ºä¾‹ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§ç¤ºä¾‹æ¯”æœ€å…ˆè¿›çš„åŸºäºè½¬ç§»çš„å¯¹æŠ—æ”»å‡»å¿«5å€åˆ°10å€ï¼ŒåŒæ—¶ä¿æŒæ›´é«˜è´¨é‡çš„å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚æ­¤å¤–ï¼Œä¸ä»¥å¾€çš„åŸºäºè½¬ç§»çš„å¯¹æŠ—æ€§æ”»å‡»ç›¸æ¯”ï¼Œæˆ‘ä»¬æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§ç¤ºä¾‹å…·æœ‰æ›´å¥½çš„å¯è½¬ç§»æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAdvDiffVLMåœ¨é»‘ç›’ç¯å¢ƒä¸­æˆåŠŸæ”»å‡»äº†å„ç§å•†ç”¨VLMsï¼ŒåŒ…æ‹¬GPT-4Vã€‚è¯¥ä»£ç å¯åœ¨https://github.com/gq-max/AdvDiffVLMæ‰¾åˆ°ã€‚},
	keywords = {Adaptation models, Adversarial Attack, Closed box, Diffusion Models, Diffusion models, Electronic mail, Focusing, Glass box, Image quality, Noise, Robustness, Score Matching, Semantics, Visual Language Models, ccfInfo: Not Found, citationNumber: 0, â­â­â­},
	pages = {1--1},
}

@inproceedings{chen_advdiffuser_2023,
	address = {Paris, France},
	title = {{AdvDiffuser}: natural adversarial example synthesis with diffusion models},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0718-4},
	shorttitle = {{AdvDiffuser}},
	url = {https://ieeexplore.ieee.org/document/10377423/},
	doi = {10.1109/ICCV51070.2023.00421},
	language = {en},
	urldate = {2025-02-19},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Chen, Xinquan and Gao, Xitong and Zhao, Juanjuan and Ye, Kejiang and Xu, Cheng-Zhong},
	month = oct,
	year = {2023},
	note = {TLDR: AdvDiffuser is proposed, a new method for synthesizing natural adversarial examples (UAEs) using diffusion models that are not only more natural but also stronger compared to the current state-of-the-art attacks.},
	keywords = {citationNumber: 5},
	pages = {4539--4549},
}

@inproceedings{xu_highly_2024,
	address = {Melbourne VIC Australia},
	title = {Highly transferable diffusion-based unrestricted adversarial attack on pre-trained vision-language models},
	isbn = {979-8-4007-0686-8},
	url = {https://dl.acm.org/doi/10.1145/3664647.3681538},
	doi = {10.1145/3664647.3681538},
	language = {en},
	urldate = {2025-02-19},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Xu, Wenzhuo and Chen, Kai and Gao, Ziyi and Wei, Zhipeng and Chen, Jingjing and Jiang, Yu-Gang},
	month = oct,
	year = {2024},
	keywords = {ccfInfo: CCF-A ACM MM, citationNumber: 0},
	pages = {748--757},
}

@misc{xiao_generating_2019,
	title = {Generating adversarial examples with adversarial networks},
	url = {http://arxiv.org/abs/1801.02610},
	doi = {10.48550/arXiv.1801.02610},
	abstract = {Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results. Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses. We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly. Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack has placed the first with 92.76\% accuracy on a public MNIST black-box attack challenge.},
	language = {en},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Xiao, Chaowei and Li, Bo and Zhu, Jun-Yan and He, Warren and Liu, Mingyan and Song, Dawn},
	month = feb,
	year = {2019},
	note = {arXiv:1801.02610 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Statistics - Machine Learning, ccfInfo: CCF-A IJCAI, citationNumber: 1047},
}

@misc{liang_adversarial_2023,
	title = {Adversarial example does good: preventing painting imitation from diffusion models via adversarial examples},
	shorttitle = {Adversarial example does good},
	url = {http://arxiv.org/abs/2302.04578},
	doi = {10.48550/arXiv.2302.04578},
	abstract = {Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications. The code of our method is available on GitHub: https://github.com/mist-project/mist.git.},
	language = {en},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Liang, Chumeng and Wu, Xiaoyu and Hua, Yang and Zhang, Jiaru and Xue, Yiming and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
	month = jun,
	year = {2023},
	note = {arXiv:2302.04578 [cs]
TLDR: This paper is the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks against infringers equipped with DM-based AI-for-Art applications.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, ccfInfo: CCF-A ICML, citationNumber: 5},
}

@misc{kai_superedge_2024,
	title = {{SuperEdge}: towards a generalization model for self-supervised edge detection},
	shorttitle = {{SuperEdge}},
	url = {http://arxiv.org/abs/2401.02313},
	doi = {10.48550/arXiv.2401.02313},
	abstract = {Edge detection is a fundamental technique in various computer vision tasks. Edges are indeed effectively delineated by pixel discontinuity and can offer reliable structural information even in textureless areas. State-of-the-art heavily relies on pixel-wise annotations, which are labor-intensive and subject to inconsistencies when acquired manually. In this work, we propose a novel self-supervised approach for edge detection that employs a multi-level, multi-homography technique to transfer annotations from synthetic to real-world datasets. To fully leverage the generated edge annotations, we developed SuperEdge, a streamlined yet efficient model capable of concurrently extracting edges at pixel-level and object-level granularity. Thanks to self-supervised training, our method eliminates the dependency on manual annotated edge labels, thereby enhancing its generalizability across diverse datasets. Comparative evaluations reveal that SuperEdge advances edge detection, demonstrating improvements of 4.9\% in ODS and 3.3\% in OIS over the existing STEdge method on BIPEDv2.},
	language = {en},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Kai, Leng and Zhijie, Zhang and Jie, Liu and Boukhers, Zed and Wei, Sui and Yang, Cong and Zhijun, Li},
	month = jan,
	year = {2024},
	note = {arXiv:2401.02313 [cs]
TLDR: A novel self-supervised approach for edge detection that employs a multi-level, multi-homography technique to transfer annotations from synthetic to real-world datasets and eliminates the dependency on manual annotated edge labels, thereby enhancing its generalizability across diverse datasets.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ccfInfo: Net Error: 0, citationNumber: 0},
}

@misc{li_vision-language_2024,
	title = {Vision-language foundation models as effective robot imitators},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	url = {http://arxiv.org/abs/2311.01378},
	doi = {10.48550/arXiv.2311.01378},
	abstract = {Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step visionlanguage comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on languageconditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show that RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. RoboFlamingo can be trained or evaluated on a single GPU server, and we believe it has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy. Codes and models will be public.},
	language = {en},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and Li, Hang and Kong, Tao},
	month = feb,
	year = {2024},
	note = {arXiv:2311.01378 [cs]
TLDR: RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.},
	keywords = {Artificial Intelligence (cs.AI), Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, FOS: Computer and information sciences, Machine Learning (cs.LG), Robotics (cs.RO), ccfInfo: CCF-None ICLR, ccfInfo: Net Error: 0, citationNumber: 6},
}

@misc{boneinscri_canny_2022,
	type = {çŸ¥ä¹ä¸“æ æ–‡ç« },
	title = {è¾¹ç¼˜æ£€æµ‹å…¸ä¸­å…¸â€”â€”{Canny} ç®—æ³•},
	url = {https://zhuanlan.zhihu.com/p/494567705},
	abstract = {è¯ä¸å¤šè¯´ï¼Œå…ˆæŠ›å‡ºç®—æ³•çš„æ­¥éª¤ï¼šä½¿ç”¨é«˜æ–¯æ»¤æ³¢å™¨æ¥å¹³æ»‘å›¾åƒï¼Œè¾¾åˆ°æ»¤é™¤å™ªå£°çš„æ•ˆæœã€‚(é™å™ª)è®¡ç®—å›¾åƒä¸­æ¯ä¸ªåƒç´ ç‚¹çš„æ¢¯åº¦å¤§å°å’Œæ–¹å‘ï¼ˆæ±‚æ¢¯åº¦ï¼‰ä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶ï¼Œæ¶ˆé™¤è¾¹ç¼˜æ£€æµ‹å¸¦æ¥çš„ä¸åˆ©å½±å“ï¼ˆéæå¤§æŠ‘åˆ¶ï¼‰ä½¿ç”¨åŒåŸŸå€¼æ³•ç›‘â€¦},
	language = {zh},
	urldate = {2025-02-17},
	journal = {å›¾åƒå·¥ç¨‹},
	author = {{BoneInscri}},
	month = oct,
	year = {2022},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
}

@misc{xu_embodiedsam_2025,
	title = {{EmbodiedSAM}: online segment any {3D} thing in real time},
	shorttitle = {{EmbodiedSAM}},
	url = {http://arxiv.org/abs/2408.11811},
	doi = {10.48550/arXiv.2408.11811},
	abstract = {Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an online, real-time, fine-grained and highly-generalized 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is almost infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the field of 2D computer vision with superior performance, which makes the use of VFM to assist embodied 3D perception a promising direction. However, most existing VFM-assisted 3D perception methods are either offline or too slow that cannot be applied in practical embodied tasks. In this paper, we aim to leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in an online setting. This is a challenging problem since future frames are not available in the input streaming RGB-D video, and an instance may be observed in several frames so object matching between frames is required. To address these challenges, we first propose a geometric-aware query lifting module to represent the 2D masks generated by SAM by 3D-aware queries, which is then iteratively refined by a dual-level query decoder. In this way, the 2D masks are transferred to fine-grained shapes on 3D point clouds. Benefit from the query representation for 3D masks, we can compute the similarity matrix between the 3D masks from different views by efficient matrix operation, which enables real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan show our method achieves leading performance even compared with offline methods. Our method also demonstrates great generalization ability in several zero-shot dataset transferring experiments and show great potential in open-vocabulary and data-efficient setting. Code and demo are available at https://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for training and evaluation.},
	language = {en},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Xu, Xiuwei and Chen, Huangxing and Zhao, Linqing and Wang, Ziwei and Zhou, Jie and Lu, Jiwen},
	month = feb,
	year = {2025},
	note = {arXiv:2408.11811 [cs]
TLDR: A geometric-aware query lifting module is proposed to represent the 2D masks generated by SAM by 3D-aware queries, which are transferred to fine-grained shapes on 3D point clouds and benefit from the query representation for 3D masks, which enables real-time inference.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{noauthor_addon_nodate,
	title = {Addon item},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
}

@inproceedings{hua_trustagent_2024,
	title = {{TrustAgent}: towards safe and trustworthy {LLM}-based agents through agent constitution},
	shorttitle = {{TrustAgent}},
	url = {https://openreview.net/forum?id=ejl3NCLQBj},
	abstract = {The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stakes domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agentâ€™s safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments.},
	language = {en},
	urldate = {2025-02-10},
	author = {Hua, Wenyue and Yang, Xianjun and Jin, Mingyu and Li, Zelong and Cheng, Wei and Tang, Ruixiang and Zhang, Yongfeng},
	month = jun,
	year = {2024},
}

@misc{xu_survey_2024,
	title = {A {Survey} on {Robotics} with {Foundation} {Models}: toward {Embodied} {AI}},
	shorttitle = {A {Survey} on {Robotics} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2402.02385},
	doi = {10.48550/arXiv.2402.02385},
	abstract = {While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.},
	language = {en-US},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Xu, Zhiyuan and Wu, Kun and Wen, Junjie and Li, Jinming and Liu, Ning and Che, Zhengping and Tang, Jian},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02385 [cs]
TLDR: This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control, and showcases their commonly used datasets, simulators, and benchmarks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{yin_safeagentbench_2024,
	title = {{SafeAgentBench}: {A} {Benchmark} for {Safe} {Task} {Planning} of {Embodied} {LLM} {Agents}},
	shorttitle = {{SafeAgentBench}},
	url = {http://arxiv.org/abs/2412.13178},
	doi = {10.48550/arXiv.2412.13178},
	abstract = {With the integration of large language models (LLMs), embodied agents have strong capabilities to execute complicated instructions in natural language, paving a way for the potential deployment of embodied robots. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in real world. To study this issue, we present SafeAgentBenchâ€”a new benchmark for safety-aware task planning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset with 750 tasks, covering 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 8 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that the best-performing baseline gets 69\% success rate and only 5\% rejection rate for hazardous tasks, indicating significant safety risks. More details and codes are available at https://github.com/shengyin1224/SafeAgentBench.},
	language = {en},
	urldate = {2024-12-23},
	publisher = {arXiv},
	author = {Yin, Sheng and Pang, Xianghe and Ding, Yuanzhuo and Chen, Menglan and Bi, Yutong and Xiong, Yichen and Huang, Wenhao and Xiang, Zhen and Shao, Jing and Chen, Siheng},
	month = dec,
	year = {2024},
	note = {arXiv:2412.13178 [cs]
titleTranslation: SafeAgentBenchï¼šæ “å¡LLMè¯ç‰©å®‰å…¨ä»»åŠ¡è§„åˆ’çš„åŸºå‡†
TLDR: A new benchmark for safety-aware task planning of embodied LLM agents, with results showing that the best-performing baseline gets 69\% success rate for safe tasks, but only 5\% rejection rate for hazardous tasks, indicating significant safety risks.},
	keywords = {/reading, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Robotics, ccfInfo: Not Found, citationNumber: 0, notion, â­â­â­â­},
}

@misc{huang_voxposer_2023,
	title = {{VoxPoser}: composable {3D} value maps for robotic manipulation with language models},
	shorttitle = {{VoxPoser}},
	url = {http://arxiv.org/abs/2307.05973},
	doi = {10.48550/arXiv.2307.05973},
	abstract = {Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li},
	month = nov,
	year = {2023},
	note = {arXiv:2307.05973 [cs]
TLDR: A large-scale study of the proposed framework to synthesize closed-loop robot trajectories with robustness to dynamic perturbations is presented, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{vemprala_chatgpt_2024,
	title = {{ChatGPT} for robotics: design principles and model abilities},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	shorttitle = {{ChatGPT} for robotics},
	url = {https://ieeexplore.ieee.org/document/10500490/},
	doi = {10.1109/ACCESS.2024.3387941},
	language = {en},
	urldate = {2025-02-10},
	journal = {IEEE Access},
	author = {Vemprala, Sai H. and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
	year = {2024},
	keywords = {ccfInfo: CCF-None ACCESS, citationNumber: 422},
	pages = {55682--55696},
}

@misc{ma_survey_2024,
	title = {A survey on vision-language-action models for embodied {AI}},
	url = {http://arxiv.org/abs/2405.14093},
	doi = {10.48550/arXiv.2405.14093},
	abstract = {Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Ma, Yueen and Song, Zixing and Zhuang, Yuzheng and Hao, Jianye and King, Irwin},
	month = nov,
	year = {2024},
	note = {arXiv:2405.14093 [cs]
TLDR: It is imperative to capture the evolving landscape of embodied AI through a comprehensive survey of vision-language-action models, reflecting the rapid advancement of embodied AI.},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{singh_progprompt_2023,
	address = {London, United Kingdom},
	title = {{ProgPrompt}: generating situated robot task plans using large language models},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-2365-8},
	shorttitle = {{ProgPrompt}},
	url = {https://ieeexplore.ieee.org/document/10161317/},
	doi = {10.1109/ICRA48891.2023.10161317},
	language = {en},
	urldate = {2025-02-10},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
	month = may,
	year = {2023},
	keywords = {ccfInfo: CCF-B ICRA, citationNumber: 93},
	pages = {11523--11530},
}

@misc{nair_r3m_2022,
	title = {{R3M}: a universal visual representation for robot manipulation},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {R3m},
	url = {https://arxiv.org/abs/2203.12601},
	doi = {10.48550/ARXIV.2203.12601},
	abstract = {We study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks. Concretely, we pre-train a visual representation using the Ego4D human video dataset using a combination of time-contrastive learning, video-language alignment, and an L1 penalty to encourage sparse and compact representations. The resulting representation, R3M, can be used as a frozen perception module for downstream policy learning. Across a suite of 12 simulated robot manipulation tasks, we find that R3M improves task success by over 20\% compared to training from scratch and by over 10\% compared to state-of-the-art visual representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered apartment given just 20 demonstrations. Code and pre-trained models are available at https://tinyurl.com/robotr3m.},
	language = {en},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Nair, Suraj and Rajeswaran, Aravind and Kumar, Vikash and Finn, Chelsea and Gupta, Abhinav},
	year = {2022},
	note = {Version Number: 3
TLDR: R3M enables a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered apartment given just 20 demonstrations and can be used as a frozen perception module for downstream policy learning.},
	keywords = {Artificial Intelligence (cs.AI), Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Robotics (cs.RO), ccfInfo: Net Error: 0, citationNumber: 154},
}

@misc{noauthor_ai_nodate,
	title = {{ä¸€é¡¹æœ‰å…³ä½“ç°AIçš„è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„è°ƒæŸ¥} --- {A} {Survey} on {Vision}-{Language}-{Action} {Models} for {Embodied} {AI}},
	url = {https://arxiv.org/html/2405.14093v2},
	language = {zh},
	urldate = {2025-02-10},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
}

@article{padmakumar_teach_2022,
	title = {{TEACh}: task-driven embodied agents that chat},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{TEACh}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20097},
	doi = {10.1609/aaai.v36i2.20097},
	abstract = {Robots operating in human spaces must be able to engage in natural language interaction, both understanding and executing instructions, and using conversation to resolve ambiguity and correct mistakes. To study this, we introduce TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation. A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from "Make Coffee" to "Prepare Breakfast", asking questions and getting additional information from the Commander. We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models' abilities in dialogue understanding, language grounding, and task execution.},
	language = {en},
	number = {2},
	urldate = {2025-02-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Padmakumar, Aishwarya and Thomason, Jesse and Shrivastava, Ayush and Lange, Patrick and Narayan-Chen, Anjali and Gella, Spandana and Piramuthu, Robinson and Tur, Gokhan and Hakkani-Tur, Dilek},
	month = jun,
	year = {2022},
	note = {Number: 2
TLDR: TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation, is introduced and initial models' abilities in dialogue understanding, language grounding, and task execution are evaluated.},
	keywords = {Speech \& Natural Language Processing (SNLP)},
	pages = {2017--2025},
}

@inproceedings{song_llm-planner_2023,
	title = {{LLM}-planner: few-shot grounded planning for embodied agents with large language models},
	shorttitle = {{LLM}-planner},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.html},
	language = {en},
	urldate = {2025-02-10},
	author = {Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M. and Chao, Wei-Lun and Su, Yu},
	year = {2023},
	keywords = {ccfInfo: CCF-A ICCV, citationNumber: 291},
	pages = {2998--3009},
}

@article{mu_embodiedgpt_2023,
	title = {{EmbodiedGPT}: vision-language pre-training via embodied chain of thought},
	volume = {36},
	shorttitle = {{EmbodiedGPT}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/4ec43957eda1126ad4887995d05fae3b-Abstract-Conference.html},
	language = {en},
	urldate = {2025-02-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
	month = dec,
	year = {2023},
	keywords = {ccfInfo: CCF-A NeurIPS, citationNumber: 124},
	pages = {25081--25094},
}

@misc{wu_embodied_2023,
	title = {Embodied task planning with large language models},
	url = {http://arxiv.org/abs/2307.01848},
	doi = {10.48550/arXiv.2307.01848},
	abstract = {Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Wu, Zhenyu and Wang, Ziwei and Xu, Xiuwei and Lu, Jiwen and Yan, Haibin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01848 [cs]
TLDR: Experimental results show that the generated plan from the TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{lin_grounded_2023,
	title = {On grounded planning for embodied tasks with language models},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26549},
	doi = {10.1609/aaai.v37i11.26549},
	abstract = {Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear whether they have the capacity to generate grounded, executable plans for embodied tasks. This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named G-PlanET, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an evaluation protocol and design a dedicated metric, KAS, to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. Our analysis also reveals interesting and non-trivial findings.},
	language = {en},
	number = {11},
	urldate = {2025-02-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lin, Bill Yuchen and Huang, Chengsong and Liu, Qian and Gu, Wenda and Sommerer, Sam and Ren, Xiang},
	month = jun,
	year = {2023},
	note = {Number: 11
TLDR: This paper addresses the question of whether language models have the capacity to generate grounded, executable plans for embodied tasks and demonstrates that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning.},
	keywords = {SNLP: Applications, ccfInfo: CCF-A AAAI, citationNumber: 13},
	pages = {13192--13200},
}

@misc{noauthor_embodied_nodate,
	title = {Embodied intelligent task planning - google scholar},
	url = {https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Embodied+Intelligent+task+planning&btnG=},
	language = {en},
	urldate = {2025-02-10},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
}

@misc{zhang_et-plan-bench_2024,
	title = {{ET}-plan-bench: embodied task-level planning benchmark towards spatial-temporal cognition with foundation models},
	shorttitle = {{ET}-plan-bench},
	url = {http://arxiv.org/abs/2410.14682},
	doi = {10.48550/arXiv.2410.14682},
	abstract = {Recent advancements in Large Language Models (LLMs) have spurred numerous attempts to apply these technologies to embodied tasks, particularly focusing on high-level task planning and task decomposition. To further explore this area, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which specifically targets embodied task planning using LLMs. It features a controllable and diverse set of embodied tasks varying in different levels of difficulties and complexities, and is designed to evaluate two critical dimensions of LLMs' application in embodied task understanding: spatial (relation constraint, occlusion for target objects) and temporal \& causal understanding of the sequence of actions in the environment. By using multi-source simulators as the backend simulator, it can provide immediate environment feedback to LLMs, which enables LLMs to interact dynamically with the environment and re-plan as necessary. We evaluated the state-of-the-art open source and closed source foundation models, including GPT-4, LLAMA and Mistral on our proposed benchmark. While they perform adequately well on simple navigation tasks, their performance can significantly deteriorate when faced with tasks that require a deeper understanding of spatial, temporal, and causal relationships. Thus, our benchmark distinguishes itself as a large-scale, quantifiable, highly automated, and fine-grained diagnostic framework that presents a significant challenge to the latest foundation models. We hope it can spark and drive further research in embodied task planning using foundation models.},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Zhang, Lingfeng and Wang, Yuening and Gu, Hongjian and Hamidizadeh, Atia and Zhang, Zhanguang and Liu, Yuecheng and Wang, Yutong and Bravo, David Gamaliel Arcos and Dong, Junyi and Zhou, Shunbo and Cao, Tongtong and Zhuang, Yuzheng and Zhang, Yingxue and Hao, Jianye},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14682 [cs]
TLDR: A new embodied task planning benchmark, ET-Plan-Bench, which specifically targets embodied task planning using LLMs, and is designed to evaluate two critical dimensions of LLMs' application in embodied task understanding: spatial (relation constraint, occlusion for target objects) and temporal\&causal understanding of the sequence of actions in the environment.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@article{gan_vision-language_2022,
	title = {Vision-language pre-training: basics, recent advances, and future trends},
	volume = {14},
	issn = {1572-2740, 1572-2759},
	shorttitle = {Vision-language pre-training},
	url = {https://www.nowpublishers.com/article/Details/CGV-105},
	doi = {10.1561/0600000105},
	abstract = {Vision-Language Pre-Training: Basics, Recent Advances, and Future Trends},
	language = {en},
	number = {3â€“4},
	urldate = {2025-01-06},
	journal = {Foundations and TrendsÂ® in Computer Graphics and Vision},
	author = {Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng},
	month = dec,
	year = {2022},
	note = {Publisher: Now Publishers, Inc.},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 63},
	pages = {163--352},
}

@inproceedings{tu_end--end_2022,
	title = {End-to-end human-gaze-target detection with transformers},
	url = {https://ieeexplore.ieee.org/abstract/document/9879533},
	doi = {10.1109/CVPR52688.2022.00224},
	abstract = {In this paper, we propose an effective and efficient method for Human-Gaze-Target (HGT) detection, i.e., gaze following. Current approaches decouple the HGT detection task into separate branches of salient object detection and human gaze prediction, employing a two-stage framework where human head locations must first be detected and then be fed into the next gaze target prediction sub-network. In contrast, we redefine the HGT detection task as detecting human head locations and their gaze targets, simultaneously. By this way, our method, named Human-Gaze-Target detection TRansformer or HGTTR, streamlines the HGT detection pipeline by eliminating all other additional components. HGTTR reasons about the relations of salient objects and human gaze from the global image context. Moreover, unlike existing two-stage methods that require human head locations as input and can predict only one human's gaze target at a time, HGTTR can directly predict the locations of all people and their gaze targets at one time in an end-to-end manner. The effectiveness and robustness of our proposed method are verified with extensive experiments on the two standard benchmark datasets, GazeFollowing and VideoAttentionTarget. Without bells and whistles, HGTTR outperforms existing state-of-the-art methods by large margins (6.4 mAP gain on GazeFollowing and 10.3 mAP gain on VideoAttentionTarget) with a much simpler architecture.},
	language = {en},
	urldate = {2025-01-06},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Tu, Danyang and Min, Xiongkuo and Duan, Huiyu and Guo, Guodong and Zhai, Guangtao and Shen, Wei},
	month = jun,
	year = {2022},
	note = {GSCC: 0000059 
ISSN: 2575-7075
TLDR: Unlike existing two-stage methods that require human head locations as input and can predict only one human's gaze target at a time, HGTTR can directly predict the locations of all people and their gaze targets at one time in an end-to-end manner.
abstractTranslation: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„äººç±»å‡è§†ç›®æ ‡ ï¼ˆHGTï¼‰ æ£€æµ‹æ–¹æ³•ï¼Œå³å‡è§†è·Ÿéšã€‚ç›®å‰çš„æ–¹æ³•å°† HGT æ£€æµ‹ä»»åŠ¡è§£è€¦ä¸ºæ˜¾è‘—ç›®æ ‡æ£€æµ‹å’Œäººç±»å‡è§†é¢„æµ‹çš„ä¸åŒåˆ†æ”¯ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå…¶ä¸­å¿…é¡»é¦–å…ˆæ£€æµ‹äººç±»å¤´éƒ¨ä½ç½®ï¼Œç„¶åé¦ˆé€åˆ°ä¸‹ä¸€ä¸ªå‡è§†ç›®æ ‡é¢„æµ‹å­ç½‘ç»œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å°† HGT æ£€æµ‹ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºåŒæ—¶æ£€æµ‹äººç±»å¤´éƒ¨ä½ç½®åŠå…¶å‡è§†ç›®æ ‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬åä¸º Human-Gaze-Target detection TRansformer æˆ– HGTTR çš„æ–¹æ³•é€šè¿‡æ¶ˆé™¤æ‰€æœ‰å…¶ä»–é™„åŠ ç»„ä»¶æ¥ç®€åŒ– HGT æ£€æµ‹ç®¡é“ã€‚HGTTR ä»å…¨å±€å›¾åƒèƒŒæ™¯ä¸­æ¨ç†çªå‡ºç‰©ä½“ä¸äººç±»å‡è§†çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰çš„éœ€è¦äººç±»å¤´éƒ¨ä½ç½®ä½œä¸ºè¾“å…¥ä¸”ä¸€æ¬¡åªèƒ½é¢„æµ‹ä¸€ä¸ªäººçš„æ³¨è§†ç›®æ ‡çš„ä¸¤é˜¶æ®µæ–¹æ³•ä¸åŒï¼ŒHGTTR å¯ä»¥ç«¯åˆ°ç«¯åœ°ç›´æ¥é¢„æµ‹æ‰€æœ‰äººåŠå…¶æ³¨è§†ç›®æ ‡çš„ä½ç½®ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§é€šè¿‡åœ¨ä¸¤ä¸ªæ ‡å‡†åŸºå‡†æ•°æ®é›† GazeFollowing å’Œ VideoAttentionTarget ä¸Šè¿›è¡Œå¤§é‡å®éªŒå¾—åˆ°äº†éªŒè¯ã€‚æ²¡æœ‰èŠ±é‡Œèƒ¡å“¨çš„ä¸œè¥¿ï¼ŒHGTTR çš„æ€§èƒ½å¤§å¤§ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ˆGazeFollowing å¢ç›Š 6.4 mAPï¼ŒVideoAttentionTarget å¢ç›Š 10.3 mAPï¼‰ï¼Œæ¶æ„è¦ç®€å•å¾—å¤šã€‚
titleTranslation: ä½¿ç”¨ Transformer è¿›è¡Œç«¯åˆ°ç«¯äººç±»å‡è§†ç›®æ ‡æ£€æµ‹},
	keywords = {Behavior analysis, Computer vision, Head, Object detection, Pipelines, Robustness, Streaming media, Transformers, ccfInfo: Net Error: 0, citationNumber: 33},
	pages = {2192--2200},
}

@misc{gu_open-vocabulary_2022,
	title = {Open-vocabulary object detection via vision and language knowledge distillation},
	url = {http://arxiv.org/abs/2104.13921},
	doi = {10.48550/arXiv.2104.13921},
	abstract = {We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP\$\_r\$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP\$\_r\$. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP\$\_\{50\}\$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.},
	language = {en},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
	month = may,
	year = {2022},
	note = {arXiv:2104.13921 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ccfInfo: Net Error: 0, citationNumber: 894},
}

@inproceedings{tonini_object-aware_2023,
	title = {Object-aware gaze target detection},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Tonini_Object-aware_Gaze_Target_Detection_ICCV_2023_paper.html},
	language = {en},
	urldate = {2025-01-06},
	author = {Tonini, Francesco and Dall'Asen, Nicola and Beyan, Cigdem and Ricci, Elisa},
	year = {2023},
	note = {GSCC: 0000027 
titleTranslation: å¯¹è±¡æ„ŸçŸ¥å‡è§†ç›®æ ‡æ£€æµ‹},
	keywords = {citationNumber: 5},
	pages = {21860--21869},
}

@inproceedings{bao_escnet_2022,
	title = {{ESCNet}: gaze target detection with the understanding of {3D} scenes},
	shorttitle = {{ESCNet}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Bao_ESCNet_Gaze_Target_Detection_With_the_Understanding_of_3D_Scenes_CVPR_2022_paper.html},
	language = {en},
	urldate = {2025-01-06},
	author = {Bao, Jun and Liu, Buyu and Yu, Jun},
	year = {2022},
	note = {GSCC: 0000034 
titleTranslation: ESCNetï¼šé€šè¿‡ç†è§£ 3D åœºæ™¯è¿›è¡Œå‡è§†ç›®æ ‡æ£€æµ‹},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 11},
	pages = {14126--14135},
}

@inproceedings{gadre_cows_2023,
	address = {Vancouver, BC, Canada},
	title = {{CoWs} on pasture: baselines and benchmarks for language-driven zero-shot object navigation},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0129-8},
	shorttitle = {{CoWs} on pasture},
	url = {https://ieeexplore.ieee.org/document/10203853/},
	doi = {10.1109/CVPR52729.2023.02219},
	language = {en},
	urldate = {2025-01-06},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran},
	month = jun,
	year = {2023},
	note = {TLDR: This work investigates a straightforward framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without fine-tuning and introduces the Pasturebenchmark, which considers finding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described relative to visible objects.},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 43},
	pages = {23171--23181},
}

@misc{static__nodate,
	title = {é˜…è¯»å†å²è®°å½•},
	shorttitle = {chartero},
	url = {https://github.com/volatile-static/Chartero},
	abstract = {Charteroè®°å½•çš„é˜…è¯»å†å²æ•°æ®ã€‚
è¯·å‹¿ä¿®æ”¹æœ¬æ¡ç›®ï¼ï¼ˆå¯ä»¥ç§»åŠ¨ï¼‰},
	author = {Static, Volatile},
	keywords = {ccfInfo: Net Error: 0, citationNumber: Not Found},
}

@article{chung_continuous_2024,
	title = {Continuous prediction of pointing targets with motion and eye-tracking in virtual reality},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10382527},
	doi = {10.1109/ACCESS.2024.3350788},
	abstract = {We present a study on continuously predicting the direction to a pointing target in virtual environments using motion and eye-tracker data throughout the pointing process. We first collect time series data for user motion and eye-tracker in a cursorless, single-target pointing task. Results from analyzing fixation points from different sensors and observing velocity profiles over the course of pointing provide insights into optimally configuring features for predicting the target angles. Following this analysis, we train a recurrent neural network that feeds on sliding window inputs for continuously operating target direction prediction from start to finish. The input window contains historical data from past to current frames, capturing temporal changes in the feature data. By feeding on this input, our model can predict the direction of the target at any given time during pointing. Our findings demonstrate that incorporating eye-tracker data into the prediction model boosts the maximum achievable accuracy by 2.5 times when compared to baselines without eye-tracker data inputs. The results suggest that using features from both the eye-tracker and joint motion contributes to higher prediction performance, as well as faster stabilization of output values at the starting phase of pointing.},
	language = {en},
	urldate = {2025-01-06},
	journal = {IEEE Access},
	author = {Chung, Choongho and Lee, Sung-Hee},
	year = {2024},
	note = {Conference Name: IEEE Access
TLDR: This study presents a study on continuously predicting the direction to a pointing target in virtual environments using motion and eye-tracker data throughout the pointing process, and demonstrates that incorporating eye-tracker data into the prediction model boosts the maximum achievable accuracy.
titleTranslation: åœ¨è™šæ‹Ÿç°å®ä¸­é€šè¿‡è¿åŠ¨å’Œçœ¼åŠ¨è¿½è¸ªè¿ç»­é¢„æµ‹æŒ‡å‘ç›®æ ‡
abstractTranslation: æˆ‘ä»¬æå‡ºäº†ä¸€é¡¹ç ”ç©¶ï¼Œå³åœ¨æ•´ä¸ªæŒ‡å‘è¿‡ç¨‹ä¸­ä½¿ç”¨è¿åŠ¨å’Œçœ¼åŠ¨è·Ÿè¸ªå™¨æ•°æ®åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿ç»­é¢„æµ‹æŒ‡å‘ç›®æ ‡çš„æ–¹å‘ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨æ— å…‰æ ‡ã€å•ç›®æ ‡æŒ‡å‘ä»»åŠ¡ä¸­æ”¶é›†ç”¨æˆ·è¿åŠ¨å’Œçœ¼åŠ¨è·Ÿè¸ªå™¨çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚åˆ†ææ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„å›ºå®šç‚¹å’Œåœ¨æŒ‡å‘è¿‡ç¨‹ä¸­è§‚å¯Ÿé€Ÿåº¦æ›²çº¿çš„ç»“æœä¸ºä¼˜åŒ–é…ç½®ç‰¹å¾ä»¥é¢„æµ‹ç›®æ ‡è§’åº¦æä¾›äº†è§è§£ã€‚æ ¹æ®æ­¤åˆ†æï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªé€’å½’ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œä»¥æ»‘åŠ¨çª—å£è¾“å…¥ä¸ºé£Ÿï¼Œç”¨äºä»å¤´åˆ°å°¾è¿ç»­è¿è¡Œçš„ç›®æ ‡æ–¹å‘é¢„æµ‹ã€‚è¾“å…¥çª—å£åŒ…å«ä»è¿‡å»å¸§åˆ°å½“å‰å¸§çš„å†å²æ•°æ®ï¼Œç”¨äºæ•è·è¦ç´ æ•°æ®ä¸­çš„æ—¶é—´å˜åŒ–ã€‚é€šè¿‡ä»¥è¿™äº›è¾“å…¥ä¸ºé£Ÿï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨æŒ‡å‘è¿‡ç¨‹ä¸­çš„ä»»ä½•ç»™å®šæ—¶é—´é¢„æµ‹ç›®æ ‡çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸æ²¡æœ‰çœ¼åŠ¨ä»ªæ•°æ®è¾“å…¥çš„åŸºçº¿ç›¸æ¯”ï¼Œå°†çœ¼åŠ¨ä»ªæ•°æ®çº³å…¥é¢„æµ‹æ¨¡å‹å¯å°†å¯å®ç°çš„æœ€å¤§å‡†ç¡®æ€§æé«˜ 2.5 å€ã€‚ç»“æœè¡¨æ˜ï¼ŒåŒæ—¶ä½¿ç”¨çœ¼åŠ¨ä»ªå’Œå…³èŠ‚è¿åŠ¨çš„ç‰¹å¾æœ‰åŠ©äºæé«˜é¢„æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨æŒ‡å‘çš„èµ·å§‹é˜¶æ®µæ›´å¿«åœ°ç¨³å®šè¾“å‡ºå€¼ã€‚},
	keywords = {Gaze, Gaze tracking, Motion capture, Predictive models, Resists, Solid modeling, Task analysis, Virtual reality, ccfInfo: Net Error: 0, citationNumber: 0, eye-tracker, pointing, prediction, virtual reality},
	pages = {5933--5946},
}

@misc{stone_open-world_2023,
	title = {Open-world object manipulation using pre-trained vision-language models},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.00905},
	doi = {10.48550/ARXIV.2303.00905},
	abstract = {For robots to follow instructions from people, they must be able to connect the rich semantic information in human vocabulary, e.g. "can you get me the pink stuffed whale?" to their sensory observations and actions. This brings up a notably difficult challenge for robots: while robot learning approaches allow robots to learn many different behaviors from first-hand experience, it is impractical for robots to have first-hand experiences that span all of this semantic information. We would like a robot's policy to be able to perceive and pick up the pink stuffed whale, even if it has never seen any data interacting with a stuffed whale before. Fortunately, static data on the internet has vast semantic information, and this information is captured in pre-trained vision-language models. In this paper, we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never seen first-hand. We develop a simple approach, which we call Manipulation of Open-World Objects (MOO), which leverages a pre-trained vision-language model to extract object-identifying information from the language command and image, and conditions the robot policy on the current image, the instruction, and the extracted object information. In a variety of experiments on a real mobile manipulator, we find that MOO generalizes zero-shot to a wide range of novel object categories and environments. In addition, we show how MOO generalizes to other, non-language-based input modalities to specify the object of interest such as finger pointing, and how it can be further extended to enable open-world navigation and manipulation. The project's website and evaluation videos can be found at https://robot-moo.github.io/},
	language = {en},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Stone, Austin and Xiao, Ted and Lu, Yao and Gopalakrishnan, Keerthana and Lee, Kuang-Huei and Vuong, Quan and Wohlhart, Paul and Kirmani, Sean and Zitkovich, Brianna and Xia, Fei and Finn, Chelsea and Hausman, Karol},
	year = {2023},
	note = {Version Number: 2
TLDR: Manipulation of Open-World Objects (MOO) is developed, which leverages a pre-trained vision-language model to extract object-identifying information from the language command and image, and conditions the robot policy on the current image, the instruction, and the extracted object information.},
	keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Robotics (cs.RO), ccfInfo: Net Error: 0, citationNumber: 32},
}

@misc{black__0_2024,
	title = {\$Ï€\_0\$: a vision-language-action flow model for general robot control},
	shorttitle = {\$Ï€\_0\$},
	url = {http://arxiv.org/abs/2410.24164},
	doi = {10.48550/arXiv.2410.24164},
	abstract = {Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.},
	language = {en},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and Levine, Sergey and Li-Bell, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang, Haohuan and Zhilinsky, Ury},
	month = nov,
	year = {2024},
	note = {arXiv:2410.24164 [cs]
TLDR: A novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge is proposed and evaluated in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning.},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, ccfInfo: Net Error: 0, citationNumber: 0},
}

@inproceedings{papadopoulos_training_2014,
	address = {Cham},
	title = {Training object class detectors from eye tracking data},
	isbn = {978-3-319-10602-1},
	doi = {10.1007/978-3-319-10602-1_24},
	abstract = {Training an object class detector typically requires a large set of images annotated with bounding-boxes, which is expensive and time consuming to create. We propose novel approach to annotate object locations which can substantially reduce annotation time. We first track the eye movements of annotators instructed to find the object and then propose a technique for deriving object bounding-boxes from these fixations. To validate our idea, we collected eye tracking data for the trainval part of 10 object classes of Pascal VOC 2012 (6,270 images, 5Â observers). Our technique correctly produces bounding-boxes in 50\%of the images, while reducing the total annotation time by factor 6.8Ã— compared to drawing bounding-boxes. Any standard object class detector can be trained on the bounding-boxes predicted by our model. Our large scale eye tracking dataset is available at groups.inf.ed.ac.uk/calvin/eyetrackdataset/.},
	language = {en},
	booktitle = {Computer {Vision} â€“ {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Papadopoulos, Dim P. and Clarke, Alasdair D. F. and Keller, Frank and Ferrari, Vittorio},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	note = {TLDR: A novel approach to annotate object locations which can substantially reduce annotation time is proposed, which first track the eye movements of annotators instructed to find the object and then a technique for deriving object bounding-boxes from these fixations is proposed.
abstractTranslation: è®­ç»ƒå¯¹è±¡ç±»æ£€æµ‹å™¨é€šå¸¸éœ€è¦å¤§é‡ç”¨è¾¹ç•Œæ¡†æ³¨é‡Šçš„å›¾åƒï¼Œè¿™æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ³¨é‡Šå¯¹è±¡ä½ç½®ï¼Œè¿™å¯ä»¥å¤§å¤§å‡å°‘æ³¨é‡Šæ—¶é—´ã€‚æˆ‘ä»¬é¦–å…ˆè·Ÿè¸ªè¢«æŒ‡ç¤ºå¯»æ‰¾å¯¹è±¡çš„æ³¨é‡Šè€…çš„çœ¼çƒè¿åŠ¨ï¼Œç„¶åæå‡ºä¸€ç§ä»è¿™äº›æ³¨è§†ç‚¹ä¸­æ¨å¯¼å‡ºå¯¹è±¡è¾¹ç•Œæ¡†çš„æŠ€æœ¯ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æƒ³æ³•ï¼Œæˆ‘ä»¬æ”¶é›†äº† Pascal VOC 2012 çš„ 10 ä¸ªå¯¹è±¡ç±»åˆ«çš„ trainval éƒ¨åˆ†çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®ï¼ˆ6,270 å¼ å›¾åƒï¼Œ5 ä¸ªè§‚å¯Ÿè€…ï¼‰ã€‚æˆ‘ä»¬çš„æŠ€æœ¯åœ¨ 50\% çš„å›¾åƒä¸­æ­£ç¡®ç”Ÿæˆäº†è¾¹ç•Œæ¡†ï¼ŒåŒæ—¶ä¸ç»˜åˆ¶è¾¹ç•Œæ¡†ç›¸æ¯”ï¼Œæ€»æ³¨é‡Šæ—¶é—´å‡å°‘äº† 6.8Ã— å€ã€‚ä»»ä½•æ ‡å‡†å¯¹è±¡ç±»æ£€æµ‹å™¨éƒ½å¯ä»¥åœ¨æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹çš„è¾¹ç•Œæ¡†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„å¤§è§„æ¨¡çœ¼åŠ¨è¿½è¸ªæ•°æ®é›†å¯åœ¨ groups.inf.ed.ac.uk/calvin/eyetrackdataset/ è·å–ã€‚
titleTranslation: ä»çœ¼åŠ¨è¿½è¸ªæ•°æ®ä¸­è®­ç»ƒå¯¹è±¡ç±»åˆ«æ£€æµ‹å™¨},
	keywords = {Appearance ModelÂ Â å¤–è§‚æ¨¡å‹, Gaussian Mixture ModelÂ Â é«˜æ–¯æ··åˆæ¨¡å‹, Object ClassÂ Â Object ç±», Target ObjectÂ Â ç›®æ ‡å¯¹è±¡, Visual Search TaskÂ Â è§†è§‰æœç´¢ä»»åŠ¡, ccfInfo: Net Error: 0, citationNumber: 175},
	pages = {361--376},
}

@article{wang_dynamic_2021,
	title = {Dynamic attention guided multi-trajectory analysis for single object tracking},
	volume = {31},
	url = {https://consensus.app/papers/dynamic-attention-guided-multitrajectory-analysis-for-wang-chen/29a772980f7d5412856ec74029c57504/},
	doi = {10.1109/TCSVT.2021.3056684},
	abstract = {Most of the existing single object trackers track the target in a unitary local search window, making them particularly vulnerable to challenging factors such as heavy occlusions and out-of-view movements. Despite the attempts to further incorporate global search, prevailing mechanisms that cooperate local and global search are relatively static, thus are still sub-optimal for improving tracking performance. By further studying the local and global search results, we raise a question: can we allow more dynamics for cooperating both results? In this paper, we propose to introduce more dynamics by devising a dynamic attention-guided multi-trajectory tracking strategy. In particular, we construct dynamic appearance model that contains multiple target templates, each of which provides its own attention for locating the target in the new frame. Guided by different attention, we maintain diversified tracking results for the target to build multi-trajectory tracking history, allowing more candidates to represent the true target trajectory. After spanning the whole sequence, we introduce a multi-trajectory selection network to find the best trajectory that deliver improved tracking performance. Extensive experimental results show that our proposed tracking strategy achieves compelling performance on various large-scale tracking benchmarks. The project page of this paper can be found at https://sites.google.com/view/mt-track/.},
	language = {en},
	urldate = {2025-01-06},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Wang, Xiao and Chen, Zhe and Tang, Jin and Luo, B. and Wang, Yaowei and Tian, Yonghong and Wu, Feng},
	year = {2021},
	note = {TLDR: This paper proposes to introduce more dynamics by devising a dynamic attention-guided multi-trajectory tracking strategy that contains multiple target templates, each of which provides its own attention for locating the target in the new frame.
titleTranslation: ç”¨äºå•ä¸ªç›®æ ‡è·Ÿè¸ªçš„åŠ¨æ€æ³¨æ„åŠ›å¼•å¯¼å¤šè½¨è¿¹åˆ†æ
abstractTranslation: å¤§å¤šæ•°ç°æœ‰çš„å•ä¸ªç›®æ ‡è·Ÿè¸ªå™¨åœ¨å•ä¸€çš„æœ¬åœ°æœç´¢çª—å£ä¸­è·Ÿè¸ªç›®æ ‡ï¼Œè¿™ä½¿å¾—å®ƒä»¬ç‰¹åˆ«å®¹æ˜“å—åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„å› ç´ çš„å½±å“ï¼Œä¾‹å¦‚ä¸¥é‡é®æŒ¡å’Œè¶…å‡ºè§†é‡çš„ç§»åŠ¨ã€‚å°½ç®¡å°è¯•è¿›ä¸€æ­¥æ•´åˆå…¨å±€æœç´¢ï¼Œä½†åä½œæœ¬åœ°å’Œå…¨å±€æœç´¢çš„ç°è¡Œæœºåˆ¶ç›¸å¯¹é™æ€ï¼Œå› æ­¤å¯¹äºæé«˜è·Ÿè¸ªæ€§èƒ½æ¥è¯´ä»ç„¶æ˜¯æ¬¡ä¼˜çš„ã€‚é€šè¿‡è¿›ä¸€æ­¥ç ”ç©¶æœ¬åœ°å’Œå…¨å±€æœç´¢ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬æ˜¯å¦å¯ä»¥å…è®¸æ›´å¤šçš„åŠ¨æ€æ¥é…åˆè¿™ä¸¤ä¸ªç»“æœï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è®¾è®¡ä¸€ç§åŠ¨æ€çš„æ³¨æ„åŠ›å¼•å¯¼å¤šè½¨è¿¹è·Ÿè¸ªç­–ç•¥æ¥å¼•å…¥æ›´å¤šçš„åŠ¨åŠ›å­¦ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«å¤šä¸ªç›®æ ‡æ¨¡æ¿çš„åŠ¨æ€å¤–è§‚æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡æ¿éƒ½ä¸ºåœ¨æ–°å¸§ä¸­å®šä½ç›®æ ‡æä¾›äº†è‡ªå·±çš„å…³æ³¨ã€‚åœ¨ä¸åŒçš„å…³æ³¨ç‚¹å¼•å¯¼ä¸‹ï¼Œæˆ‘ä»¬ç»´æŠ¤å¯¹ç›®æ ‡çš„å¤šå…ƒåŒ–è·Ÿè¸ªç»“æœï¼Œæ„å»ºå¤šè½¨è¿¹è·Ÿè¸ªå†å²ï¼Œè®©æ›´å¤šçš„å€™é€‰è€…èƒ½å¤Ÿä»£è¡¨çœŸå®çš„ç›®æ ‡è½¨è¿¹ã€‚åœ¨è·¨è¶Šæ•´ä¸ªåºåˆ—ä¹‹åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè½¨è¿¹é€‰æ‹©ç½‘ç»œï¼Œä»¥æ‰¾åˆ°æä¾›æ”¹è¿›è·Ÿè¸ªæ€§èƒ½çš„æœ€ä½³è½¨è¿¹ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„è·Ÿè¸ªç­–ç•¥åœ¨å„ç§å¤§è§„æ¨¡è·Ÿè¸ªåŸºå‡†ä¸Šå–å¾—äº†ä»¤äººä¿¡æœçš„æ€§èƒ½ã€‚æœ¬æ–‡çš„é¡¹ç›®é¡µé¢å¯ä»¥åœ¨ https://sites.google.com/view/mt-track/ ä¸Šæ‰¾åˆ°ã€‚},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 33},
	pages = {4895--4908},
}

@misc{pham_gazesearch_2024,
	title = {{GazeSearch}: radiology findings search benchmark},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {{GazeSearch}},
	url = {https://arxiv.org/abs/2411.05780},
	doi = {10.48550/ARXIV.2411.05780},
	abstract = {Medical eye-tracking data is an important information source for understanding how radiologists visually interpret medical images. This information not only improves the accuracy of deep learning models for X-ray analysis but also their interpretability, enhancing transparency in decision-making. However, the current eye-tracking data is dispersed, unprocessed, and ambiguous, making it difficult to derive meaningful insights. Therefore, there is a need to create a new dataset with more focus and purposeful eyetracking data, improving its utility for diagnostic applications. In this work, we propose a refinement method inspired by the target-present visual search challenge: there is a specific finding and fixations are guided to locate it. After refining the existing eye-tracking datasets, we transform them into a curated visual search dataset, called GazeSearch, specifically for radiology findings, where each fixation sequence is purposefully aligned to the task of locating a particular finding. Subsequently, we introduce a scan path prediction baseline, called ChestSearch, specifically tailored to GazeSearch. Finally, we employ the newly introduced GazeSearch as a benchmark to evaluate the performance of current state-of-the-art methods, offering a comprehensive assessment for visual search in the medical imaging domain. Code is available at {\textbackslash}url\{https://github.com/UARK-AICV/GazeSearch\}.},
	language = {en},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Pham, Trong Thang and Nguyen, Tien-Phat and Ikebe, Yuki and Awasthi, Akash and Deng, Zhigang and Wu, Carol C. and Nguyen, Hien and Le, Ngan},
	year = {2024},
	note = {Version Number: 2
TLDR: This work transforms the existing eye-tracking datasets into a curated visual search dataset, called GazeSearch, specifically for radiology findings, where each fixation sequence is purposefully aligned to the task of locating a particular finding.
abstractTranslation: åŒ»å­¦çœ¼åŠ¨è¿½è¸ªæ•°æ®æ˜¯äº†è§£æ”¾å°„ç§‘åŒ»ç”Ÿå¦‚ä½•ç›´è§‚åœ°è§£é‡ŠåŒ»å­¦å›¾åƒçš„é‡è¦ä¿¡æ¯æ¥æºã€‚è¿™äº›ä¿¡æ¯ä¸ä»…æé«˜äº†ç”¨äº X å°„çº¿åˆ†æçš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œè¿˜æé«˜äº†å®ƒä»¬çš„å¯è§£é‡Šæ€§ï¼Œä»è€Œæé«˜äº†å†³ç­–çš„é€æ˜åº¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®æ˜¯åˆ†æ•£çš„ã€æœªç»å¤„ç†çš„å’Œæ¨¡æ£±ä¸¤å¯çš„ï¼Œå› æ­¤å¾ˆéš¾è·å¾—æœ‰æ„ä¹‰çš„è§è§£ã€‚å› æ­¤ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªå…·æœ‰æ›´å¤šç„¦ç‚¹å’Œç›®çš„æ€§çœ¼åŠ¨è¿½è¸ªæ•°æ®çš„æ–°æ•°æ®é›†ï¼Œä»¥æé«˜å…¶åœ¨è¯Šæ–­åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—ç›®æ ‡-å½“å‰è§†è§‰æœç´¢æŒ‘æˆ˜å¯å‘çš„æ”¹è¿›æ–¹æ³•ï¼šæœ‰ä¸€ä¸ªç‰¹å®šçš„å‘ç°ï¼Œå¹¶æŒ‡å¯¼æ³¨è§†ç‚¹å®šä½å®ƒã€‚åœ¨å®Œå–„ç°æœ‰çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®é›†åï¼Œæˆ‘ä»¬å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªç²¾é€‰çš„è§†è§‰æœç´¢æ•°æ®é›†ï¼Œç§°ä¸º GazeSearchï¼Œä¸“é—¨ç”¨äºæ”¾å°„å­¦å‘ç°ï¼Œå…¶ä¸­æ¯ä¸ªæ³¨è§†åºåˆ—éƒ½æœ‰ç›®çš„åœ°ä¸å®šä½ç‰¹å®šå‘ç°çš„ä»»åŠ¡ä¿æŒä¸€è‡´ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ‰«æè·¯å¾„é¢„æµ‹åŸºçº¿ï¼Œç§°ä¸º ChestSearchï¼Œä¸“é—¨ä¸º GazeSearch é‡èº«å®šåˆ¶ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨æ–°å¼•å…¥çš„ GazeSearch ä½œä¸ºåŸºå‡†æ¥è¯„ä¼°å½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ï¼Œä¸ºåŒ»å­¦æˆåƒé¢†åŸŸçš„è§†è§‰æœç´¢æä¾›å…¨é¢çš„è¯„ä¼°ã€‚ä»£ç ä½äº {\textbackslash}url\{https://github.com/UARK-AICV/GazeSearch\}ã€‚
titleTranslation: GazeSearchï¼šæ”¾å°„å­¦ç»“æœæœç´¢åŸºå‡†},
	keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, ccfInfo: Net Error: 0, citationNumber: 0},
}

@misc{noauthor_tara_backup_nodate,
	title = {Tara\_Backup},
	language = {en},
	keywords = {ccfInfo: Net Error: 0, citationNumber: Not Found},
}

@misc{park_towards_2020,
	title = {Towards end-to-end video-based eye-tracking},
	url = {http://arxiv.org/abs/2007.13120},
	doi = {10.48550/arXiv.2007.13120},
	abstract = {Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the user's eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with up to a 28 percent improvement in Point-of-Gaze estimates (resulting in 2.49 degrees in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at https://ait.ethz.ch/projects/2020/EVE},
	language = {en},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Park, Seonwook and Aksan, Emre and Zhang, Xucong and Hilliges, Otmar},
	month = jul,
	year = {2020},
	note = {arXiv:2007.13120 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ccfInfo: Net Error: 0, citationNumber: 84},
}

@inproceedings{dai_enhanced_2024,
	title = {Enhanced multidimensional root cause localization with {JSqueeze} in {AIOps} ({S})},
	url = {http://ksiresearchorg.ipage.com/seke/seke24paper/paper034.pdf},
	doi = {10.18293/SEKE2024-034},
	language = {en},
	urldate = {2025-01-13},
	author = {Dai, Yukun and Li, Jian and Wan, Yuan and Xie, Liang and Fan, Pengfei},
	month = oct,
	year = {2024},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 0, linter/error},
	pages = {327--332},
}

@misc{zhu_earbench_2024,
	title = {{EARBench}: towards evaluating physical risk awareness for task planning of foundation model-based embodied {AI} agents},
	shorttitle = {{EARBench}},
	url = {http://arxiv.org/abs/2408.04449},
	doi = {10.48550/arXiv.2408.04449},
	abstract = {Embodied artificial intelligence (EAI) integrates advanced AI models into physical entities for real-world interaction. The emergence of foundation models as the "brain" of EAI agents for high-level task planning has shown promising results. However, the deployment of these agents in physical environments presents significant safety challenges. For instance, a housekeeping robot lacking sufficient risk awareness might place a metal container in a microwave, potentially causing a fire. To address these critical safety concerns, comprehensive pre-deployment risk assessments are imperative. This study introduces EARBench, a novel framework for automated physical risk assessment in EAI scenarios. EAIRiskBench employs a multi-agent cooperative system that leverages various foundation models to generate safety guidelines, create risk-prone scenarios, make task planning, and evaluate safety systematically. Utilizing this framework, we construct EARDataset, comprising diverse test cases across various domains, encompassing both textual and visual scenarios. Our comprehensive evaluation of state-of-the-art foundation models reveals alarming results: all models exhibit high task risk rates (TRR), with an average of 95.75\% across all evaluated models. To address these challenges, we further propose two prompting-based risk mitigation strategies. While these strategies demonstrate some efficacy in reducing TRR, the improvements are limited, still indicating substantial safety concerns. This study provides the first large-scale assessment of physical risk awareness in EAI agents. Our findings underscore the critical need for enhanced safety measures in EAI systems and provide valuable insights for future research directions in developing safer embodied artificial intelligence system. Data and code are available at https://github.com/zihao-ai/EARBench.},
	language = {en},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Zhu, Zihao and Wu, Bingzhe and Zhang, Zhengyou and Han, Lei and Liu, Qingshan and Wu, Baoyuan},
	month = nov,
	year = {2024},
	note = {arXiv:2408.04449 [cs]
titleTranslation: EARBenchï¼šä¸ºåŸºäºåŸºæœ¬æ¨¡å‹çš„å…·èº«æ™ºèƒ½ä»£ç†ä»»åŠ¡è§„åˆ’è¯„ä¼°ç‰©ç†é£é™©æ„è¯†è€Œè®¾è®¡
abstractTranslation: å…·èº«äººå·¥æ™ºèƒ½ï¼ˆEAIï¼‰å°†å…ˆè¿›çš„AIæ¨¡å‹æ•´åˆåˆ°ç‰©ç†å®ä½“ä¸­ï¼Œä»¥è¿›è¡ŒçœŸå®ä¸–ç•Œçš„äº’åŠ¨ã€‚åŸºç¡€æ¨¡å‹ä½œä¸ºEAIä»£ç†çš„â€œå¤§è„‘â€åœ¨é«˜çº§ä»»åŠ¡è§„åˆ’æ–¹é¢çš„å‡ºç°æ˜¾ç¤ºäº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œåœ¨ç‰©ç†ç¯å¢ƒä¸­éƒ¨ç½²è¿™äº›ä»£ç†å­˜åœ¨é‡å¤§å®‰å…¨æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œç¼ºä¹è¶³å¤Ÿé£é™©æ„è¯†çš„å®¶æ”¿æœºå™¨äººå¯èƒ½ä¼šå°†ä¸€ä¸ªé‡‘å±å®¹å™¨æ”¾å…¥å¾®æ³¢ç‚‰ï¼Œæ½œåœ¨åœ°å¼•å‘ç«ç¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å…³é”®çš„å®‰å…¨é—®é¢˜ï¼Œå¿…é¡»è¿›è¡Œå…¨é¢çš„éƒ¨ç½²å‰é£é™©è¯„ä¼°ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†EARBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºEAIåœºæ™¯ä¸­è‡ªåŠ¨åŒ–ç‰©ç†é£é™©è¯„ä¼°çš„æ–°æ¡†æ¶ã€‚EAIRiskBenché‡‡ç”¨äº†ä¸€ä¸ªå¤šä»£ç†åä½œç³»ç»Ÿï¼Œåˆ©ç”¨å„ç§åŸºç¡€æ¨¡å‹ç”Ÿæˆå®‰å…¨æŒ‡å—ï¼Œåˆ›å»ºé£é™©å€¾å‘åœºæ™¯ï¼Œè¿›è¡Œä»»åŠ¡è§„åˆ’ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°å®‰å…¨æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†EARDatasetï¼ŒåŒ…æ‹¬å„ä¸ªé¢†åŸŸçš„å¤šæ ·åŒ–æµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–æ–‡æœ¬å’Œè§†è§‰åœºæ™¯ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹è¿›è¡Œäº†ç»¼åˆè¯„ä¼°ï¼Œç»“æœå¼•äººæ·±æ€ï¼šæ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°å‡ºé«˜ä»»åŠ¡é£é™©ç‡ï¼ˆTRRï¼‰ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­å¹³å‡ä¸º95.75\%ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸¤ç§åŸºäºæç¤ºçš„é£é™©ç¼“è§£ç­–ç•¥ã€‚è™½ç„¶è¿™äº›ç­–ç•¥åœ¨é™ä½TRRæ–¹é¢è¡¨ç°å‡ºä¸€å®šçš„æœ‰æ•ˆæ€§ï¼Œä½†æ”¹è¿›ä»å—é™åˆ¶ï¼Œä»ç„¶æ˜¾ç¤ºå‡ºé‡å¤§çš„å®‰å…¨é—®é¢˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹EAIä»£ç†ä¸­çš„ç‰©ç†é£é™©æ„è¯†è¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨EAIç³»ç»Ÿä¸­åŠ å¼ºå®‰å…¨æªæ–½çš„è¿«åˆ‡éœ€æ±‚ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æ•°æ®å’Œä»£ç å¯åœ¨https://github.com/zihao-ai/EARBench ä¸Šæ‰¾åˆ°ã€‚},
	keywords = {Computer Science - Artificial Intelligence, ccfInfo: Net Error: 0, citationNumber: 0},
}

@article{panetta_software_2019,
	title = {Software architecture for automating cognitive science eye-tracking data analysis and object annotation},
	volume = {49},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/OAPA.html},
	issn = {2168-2291, 2168-2305},
	url = {https://ieeexplore.ieee.org/document/8633339/},
	doi = {10.1109/THMS.2019.2892919},
	language = {en},
	number = {3},
	urldate = {2025-01-06},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Panetta, Karen and Wan, Qianwen and Kaszowska, Aleksandra and Taylor, Holly A. and Agaian, Sos},
	month = jun,
	year = {2019},
	note = {TLDR: The proposed software architecture, gaze to object classification (GoC), processes the gaze-overlaid video from commercially available wearable eye trackers, recognizes and classifies the specific object a user is focusing on and calculates the gaze duration time.
titleTranslation: ç”¨äºè‡ªåŠ¨åŒ–è®¤çŸ¥ç§‘å­¦çœ¼åŠ¨è¿½è¸ªæ•°æ®åˆ†æå’Œå¯¹è±¡æ³¨é‡Šçš„è½¯ä»¶æ¶æ„},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 14, linter/error},
	pages = {268--277},
}

@misc{zhang_badrobot_2024,
	title = {{BadRobot}: manipulating embodied {LLMs} in the physical world},
	shorttitle = {{BadRobot}},
	url = {http://arxiv.org/abs/2407.20242},
	doi = {10.48550/arXiv.2407.20242},
	abstract = {Embodied AI represents systems where AI is integrated into physical entities, enabling them to perceive and interact with their surroundings. Large Language Model (LLM), which exhibits powerful language understanding abilities, has been extensively employed in embodied AI by facilitating sophisticated task planning. However, a critical safety issue remains overlooked: could these embodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot, a novel attack paradigm aiming to make embodied LLMs violate safety and ethical constraints through typical voice-based user-system interactions. Specifically, three vulnerabilities are exploited to achieve this type of attack: (i) manipulation of LLMs within robotic systems, (ii) misalignment between linguistic outputs and physical actions, and (iii) unintentional hazardous behaviors caused by world knowledge's flaws. Furthermore, we construct a benchmark of various malicious physical action queries to evaluate BadRobot's attack performance. Based on this benchmark, extensive experiments against existing prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the effectiveness of our BadRobot. Warning: This paper contains harmful AI-generated language and aggressive actions.},
	language = {en},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Zhang, Hangtao and Zhu, Chenyu and Wang, Xianlong and Zhou, Ziqi and Yin, Changgan and Li, Minghui and Xue, Lulu and Wang, Yichen and Hu, Shengshan and Liu, Aishan and Guo, Peijin and Zhang, Leo Yu},
	month = oct,
	year = {2024},
	note = {arXiv:2407.20242 [cs]
TLDR: This research investigates for the first time how to induce threatening actions in embodied AI, confirming the severe risks posed by these soon-to-be-marketed robots, which starkly contravene Asimovâ€™s Three Laws of Robotics and threaten human safety.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Robotics, ccfInfo: Net Error: 0, citationNumber: 0},
}

@misc{lu_poex_2024,
	title = {{POEX}: policy executable embodied {AI} jailbreak attacks},
	shorttitle = {Poex},
	url = {http://arxiv.org/abs/2412.16633},
	doi = {10.48550/arXiv.2412.16633},
	abstract = {The integration of large language models (LLMs) into the planning module of Embodied Artificial Intelligence (Embodied AI) systems has greatly enhanced their ability to translate complex user instructions into executable policies. In this paper, we demystified how traditional LLM jailbreak attacks behave in the Embodied AI context. We conducted a comprehensive safety analysis of the LLM-based planning module of embodied AI systems against jailbreak attacks. Using the carefully crafted Harmful-RLbench, we accessed 20 open-source and proprietary LLMs under traditional jailbreak attacks, and highlighted two key challenges when adopting the prior jailbreak techniques to embodied AI contexts: (1) The harmful text output by LLMs does not necessarily induce harmful policies in Embodied AI context, and (2) even we can generate harmful policies, we have to guarantee they are executable in practice. To overcome those challenges, we propose Policy Executable (POEX) jailbreak attacks, where harmful instructions and optimized suffixes are injected into LLM-based planning modules, leading embodied AI to perform harmful actions in both simulated and physical environments. Our approach involves constraining adversarial suffixes to evade detection and fine-tuning a policy evaluater to improve the executability of harmful policies. We conducted extensive experiments on both a robotic arm embodied AI platform and simulators, to validate the attack and policy success rates on 136 harmful instructions from Harmful-RLbench. Our findings expose serious safety vulnerabilities in LLM-based planning modules, including the ability of POEX to be transferred across models. Finally, we propose mitigation strategies, such as safety-constrained prompts, pre- and post-planning checks, to address these vulnerabilities and ensure the safe deployment of embodied AI in real-world settings.},
	language = {en},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Lu, Xuancun and Huang, Zhengxian and Li, Xinfeng and Ji, Xiaoyu and Xu, Wenyuan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.16633 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Robotics, ccfInfo: Net Error: 0, citationNumber: 0},
}

@inproceedings{komkov_advhat_2021,
	address = {Milan, Italy},
	title = {{AdvHat}: real-world adversarial attack on {ArcFace} face {ID} system},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-8808-9},
	shorttitle = {{AdvHat}},
	url = {https://ieeexplore.ieee.org/document/9412236/},
	doi = {10.1109/ICPR48806.2021.9412236},
	language = {en},
	urldate = {2025-01-16},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Komkov, Stepan and Petiushko, Aleksandr},
	month = jan,
	year = {2021},
	note = {TLDR: A novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions by printing the rectangular paper sticker on a common color printer and putting it on the hat.
remark: æå‡ºAdvHatæ”»å‡»ArcFaceäººè„¸è¯†åˆ«ç³»ç»Ÿã€‚
titleTranslation: AdvHatï¼šå¯¹ArcFaceäººè„¸è¯†åˆ«ç³»ç»Ÿçš„çœŸå®å¯¹æŠ—æ”»å‡»},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 162},
	pages = {819--826},
}

@misc{chen_towards_2024,
	title = {Towards physically-realizable adversarial attacks in embodied vision navigation},
	url = {http://arxiv.org/abs/2409.10071},
	doi = {10.48550/arXiv.2409.10071},
	abstract = {The deployment of embodied navigation agents in safety-critical environments raises concerns about their vulnerability to adversarial attacks on deep neural networks. However, current attack methods often lack practicality due to challenges in transitioning from the digital to the physical world, while existing physical attacks for object detection fail to achieve both multi-view effectiveness and naturalness. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches with learnable textures and opacity to objects. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which uses feedback from the navigation model to optimize the patch's texture. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, where opacity is refined after texture optimization. Experimental results show our adversarial patches reduce navigation success rates by about 40\%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: [https://github.com/chen37058/Physical-Attacks-in-Embodied-Navigation].},
	language = {en},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Chen, Meng and Tu, Jiawei and Qi, Chao and Dang, Yonghao and Zhou, Feng and Wei, Wei and Yin, Jianqin},
	month = nov,
	year = {2024},
	note = {arXiv:2409.10071 [cs]
TLDR: This work proposes a practical attack method for embodied navigation by attaching adversarial patches with learnable textures and opacity to objects, outperforming previous methods in practicality, effectiveness, and naturalness.
remark: æå‡ºè§†è§‰åˆç†å¯¹æŠ—è¡¥ä¸æ”»å‡»æ¡†æ¶ã€‚
titleTranslation: åœ¨å…·èº«è§†è§‰å¯¼èˆªä¸­å®ç°ç‰©ç†å¯¹æŠ—æ”»å‡»
abstractTranslation: åœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸­éƒ¨ç½²åµŒå…¥å¼å¯¼èˆªä»£ç†å¼•å‘äº†äººä»¬å¯¹å…¶æ˜“å—æ·±åº¦ç¥ç»ç½‘ç»œå¯¹æŠ—æ”»å‡»çš„æ‹…å¿§ã€‚ç„¶è€Œï¼Œç”±äºä»æ•°å­—ä¸–ç•Œè¿‡æ¸¡åˆ°ç‰©ç†ä¸–ç•Œçš„æŒ‘æˆ˜ï¼Œå½“å‰çš„æ”»å‡»æ–¹æ³•å¾€å¾€ç¼ºä¹å®ç”¨æ€§ï¼Œè€Œç°æœ‰çš„ç”¨äºç›®æ ‡æ£€æµ‹çš„ç‰©ç†æ”»å‡»æœªèƒ½å®ç°å¤šè§†å›¾æœ‰æ•ˆæ€§å’Œè‡ªç„¶æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„åµŒå…¥å¼å¯¼èˆªæ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡å°†å…·æœ‰å¯å­¦ä¹ çº¹ç†å’Œä¸é€æ˜åº¦çš„å¯¹æŠ—è¡¥ä¸é™„åŠ åˆ°å¯¹è±¡ä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†ç¡®ä¿è·¨ä¸åŒè§†ç‚¹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºå¯¹è±¡æ„ŸçŸ¥é‡‡æ ·çš„å¤šè§†å›¾ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨æ¥è‡ªå¯¼èˆªæ¨¡å‹çš„åé¦ˆæ¥ä¼˜åŒ–è¡¥ä¸çš„çº¹ç†ã€‚ä¸ºäº†ä½¿è¡¥ä¸å¯¹äººç±»è§‚å¯Ÿè€…ä¸æ˜¾çœ¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µä¸é€æ˜åº¦ä¼˜åŒ–æœºåˆ¶ï¼Œåœ¨çº¹ç†ä¼˜åŒ–åå¯¹ä¸é€æ˜åº¦è¿›è¡Œç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¯¹æŠ—è¡¥ä¸å°†å¯¼èˆªæˆåŠŸç‡é™ä½äº†çº¦40\%ï¼Œåœ¨å®ç”¨æ€§ã€æœ‰æ•ˆæ€§å’Œè‡ªç„¶æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€è·å¾—ï¼š[https://github.com/chen37058/Physical-Attacks-in-Embodied-Navigation].},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, ccfInfo: Net Error: 0, citationNumber: 0, â­â­â­â­â­},
}

@inproceedings{tan_legitimate_2021,
	address = {New York, NY, USA},
	series = {{MM} '21},
	title = {Legitimate adversarial patches: evading human eyes and detection models in the physical world},
	isbn = {978-1-4503-8651-7},
	shorttitle = {Legitimate adversarial patches},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475653},
	doi = {10.1145/3474085.3475653},
	abstract = {It is known that deep neural models are vulnerable to adversarial attacks. Digital attacks can craft imperceptible perturbations but lack of the ability to apply in physical environment. To address this issue, efforts have been investigated to study physical patch attacks in the physical world, especially for object detection models. Previous works mostly focus on evading the detection model itself but ignore the impact of human observers. In this paper, we study legitimate adversarial attacks that evade both human eyes and detection models in the physical world. To this end, we delve into the issue of patch rationality, and propose some indicators for evaluating the rationality of physical adversarial patches. Besides, we propose a novel framework with a two-stage training strategy to generate our legitimate adversarial patches (LAPs). Both in numerical simulations and physical experiments our LAPs have significant attack effects and visual rationality.},
	language = {en},
	urldate = {2025-01-16},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Jia and Ji, Nan and Xie, Haidong and Xiang, Xueshuang},
	month = oct,
	year = {2021},
	note = {TLDR: This paper studies legitimate adversarial attacks that evade both human eyes and detection models in the physical world and proposes a novel framework with a two-stage training strategy that has significant attack effects and visual rationality.
remark: æå‡ºè§†è§‰åˆç†å¯¹æŠ—è¡¥ä¸æ”»å‡»æ¡†æ¶ã€‚
titleTranslation: åˆæ³•çš„å¯¹æŠ—è¡¥ä¸ï¼šåœ¨ç‰©ç†ä¸–ç•Œä¸­èº²é¿äººçœ¼å’Œæ£€æµ‹æ¨¡å‹
abstractTranslation: ä¼—æ‰€å‘¨çŸ¥ï¼Œæ·±åº¦ç¥ç»æ¨¡å‹å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ã€‚æ•°å­—æ”»å‡»å¯ä»¥åˆ¶é€ éš¾ä»¥å¯Ÿè§‰çš„å¹²æ‰°ï¼Œä½†ç¼ºä¹åœ¨ç‰©ç†ç¯å¢ƒä¸­åº”ç”¨çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œäººä»¬å·²ç»ç ”ç©¶äº†ç‰©ç†ä¸–ç•Œä¸­çš„ç‰©ç†è¡¥ä¸æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯å¯¹è±¡æ£€æµ‹æ¨¡å‹ã€‚ä»¥å¾€çš„ç ”ç©¶å¤§å¤šä¾§é‡äºå›é¿æ£€æµ‹æ¨¡å‹æœ¬èº«ï¼Œè€Œå¿½ç•¥äº†äººç±»è§‚å¯Ÿè€…çš„å½±å“ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨ç‰©ç†ä¸–ç•Œä¸­èº²é¿äººçœ¼å’Œæ£€æµ‹æ¨¡å‹çš„åˆæ³•å¯¹æŠ—æ”»å‡»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†è¡¥ä¸åˆç†æ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€äº›è¯„ä¼°ç‰©ç†å¯¹æŠ—è¡¥ä¸åˆç†æ€§çš„æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥ç”Ÿæˆæˆ‘ä»¬çš„åˆæ³•å¯¹æŠ—è¡¥ä¸ï¼ˆLAPï¼‰ã€‚åœ¨æ•°å€¼æ¨¡æ‹Ÿå’Œç‰©ç†å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„LAPéƒ½å…·æœ‰æ˜¾è‘—çš„æ”»å‡»æ•ˆæœå’Œè§†è§‰åˆç†æ€§ã€‚},
	keywords = {ccfInfo: Net Error: 0, citationNumber: 17},
	pages = {5307--5315},
}

@misc{openai_gpt-4o_2024,
	title = {{GPT}-4o {System} {Card}},
	url = {http://arxiv.org/abs/2410.21276},
	doi = {10.48550/arXiv.2410.21276},
	abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50{\textbackslash}\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {OpenAI and Hurst, Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, A. J. and Welihinda, Akila and Hayes, Alan and Radford, Alec and MÄ…dry, Aleksander and Baker-Whitcomb, Alex and Beutel, Alex and Borzunov, Alex and Carney, Alex and Chow, Alex and Kirillov, Alex and Nichol, Alex and Paino, Alex and Renzin, Alex and Passos, Alex Tachard and Kirillov, Alexander and Christakis, Alexi and Conneau, Alexis and Kamali, Ali and Jabri, Allan and Moyer, Allison and Tam, Allison and Crookes, Amadou and Tootoochian, Amin and Tootoonchian, Amin and Kumar, Ananya and Vallone, Andrea and Karpathy, Andrej and Braunstein, Andrew and Cann, Andrew and Codispoti, Andrew and Galu, Andrew and Kondrich, Andrew and Tulloch, Andrew and Mishchenko, Andrey and Baek, Angela and Jiang, Angela and Pelisse, Antoine and Woodford, Antonia and Gosalia, Anuj and Dhar, Arka and Pantuliano, Ashley and Nayak, Avi and Oliver, Avital and Zoph, Barret and Ghorbani, Behrooz and Leimberger, Ben and Rossen, Ben and Sokolowsky, Ben and Wang, Ben and Zweig, Benjamin and Hoover, Beth and Samic, Blake and McGrew, Bob and Spero, Bobby and Giertler, Bogo and Cheng, Bowen and Lightcap, Brad and Walkin, Brandon and Quinn, Brendan and Guarraci, Brian and Hsu, Brian and Kellogg, Bright and Eastman, Brydon and Lugaresi, Camillo and Wainwright, Carroll and Bassin, Cary and Hudson, Cary and Chu, Casey and Nelson, Chad and Li, Chak and Shern, Chan Jun and Conger, Channing and Barette, Charlotte and Voss, Chelsea and Ding, Chen and Lu, Cheng and Zhang, Chong and Beaumont, Chris and Hallacy, Chris and Koch, Chris and Gibson, Christian and Kim, Christina and Choi, Christine and McLeavey, Christine and Hesse, Christopher and Fischer, Claudia and Winter, Clemens and Czarnecki, Coley and Jarvis, Colin and Wei, Colin and Koumouzelis, Constantin and Sherburn, Dane and Kappler, Daniel and Levin, Daniel and Levy, Daniel and Carr, David and Farhi, David and Mely, David and Robinson, David and Sasaki, David and Jin, Denny and Valladares, Dev and Tsipras, Dimitris and Li, Doug and Nguyen, Duc Phong and Findlay, Duncan and Oiwoh, Edede and Wong, Edmund and Asdar, Ehsan and Proehl, Elizabeth and Yang, Elizabeth and Antonow, Eric and Kramer, Eric and Peterson, Eric and Sigler, Eric and Wallace, Eric and Brevdo, Eugene and Mays, Evan and Khorasani, Farzad and Such, Felipe Petroski and Raso, Filippo and Zhang, Francis and Lohmann, Fred von and Sulit, Freddie and Goh, Gabriel and Oden, Gene and Salmon, Geoff and Starace, Giulio and Brockman, Greg and Salman, Hadi and Bao, Haiming and Hu, Haitang and Wong, Hannah and Wang, Haoyu and Schmidt, Heather and Whitney, Heather and Jun, Heewoo and Kirchner, Hendrik and Pinto, Henrique Ponde de Oliveira and Ren, Hongyu and Chang, Huiwen and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and O'Connell, Ian and Osband, Ian and Silber, Ian and Sohl, Ian and Okuyucu, Ibrahim and Lan, Ikai and Kostrikov, Ilya and Sutskever, Ilya and Kanitscheider, Ingmar and Gulrajani, Ishaan and Coxon, Jacob and Menick, Jacob and Pachocki, Jakub and Aung, James and Betker, James and Crooks, James and Lennon, James and Kiros, Jamie and Leike, Jan and Park, Jane and Kwon, Jason and Phang, Jason and Teplitz, Jason and Wei, Jason and Wolfe, Jason and Chen, Jay and Harris, Jeff and Varavva, Jenia and Lee, Jessica Gan and Shieh, Jessica and Lin, Ji and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Jang, Joanne and Candela, Joaquin Quinonero and Beutler, Joe and Landers, Joe and Parish, Joel and Heidecke, Johannes and Schulman, John and Lachman, Jonathan and McKay, Jonathan and Uesato, Jonathan and Ward, Jonathan and Kim, Jong Wook and Huizinga, Joost and Sitkin, Jordan and Kraaijeveld, Jos and Gross, Josh and Kaplan, Josh and Snyder, Josh and Achiam, Joshua and Jiao, Joy and Lee, Joyce and Zhuang, Juntang and Harriman, Justyn and Fricke, Kai and Hayashi, Kai and Singhal, Karan and Shi, Katy and Karthik, Kavin and Wood, Kayla and Rimbach, Kendra and Hsu, Kenny and Nguyen, Kenny and Gu-Lemberg, Keren and Button, Kevin and Liu, Kevin and Howe, Kiel and Muthukumar, Krithika and Luther, Kyle and Ahmad, Lama and Kai, Larry and Itow, Lauren and Workman, Lauren and Pathak, Leher and Chen, Leo and Jing, Li and Guy, Lia and Fedus, Liam and Zhou, Liang and Mamitsuka, Lien and Weng, Lilian and McCallum, Lindsay and Held, Lindsey and Ouyang, Long and Feuvrier, Louis and Zhang, Lu and Kondraciuk, Lukas and Kaiser, Lukasz and Hewitt, Luke and Metz, Luke and Doshi, Lyric and Aflak, Mada and Simens, Maddie and Boyd, Madelaine and Thompson, Madeleine and Dukhan, Marat and Chen, Mark and Gray, Mark and Hudnall, Mark and Zhang, Marvin and Aljubeh, Marwan and Litwin, Mateusz and Zeng, Matthew and Johnson, Max and Shetty, Maya and Gupta, Mayank and Shah, Meghan and Yatbaz, Mehmet and Yang, Meng Jia and Zhong, Mengchao and Glaese, Mia and Chen, Mianna and Janner, Michael and Lampe, Michael and Petrov, Michael and Wu, Michael and Wang, Michele and Fradin, Michelle and Pokrass, Michelle and Castro, Miguel and Castro, Miguel Oom Temudo de and Pavlov, Mikhail and Brundage, Miles and Wang, Miles and Khan, Minal and Murati, Mira and Bavarian, Mo and Lin, Molly and Yesildal, Murat and Soto, Nacho and Gimelshein, Natalia and Cone, Natalie and Staudacher, Natalie and Summers, Natalie and LaFontaine, Natan and Chowdhury, Neil and Ryder, Nick and Stathas, Nick and Turley, Nick and Tezak, Nik and Felix, Niko and Kudige, Nithanth and Keskar, Nitish and Deutsch, Noah and Bundick, Noel and Puckett, Nora and Nachum, Ofir and Okelola, Ola and Boiko, Oleg and Murk, Oleg and Jaffe, Oliver and Watkins, Olivia and Godement, Olivier and Campbell-Moore, Owen and Chao, Patrick and McMillan, Paul and Belov, Pavel and Su, Peng and Bak, Peter and Bakkum, Peter and Deng, Peter and Dolan, Peter and Hoeschele, Peter and Welinder, Peter and Tillet, Phil and Pronin, Philip and Tillet, Philippe and Dhariwal, Prafulla and Yuan, Qiming and Dias, Rachel and Lim, Rachel and Arora, Rahul and Troll, Rajan and Lin, Randall and Lopes, Rapha Gontijo and Puri, Raul and Miyara, Reah and Leike, Reimar and Gaubert, Renaud and Zamani, Reza and Wang, Ricky and Donnelly, Rob and Honsby, Rob and Smith, Rocky and Sahai, Rohan and Ramchandani, Rohit and Huet, Romain and Carmichael, Rory and Zellers, Rowan and Chen, Roy and Chen, Ruby and Nigmatullin, Ruslan and Cheu, Ryan and Jain, Saachi and Altman, Sam and Schoenholz, Sam and Toizer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Culver, Sara and Ethersmith, Scott and Gray, Scott and Grove, Sean and Metzger, Sean and Hermani, Shamez and Jain, Shantanu and Zhao, Shengjia and Wu, Sherwin and Jomoto, Shino and Wu, Shirong and Shuaiqi and Xia and Phene, Sonia and Papay, Spencer and Narayanan, Srinivas and Coffey, Steve and Lee, Steve and Hall, Stewart and Balaji, Suchir and Broda, Tal and Stramer, Tal and Xu, Tao and Gogineni, Tarun and Christianson, Taya and Sanders, Ted and Patwardhan, Tejal and Cunninghman, Thomas and Degry, Thomas and Dimson, Thomas and Raoux, Thomas and Shadwell, Thomas and Zheng, Tianhao and Underwood, Todd and Markov, Todor and Sherbakov, Toki and Rubin, Tom and Stasi, Tom and Kaftan, Tomer and Heywood, Tristan and Peterson, Troy and Walters, Tyce and Eloundou, Tyna and Qi, Valerie and Moeller, Veit and Monaco, Vinnie and Kuo, Vishal and Fomenko, Vlad and Chang, Wayne and Zheng, Weiyi and Zhou, Wenda and Manassra, Wesam and Sheu, Will and Zaremba, Wojciech and Patil, Yash and Qian, Yilei and Kim, Yongjik and Cheng, Youlong and Zhang, Yu and He, Yuchen and Zhang, Yuchen and Jin, Yujia and Dai, Yunxing and Malkov, Yury},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21276 [cs]
TLDR: This System Card provides a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures the authors've implemented to ensure the model is safe and aligned.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, ccfInfo: CCF-None CORR, citationNumber: 0, â­â­},
}

@article{yang_dawn_2023,
	title = {The dawn of lmms: preliminary explorations with gpt-4v (ision)},
	volume = {9},
	shorttitle = {The dawn of lmms},
	url = {https://www.stableaiprompts.com/wp-content/uploads/2023/10/Chatgpt-Updates.pdf},
	language = {en},
	number = {1},
	urldate = {2025-01-05},
	journal = {arXiv preprint arXiv:2309.17421},
	author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
	year = {2023},
	keywords = {ccfInfo: CCF-None CORR, citationNumber: 470, â›” No DOI found},
	pages = {1},
}

@inproceedings{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	language = {en-US},
	urldate = {2024-12-30},
	booktitle = {2nd {International} {Conference} on {Learning} {Representations}, {ICLR} 2014, {Banff}, {AB}, {Canada}, {April} 14-16, 2014, {Conference} {Track} {Proceedings}},
	publisher = {arXiv},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2014},
	note = {titleTranslation: ç¥ç»ç½‘ç»œçš„æœ‰è¶£ç‰¹æ€§},
	keywords = {ccfInfo: CCF-None CORR, citationNumber: 17874, â›” No DOI found, â­â­â­},
}

@misc{hancock_run-time_2024,
	title = {Run-time {Observation} {Interventions} {Make} {Vision}-{Language}-{Action} {Models} {More} {Visually} {Robust}},
	url = {http://arxiv.org/abs/2410.01971},
	doi = {10.48550/arXiv.2410.01971},
	abstract = {Vision-language-action (VLA) models trained on large-scale internet data and robot demonstrations have the potential to serve as generalist robot policies. However, despite their large-scale training, VLAs are often brittle to taskirrelevant visual details such as distractor objects or background colors. We introduce Bring Your Own VLA (BYOVLA ): a run-time intervention scheme that (1) dynamically identifies regions of the input image that the model is sensitive to, and (2) minimally alters task-irrelevant regions to reduce the modelâ€™s sensitivity using automated image editing tools. Our approach is compatible with any off the shelf VLA without model finetuning or access to the modelâ€™s weights. Hardware experiments on language-instructed manipulation tasks demonstrate that BYOVLA enables state-of-the-art VLA models to nearly retain their nominal performance in the presence of distractor objects and backgrounds, which otherwise degrade task success rates by up to 40\%. Website with additional information, videos, and code: https://aasherh.github.io/byovla/.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Hancock, Asher J. and Ren, Allen Z. and Majumdar, Anirudha},
	month = oct,
	year = {2024},
	note = {TLDR: Bring Your Own VLA (BYOVLA) is introduced: a run-time intervention scheme that dynamically identifies regions of the input image that the model is sensitive to, and minimally alters task-irrelevant regions to reduce the model's sensitivity using automated image editing tools.
titleTranslation: è¿è¡Œæ—¶è§‚å¯Ÿå¹²é¢„ä½¿è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹åœ¨è§†è§‰ä¸Šæ›´åŠ ç¨³å¥},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, ccfInfo: CCF-None CORR, citationNumber: 0, notion, â­â­â­â­},
}

@misc{liu_spatiotemporal_2020,
	title = {Spatiotemporal {Attacks} for {Embodied} {Agents}},
	url = {http://arxiv.org/abs/2005.09161},
	doi = {10.48550/arXiv.2005.09161},
	abstract = {Adversarial attacks are valuable for providing insights into the blind-spots of deep learning models and help improve their robustness. Existing work on adversarial attacks have mainly focused on static scenes; however, it remains unclear whether such attacks are effective against embodied agents, which could navigate and interact with a dynamic environment. In this work, we take the first step to study adversarial attacks for embodied agents. In particular, we generate spatiotemporal perturbations to form 3D adversarial examples, which exploit the interaction history in both the temporal and spatial dimensions. Regarding the temporal dimension, since agents make predictions based on historical observations, we develop a trajectory attention module to explore scene view contributions, which further help localize 3D objects appeared with the highest stimuli. By conciliating with clues from the temporal dimension, along the spatial dimension, we adversarially perturb the physical properties (e.g., texture and 3D shape) of the contextual objects that appeared in the most important scene views. Extensive experiments on the EQA-v1 dataset for several embodied tasks in both the white-box and black-box settings have been conducted, which demonstrate that our perturbations have strong attack and generalization abilities.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Liu, Aishan and Huang, Tairan and Liu, Xianglong and Xu, Yitao and Ma, Yuqing and Chen, Xinyun and Maybank, Stephen J. and Tao, Dacheng},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ccfInfo: CCF-B ECCV, citationNumber: 1, notion, â­â­â­â­},
}

@inproceedings{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	language = {en-US},
	urldate = {2024-12-30},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015, {San} {Diego}, {CA}, {USA}, {May} 7-9, 2015, {Conference} {Track} {Proceedings}},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
	note = {abstractTranslation: åŒ…æ‹¬ç¥ç»ç½‘ç»œåœ¨å†…çš„å‡ ç§æœºå™¨å­¦ä¹ æ¨¡å‹æ€»æ˜¯å¯¹å¯¹æŠ—æ€§ç¤ºä¾‹è¿›è¡Œé”™è¯¯åˆ†ç±»ï¼Œå¯¹æŠ—æ€§ç¤ºä¾‹æ˜¯é€šè¿‡å¯¹æ•°æ®é›†ä¸­çš„ç¤ºä¾‹æ–½åŠ è¾ƒå°ä½†æ•…æ„æœ€åæƒ…å†µçš„æ‰°åŠ¨è€Œå½¢æˆçš„è¾“å…¥ï¼Œè¿™æ ·å—æ‰°åŠ¨çš„è¾“å…¥ä¼šå¯¼è‡´æ¨¡å‹è¾“å‡ºé«˜ç½®ä¿¡åº¦çš„é”™è¯¯ç­”æ¡ˆã€‚æ—©æœŸè§£é‡Šè¿™ä¸€ç°è±¡çš„å°è¯•ä¸»è¦é›†ä¸­åœ¨éçº¿æ€§å’Œè¿‡æ‹Ÿåˆä¸Šã€‚ç›¸åï¼Œæˆ‘ä»¬è®¤ä¸ºç¥ç»ç½‘ç»œæ˜“å—å¯¹æŠ—æ€§æ‰°åŠ¨çš„ä¸»è¦åŸå› æ˜¯å®ƒä»¬çš„çº¿æ€§ç‰¹æ€§ã€‚è¿™ä¸€è§£é‡Šå¾—åˆ°äº†æ–°çš„å®šé‡ç»“æœçš„æ”¯æŒï¼ŒåŒæ—¶é¦–æ¬¡è§£é‡Šäº†å…³äºå®ƒä»¬æœ€æœ‰è¶£çš„äº‹å®ï¼šå®ƒä»¬åœ¨æ¶æ„å’Œè®­ç»ƒé›†ä¸Šçš„æ³›åŒ–ã€‚æ­¤å¤–ï¼Œè¿™ç§è§‚ç‚¹äº§ç”Ÿäº†ä¸€ç§ç®€å•å¿«é€Ÿçš„ç”Ÿæˆå¯¹æŠ—æ€§ç¤ºä¾‹çš„æ–¹æ³•ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ä¸ºå¯¹æŠ—è®­ç»ƒæä¾›ç¤ºä¾‹ï¼Œæˆ‘ä»¬å‡å°‘äº†MNISTæ•°æ®é›†ä¸Šæœ€å¤§è¾“å‡ºç½‘ç»œçš„æµ‹è¯•é›†è¯¯å·®ã€‚
titleTranslation: è§£é‡Šå’Œåˆ©ç”¨å¯¹æŠ—æ€§ä¾‹å­},
	keywords = {ccfInfo: CCF-None CORR, citationNumber: 22823, â›” No DOI found, â­â­â­},
}

@inproceedings{liu_exploring_2024,
	address = {Melbourne VIC Australia},
	title = {Exploring the {Robustness} of {Decision}-{Level} {Through} {Adversarial} {Attacks} on {LLM}-{Based} {Embodied} {Models}},
	isbn = {979-8-4007-0686-8},
	url = {https://dl.acm.org/doi/10.1145/3664647.3680616},
	doi = {10.1145/3664647.3680616},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Liu, Shuyuan and Chen, Jiawei and Ruan, Shouwei and Su, Hang and Yin, Zhaoxia},
	month = oct,
	year = {2024},
	note = {GSCC: 0000004 
titleTranslation: é€šè¿‡å¯¹åŸºäºLLMçš„å®ä½“æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»æ¢ç´¢å†³ç­–å±‚çš„ç¨³å¥æ€§},
	keywords = {ccfInfo: CCF-A ACM MM, citationNumber: 0, notion, â­â­â­â­},
	pages = {8120--8128},
}

@misc{team_octo_2024,
	title = {Octo: {An} {Open}-{Source} {Generalist} {Robot} {Policy}},
	shorttitle = {Octo},
	url = {http://arxiv.org/abs/2405.12213},
	doi = {10.48550/arXiv.2405.12213},
	abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
	language = {en-US},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
	month = may,
	year = {2024},
	note = {GSCC: 0000151 
TLDR: è¿™é¡¹å·¥ä½œä»‹ç»äº† Octoï¼Œè¿™æ˜¯ä¸€ç§åŸºäº transformer çš„å¤§å‹ç­–ç•¥ï¼Œåœ¨ Open X-Embodiment æ•°æ®é›†ï¼ˆè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼‰çš„ 800k è½¨è¿¹ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶å±•ç¤ºäº† Octo ä½œä¸ºä¸€ç§å¤šåŠŸèƒ½ç­–ç•¥åˆå§‹åŒ–ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¾®è°ƒåˆ°æ–°çš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ã€‚
titleTranslation: Octoï¼šå¼€æºé€šæ‰æœºå™¨äººç­–ç•¥
abstractTranslation: åœ¨ä¸åŒæœºå™¨äººæ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒçš„å¤§å‹ç­–ç•¥æœ‰å¯èƒ½æ”¹å˜æœºå™¨äººå­¦ä¹ ï¼šè¿™ç§é€šç”¨çš„æœºå™¨äººç­–ç•¥å¯èƒ½åªä½¿ç”¨å°‘é‡çš„åŸŸå†…æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä½†å¯ä»¥å¹¿æ³›æ¨å¹¿ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ–°ç­–ç•¥ã€‚ç„¶è€Œï¼Œä¸ºäº†å¹¿æ³›é€‚ç”¨äºä¸€ç³»åˆ—æœºå™¨äººå­¦ä¹ åœºæ™¯ã€ç¯å¢ƒå’Œä»»åŠ¡ï¼Œæ­¤ç±»ç­–ç•¥éœ€è¦å¤„ç†ä¸åŒçš„ä¼ æ„Ÿå™¨å’ŒåŠ¨ä½œç©ºé—´ï¼Œé€‚åº”å„ç§å¸¸ç”¨çš„æœºå™¨äººå¹³å°ï¼Œå¹¶éšæ—¶æœ‰æ•ˆåœ°é’ˆå¯¹æ–°é¢†åŸŸè¿›è¡Œå¾®è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºå¼€å‘å¼€æºã€å¹¿æ³›é€‚ç”¨çš„æœºå™¨äººæ“ä½œé€šç”¨ç­–ç•¥å¥ å®šåŸºç¡€ã€‚ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å¼•å…¥äº† Octoï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº transformer çš„å¤§å‹ç­–ç•¥ï¼Œåœ¨ Open X-Embodiment æ•°æ®é›†çš„ 800k è½¨è¿¹ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æœºå™¨äººæ“ä½œæ•°æ®é›†ã€‚å®ƒå¯ä»¥é€šè¿‡è¯­è¨€å‘½ä»¤æˆ–ç›®æ ‡å›¾åƒè¿›è¡ŒæŒ‡ç¤ºï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ ‡å‡†æ¶ˆè´¹ç±» GPU ä¸Šé€šè¿‡æ–°çš„æ„Ÿå®˜è¾“å…¥å’ŒåŠ¨ä½œç©ºé—´åœ¨å‡ ä¸ªå°æ—¶å†…æœ‰æ•ˆåœ°å¾®è°ƒä¸ºæœºå™¨äººè®¾ç½®ã€‚åœ¨ 9 ä¸ªæœºå™¨äººå¹³å°ä¸Šçš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº† Octo æ˜¯ä¸€ç§å¤šåŠŸèƒ½çš„ç­–ç•¥åˆå§‹åŒ–ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¾®è°ƒåˆ°æ–°çš„è§‚å¯Ÿå’Œæ“ä½œç©ºé—´ã€‚æˆ‘ä»¬è¿˜å¯¹ Octo æ¨¡å‹çš„è®¾è®¡å†³ç­–è¿›è¡Œè¯¦ç»†çš„åˆ†æï¼Œä»æ¶æ„åˆ°è®­ç»ƒæ•°æ®ï¼Œä»¥æŒ‡å¯¼æœªæ¥æ„å»ºé€šæ‰æœºå™¨äººæ¨¡å‹çš„ç ”ç©¶ã€‚},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, ccfInfo: CCF-None CORR, citationNumber: 116, notion, â­â­â­},
}

@article{vatsa_adventures_2024,
	title = {Adventures of {Trustworthy} {Vision}-{Language} {Models}: {A} {Survey}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Adventures of {Trustworthy} {Vision}-{Language} {Models}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30275},
	doi = {10.1609/aaai.v38i20.30275},
	abstract = {Recently, transformers have become incredibly popular in computer vision and vision-language tasks. This notable rise in their usage can be primarily attributed to the capabilities offered by attention mechanisms and the outstanding ability of transformers to adapt and apply themselves to a variety of tasks and domains. Their versatility and state-of-the-art performance have established them as indispensable tools for a wide array of applications. However, in the constantly changing landscape of machine learning, the assurance of the trustworthiness of transformers holds utmost importance. This paper conducts a thorough examination of vision-language transformers, employing three fundamental principles of responsible AI: Bias, Robustness, and Interpretability. The primary objective of this paper is to delve into the intricacies and complexities associated with the practical use of transformers, with the overarching goal of advancing our comprehension of how to enhance their reliability and accountability.},
	language = {en},
	number = {20},
	urldate = {2024-12-13},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Vatsa, Mayank and Jain, Anubhooti and Singh, Richa},
	month = mar,
	year = {2024},
	note = {GSCC: 0000001 
0 citations (Crossref/DOI) [2024-12-13]
EI: æ˜¯},
	keywords = {/done, ccfInfo: CCF-A AAAI, citationNumber: 0, notion, â­â­},
	pages = {22650--22658},
}

@misc{miyai_unsolvable_2024,
	title = {Unsolvable {Problem} {Detection}: {Evaluating} {Trustworthiness} of {Vision} {Language} {Models}},
	shorttitle = {Unsolvable {Problem} {Detection}},
	url = {http://arxiv.org/abs/2403.20331},
	doi = {10.48550/arXiv.2403.20331},
	abstract = {This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of more practical and reliable VLMs.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Miyai, Atsuyuki and Yang, Jingkang and Zhang, Jingyang and Ming, Yifei and Yu, Qing and Irie, Go and Li, Yixuan and Li, Hai and Liu, Ziwei and Aizawa, Kiyoharu},
	month = mar,
	year = {2024},
	note = {TLDR: To address UPD, both training-free and training-based solutions are explored, offering new insights into their effectiveness and limitations, and it is hoped these insights will enhance the broader understanding and development of more practical and reliable VLMs.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ccfInfo: CCF-None CORR, citationNumber: 0, notion, â­â­â­},
}

@inproceedings{bharadhwaj_roboagent_2024,
	address = {Yokohama, Japan},
	title = {{RoboAgent}: {Generalization} and {Efficiency} in {Robot} {Manipulation} via {Semantic} {Augmentations} and {Action} {Chunking}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-8457-4},
	shorttitle = {{RoboAgent}},
	url = {https://ieeexplore.ieee.org/document/10611293/},
	doi = {10.1109/ICRA57147.2024.10611293},
	language = {en},
	urldate = {2024-12-13},
	booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Bharadhwaj, Homanga and Vakil, Jay and Sharma, Mohit and Gupta, Abhinav and Tulsiani, Shubham and Kumar, Vikash},
	month = may,
	year = {2024},
	note = {7 citations (Crossref/DOI) [2024-12-13]
TLDR: An efficient framework for training universal agents capable of multi-task manipulation skills using semantic augmentations that can rapidly multiply existing datasets and action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting is developed.},
	keywords = {ccfInfo: CCF-B ICRA, citationNumber: 0, notion, â­â­â­},
	pages = {4788--4795},
}

@inproceedings{radford_learning_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/radford21a.html},
	urldate = {2025-01-04},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	keywords = {ccfInfo: CCF-A ICML, citationNumber: 24429, notion, â›” No DOI found, â­â­â­},
	pages = {8748--8763},
}

@article{zhu_retrieval-augmented_nodate,
	title = {Retrieval-{Augmented} {Embodied} {Agents}},
	doi = {10.1109/CVPR52733.2024.01703},
	abstract = {Embodied agents operating in complex and uncertain environments face considerable challenges. While some advanced agents handle complex manipulation tasks with proficiency, their success often hinges on extensive training data to develop their capabilities. In contrast, humans typically rely on recalling past experiences and analogous situations to solve new problems. Aiming to emulate this human approach in robotics, we introduce the RetrievalAugmented Embodied Agent (RAEA). This innovative system equips robots with a form of shared memory, significantly enhancing their performance. Our approach integrates a policy retriever, allowing robots to access relevant strategies from an external policy memory bank based on multi-modal inputs. Additionally, a policy generator is employed to assimilate these strategies into the learning process, enabling robots to formulate effective responses to tasks. Extensive testing of RAEA in both simulated and real-world scenarios demonstrates its superior performance over traditional methods, representing a major leap forward in robotic technology.},
	language = {en},
	author = {Zhu, Yichen and Ou, Zhicai and Mou, Xiaofeng and Tang, Jian},
	note = {GSCC: 0000008 
TLDR: This innovative system equips robots with a form of shared memory, significantly enhancing their performance, and integrates a policy retriever, allowing robots to access relevant strategies from an external policy memory bank based on multi-modal inputs.},
	keywords = {ccfInfo: CCF-A CVPR, citationNumber: 0, notion, â­â­â­},
}

@misc{grattafiori_llama_2024,
	title = {The llama 3 herd of models},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	language = {en},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and GuzmÃ¡n, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Ã‡elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, VÃ­tor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]
TLDR: It is found that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks, and performs competitively with the state-of-the-art on image, video, and speech recognition tasks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, ccfInfo: CCF-None CORR, citationNumber: 0, â­â­},
}

@misc{chen_pali-x_2023,
	title = {{PaLI}-{X}: on scaling up a multilingual vision and language model},
	shorttitle = {{PaLI}-{X}},
	url = {http://arxiv.org/abs/2305.18565},
	doi = {10.48550/arXiv.2305.18565},
	abstract = {We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.},
	language = {en},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and Shakeri, Siamak and Dehghani, Mostafa and Salz, Daniel and Lucic, Mario and Tschannen, Michael and Nagrani, Arsha and Hu, Hexiang and Joshi, Mandar and Pang, Bo and Montgomery, Ceslee and Pietrzyk, Paulina and Ritter, Marvin and Piergiovanni, A. J. and Minderer, Matthias and Pavetic, Filip and Waters, Austin and Li, Gang and Alabdulmohsin, Ibrahim and Beyer, Lucas and Amelot, Julien and Lee, Kenton and Steiner, Andreas Peter and Li, Yang and Keysers, Daniel and Arnab, Anurag and Xu, Yuanzhong and Rong, Keran and Kolesnikov, Alexander and Seyedhosseini, Mojtaba and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
	month = may,
	year = {2023},
	note = {arXiv:2305.18565 [cs]
æ­¤ç¿»è¯‘å¼•æ“ä¸å¯ç”¨ï¼Œå¯èƒ½æ˜¯å¯†é’¥é”™è¯¯ï¼Œä¹Ÿå¯èƒ½æ˜¯è¯·æ±‚è¿‡å¿«ã€‚
å¯ä»¥å°è¯•å…¶ä»–ç¿»è¯‘å¼•æ“ï¼Œæˆ–è€…æ¥æ­¤æŸ¥çœ‹ç›¸å…³å›ç­”ï¼š
https://zotero.yuque.com/staff-gkhviy/pdf-trans/age09f
è¯·æ³¨æ„ï¼Œè¿™äº›é”™è¯¯ä¸ Zotero å’Œæœ¬ç¿»è¯‘æ’ä»¶æ— å…³ï¼Œç”±è¯¥ç¿»è¯‘æœåŠ¡å¼•èµ·ï¼š ç™¾åº¦ğŸ—ï¸
TLDR: PaLI-X, a multilingual vision and language model, advances the state-of-the-art on most vision-and-language benchmarks considered and observes emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.
Service error: 52003:UNAUTHORIZED USER},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ccfInfo: CCF-None CORR, citationNumber: 140, notion, â­â­},
}

@inproceedings{zitkovich_rt-2_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{RT}-2: {Vision}-{Language}-{Action} {Models} {Transfer} {Web} {Knowledge} to {Robotic} {Control}},
	volume = {229},
	shorttitle = {{RT}-2},
	url = {https://proceedings.mlr.press/v229/zitkovich23a.html},
	language = {en-US},
	urldate = {2024-12-30},
	booktitle = {Conference on {Robot} {Learning}, {CoRL} 2023, 6-9 {November} 2023, {Atlanta}, {GA}, {USA}},
	publisher = {PMLR},
	author = {Zitkovich, Brianna and Yu, Tianhe and Xu, Sichun and Xu, Peng and Xiao, Ted and Xia, Fei and Wu, Jialin and Wohlhart, Paul and Welker, Stefan and Wahid, Ayzaan and Vuong, Quan and Vanhoucke, Vincent and Tran, Huong T. and Soricut, Radu and Singh, Anikait and Singh, Jaspiar and Sermanet, Pierre and Sanketi, Pannag R. and Salazar, Grecia and Ryoo, Michael S. and Reymann, Krista and Rao, Kanishka and Pertsch, Karl and Mordatch, Igor and Michalewski, Henryk and Lu, Yao and Levine, Sergey and Lee, Lisa and Lee, Tsang-Wei Edward and Leal, Isabel and Kuang, Yuheng and Kalashnikov, Dmitry and Julian, Ryan and Joshi, Nikhil J. and Irpan, Alex and Ichter, Brian and Hsu, Jasmine and Herzog, Alexander and Hausman, Karol and Gopalakrishnan, Keerthana and Fu, Chuyuan and Florence, Pete and Finn, Chelsea and Dubey, Kumar Avinava and Driess, Danny and Ding, Tianli and Choromanski, Krzysztof Marcin and Chen, Xi and Chebotar, Yevgen and Carbajal, Justice and Brown, Noah and Brohan, Anthony and Arenas, Montserrat Gonzalez and Han, Kehang},
	editor = {Tan, Jie and Toussaint, Marc and Darvish, Kourosh},
	year = {2023},
	note = {GSCC: 0000735 
TLDR: This work proposes a simple, general recipe to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web.
titleTranslation: RT-2ï¼šè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹å°†ç½‘ç»œçŸ¥è¯†è½¬ç§»åˆ°æœºå™¨äººæ§åˆ¶},
	keywords = {ccfInfo: CCF-None CORL, citationNumber: 466, notion, â›” No DOI found, â­â­â­},
	pages = {2165--2183},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2024-12-31},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	note = {TLDR: A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called "dropout" that proved to be very effective.},
	keywords = {ccfInfo: CCF-A NeurIPS, citationNumber: 34234, â­â­},
	pages = {84--90},
}

@misc{collaboration_open_2024,
	title = {Open {X}-{Embodiment}: {Robotic} {Learning} {Datasets} and {RT}-{X} {Models}},
	shorttitle = {Open {X}-{Embodiment}},
	url = {http://arxiv.org/abs/2310.08864},
	doi = {10.48550/arXiv.2310.08864},
	abstract = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website https://robotics-transformer-x.github.io.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Collaboration, Open X.-Embodiment and O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and Tung, Albert and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Gupta, Anchit and Wang, Andrew and Kolobov, Andrey and Singh, Anikait and Garg, Animesh and Kembhavi, Aniruddha and Xie, Annie and Brohan, Anthony and Raffin, Antonin and Sharma, Archit and Yavary, Arefeh and Jain, Arhan and Balakrishna, Ashwin and Wahid, Ayzaan and Burgess-Limerick, Ben and Kim, Beomjoon and SchÃ¶lkopf, Bernhard and Wulfe, Blake and Ichter, Brian and Lu, Cewu and Xu, Charles and Le, Charlotte and Finn, Chelsea and Wang, Chen and Xu, Chenfeng and Chi, Cheng and Huang, Chenguang and Chan, Christine and Agia, Christopher and Pan, Chuer and Fu, Chuyuan and Devin, Coline and Xu, Danfei and Morton, Daniel and Driess, Danny and Chen, Daphne and Pathak, Deepak and Shah, Dhruv and BÃ¼chler, Dieter and Jayaraman, Dinesh and Kalashnikov, Dmitry and Sadigh, Dorsa and Johns, Edward and Foster, Ethan and Liu, Fangchen and Ceola, Federico and Xia, Fei and Zhao, Feiyu and Frujeri, Felipe Vieira and Stulp, Freek and Zhou, Gaoyue and Sukhatme, Gaurav S. and Salhotra, Gautam and Yan, Ge and Feng, Gilbert and Schiavi, Giulio and Berseth, Glen and Kahn, Gregory and Yang, Guangwen and Wang, Guanzhi and Su, Hao and Fang, Hao-Shu and Shi, Haochen and Bao, Henghui and Amor, Heni Ben and Christensen, Henrik I. and Furuta, Hiroki and Bharadhwaj, Homanga and Walke, Homer and Fang, Hongjie and Ha, Huy and Mordatch, Igor and Radosavovic, Ilija and Leal, Isabel and Liang, Jacky and Abou-Chakra, Jad and Kim, Jaehyung and Drake, Jaimyn and Peters, Jan and Schneider, Jan and Hsu, Jasmine and Vakil, Jay and Bohg, Jeannette and Bingham, Jeffrey and Wu, Jeffrey and Gao, Jensen and Hu, Jiaheng and Wu, Jiajun and Wu, Jialin and Sun, Jiankai and Luo, Jianlan and Gu, Jiayuan and Tan, Jie and Oh, Jihoon and Wu, Jimmy and Lu, Jingpei and Yang, Jingyun and Malik, Jitendra and SilvÃ©rio, JoÃ£o and Hejna, Joey and Booher, Jonathan and Tompson, Jonathan and Yang, Jonathan and Salvador, Jordi and Lim, Joseph J. and Han, Junhyek and Wang, Kaiyuan and Rao, Kanishka and Pertsch, Karl and Hausman, Karol and Go, Keegan and Gopalakrishnan, Keerthana and Goldberg, Ken and Byrne, Kendra and Oslund, Kenneth and Kawaharazuka, Kento and Black, Kevin and Lin, Kevin and Zhang, Kevin and Ehsani, Kiana and Lekkala, Kiran and Ellis, Kirsty and Rana, Krishan and Srinivasan, Krishnan and Fang, Kuan and Singh, Kunal Pratap and Zeng, Kuo-Hao and Hatch, Kyle and Hsu, Kyle and Itti, Laurent and Chen, Lawrence Yunliang and Pinto, Lerrel and Fei-Fei, Li and Tan, Liam and Fan, Linxi "Jim" and Ott, Lionel and Lee, Lisa and Weihs, Luca and Chen, Magnum and Lepert, Marion and Memmel, Marius and Tomizuka, Masayoshi and Itkina, Masha and Castro, Mateo Guaman and Spero, Max and Du, Maximilian and Ahn, Michael and Yip, Michael C. and Zhang, Mingtong and Ding, Mingyu and Heo, Minho and Srirama, Mohan Kumar and Sharma, Mohit and Kim, Moo Jin and Kanazawa, Naoaki and Hansen, Nicklas and Heess, Nicolas and Joshi, Nikhil J. and Suenderhauf, Niko and Liu, Ning and Palo, Norman Di and Shafiullah, Nur Muhammad Mahi and Mees, Oier and Kroemer, Oliver and Bastani, Osbert and Sanketi, Pannag R. and Miller, Patrick "Tree" and Yin, Patrick and Wohlhart, Paul and Xu, Peng and Fagan, Peter David and Mitrano, Peter and Sermanet, Pierre and Abbeel, Pieter and Sundaresan, Priya and Chen, Qiuyu and Vuong, Quan and Rafailov, Rafael and Tian, Ran and Doshi, Ria and Mart'in-Mart'in, Roberto and Baijal, Rohan and Scalise, Rosario and Hendrix, Rose and Lin, Roy and Qian, Runjia and Zhang, Ruohan and Mendonca, Russell and Shah, Rutav and Hoque, Ryan and Julian, Ryan and Bustamante, Samuel and Kirmani, Sean and Levine, Sergey and Lin, Shan and Moore, Sherry and Bahl, Shikhar and Dass, Shivin and Sonawani, Shubham and Tulsiani, Shubham and Song, Shuran and Xu, Sichun and Haldar, Siddhant and Karamcheti, Siddharth and Adebola, Simeon and Guist, Simon and Nasiriany, Soroush and Schaal, Stefan and Welker, Stefan and Tian, Stephen and Ramamoorthy, Subramanian and Dasari, Sudeep and Belkhale, Suneel and Park, Sungjae and Nair, Suraj and Mirchandani, Suvir and Osa, Takayuki and Gupta, Tanmay and Harada, Tatsuya and Matsushima, Tatsuya and Xiao, Ted and Kollar, Thomas and Yu, Tianhe and Ding, Tianli and Davchev, Todor and Zhao, Tony Z. and Armstrong, Travis and Darrell, Trevor and Chung, Trinity and Jain, Vidhi and Kumar, Vikash and Vanhoucke, Vincent and Zhan, Wei and Zhou, Wenxuan and Burgard, Wolfram and Chen, Xi and Chen, Xiangyu and Wang, Xiaolong and Zhu, Xinghao and Geng, Xinyang and Liu, Xiyuan and Liangwei, Xu and Li, Xuanlin and Pang, Yansong and Lu, Yao and Ma, Yecheng Jason and Kim, Yejin and Chebotar, Yevgen and Zhou, Yifan and Zhu, Yifeng and Wu, Yilin and Xu, Ying and Wang, Yixuan and Bisk, Yonatan and Dou, Yongqiang and Cho, Yoonyoung and Lee, Youngwoon and Cui, Yuchen and Cao, Yue and Wu, Yueh-Hua and Tang, Yujin and Zhu, Yuke and Zhang, Yunchu and Jiang, Yunfan and Li, Yunshuang and Li, Yunzhu and Iwasawa, Yusuke and Matsuo, Yutaka and Ma, Zehan and Xu, Zhuo and Cui, Zichen Jeff and Zhang, Zichen and Fu, Zipeng and Lin, Zipeng},
	month = jun,
	year = {2024},
	note = {GSCC: 0000286 
TLDR: æœ¬æ–‡æä¾›äº†æ ‡å‡†åŒ–æ•°æ®æ ¼å¼å’Œæ¨¡å‹çš„æ•°æ®é›†ï¼Œä½¿æ¢ç´¢æœºå™¨äººæ“ä½œèƒŒæ™¯ä¸‹é€šç”¨ X æœºå™¨äººç­–ç•¥çš„å¯èƒ½æ€§æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶å®éªŒç»“æœä¹Ÿæä¾›äº†æœ‰æ•ˆ X æœºå™¨äººç­–ç•¥çš„ç¤ºä¾‹ã€‚
titleTranslation: Open X-Implementationï¼šæœºå™¨äººå­¦ä¹ æ•°æ®é›†å’ŒRT-Xæ¨¡å‹
abstractTranslation: åœ¨ä¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒçš„å¤§å‹ã€é«˜å®¹é‡æ¨¡å‹åœ¨æœ‰æ•ˆå¤„ç†ä¸‹æ¸¸åº”ç”¨ç¨‹åºæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚åœ¨ä» NLP åˆ°è®¡ç®—æœºè§†è§‰çš„é¢†åŸŸä¸­ï¼Œè¿™å¯¼è‡´äº†é¢„è®­ç»ƒæ¨¡å‹çš„æ•´åˆï¼Œé€šç”¨çš„é¢„è®­ç»ƒä¸»å¹²æ˜¯è®¸å¤šåº”ç”¨ç¨‹åºçš„èµ·ç‚¹ã€‚è¿™æ ·çš„æ•´åˆä¼šåœ¨æœºå™¨äººé¢†åŸŸå‘ç”Ÿå—ï¼Ÿä¼ ç»Ÿä¸Šï¼Œæœºå™¨äººå­¦ä¹ æ–¹æ³•ä¸ºæ¯ä¸ªåº”ç”¨ç¨‹åºã€æ¯ä¸ªæœºå™¨äººç”šè‡³æ¯ä¸ªç¯å¢ƒè®­ç»ƒä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬èƒ½å¦è®­ç»ƒé€šæ‰ X æœºå™¨äººç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”æ–°çš„æœºå™¨äººã€ä»»åŠ¡å’Œç¯å¢ƒï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æ ‡å‡†åŒ–æ•°æ®æ ¼å¼å’Œæ¨¡å‹çš„æ•°æ®é›†ï¼Œä»¥ä¾¿åœ¨æœºå™¨äººæ“ä½œçš„èƒŒæ™¯ä¸‹æ¢ç´¢è¿™ç§å¯èƒ½æ€§ï¼Œä»¥åŠæä¾›æœ‰æ•ˆ X æœºå™¨äººç­–ç•¥ç¤ºä¾‹çš„å®éªŒç»“æœã€‚æˆ‘ä»¬ä» 21 ä¸ªæœºæ„åˆä½œæ”¶é›†çš„ 22 ä¸ªä¸åŒæœºå™¨äººä¸­æ±‡ç¼–äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå±•ç¤ºäº† 527 é¡¹æŠ€èƒ½ï¼ˆ160266ä»»åŠ¡ï¼‰ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨è¿™äº›æ•°æ®ä¸Šè®­ç»ƒçš„é«˜å®¹é‡æ¨¡å‹ï¼ˆæˆ‘ä»¬ç§°ä¸º RT-Xï¼‰è¡¨ç°å‡ºæ­£è½¬ç§»ï¼Œå¹¶é€šè¿‡åˆ©ç”¨å…¶ä»–å¹³å°çš„ç»éªŒæé«˜äº†å¤šä¸ªæœºå™¨äººçš„èƒ½åŠ›ã€‚æ›´å¤šè¯¦æƒ…å¯ä»¥åœ¨ https://robotics-transformer-x.github.io çš„é¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚},
	keywords = {Computer Science - Robotics, ccfInfo: CCF-None CORR, citationNumber: 0, notion, â­â­â­},
}

@misc{alec_radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a ï¬xed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efï¬cient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of ï¬ne-grained object classiï¬cation. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset speciï¬c training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {{Alec Radford} and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ccfInfo: CCF-A ICML, citationNumber: 24429, notion, â­â­â­},
}

@misc{sumers_distilling_2023,
	title = {Distilling {Internet}-{Scale} {Vision}-{Language} {Models} into {Embodied} {Agents}},
	url = {http://arxiv.org/abs/2301.12507},
	doi = {10.48550/arXiv.2301.12507},
	abstract = {Instruction-following agents must ground language into their observation and action spaces. Yet learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agentâ€™s behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Sumers, Theodore and Marino, Kenneth and Ahuja, Arun and Fergus, Rob and Dasgupta, Ishita},
	month = jun,
	year = {2023},
	note = {TLDR: This work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.},
	keywords = {Computer Science - Artificial Intelligence, ccfInfo: CCF-A ICML, citationNumber: 13, notion, â­â­â­},
}

@inproceedings{driess_palm-e_2023,
	title = {{PaLM}-{E}: an embodied multimodal language model},
	shorttitle = {{PaLM}-{E}},
	url = {https://proceedings.mlr.press/v202/driess23a.html},
	abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multimodal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
	language = {en},
	urldate = {2025-01-04},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498
shortConferenceName: ICML},
	keywords = {ccfInfo: CCF-A ICML, citationNumber: 1115, â­â­â­},
	pages = {8469--8488},
}

@inproceedings{zha_distilling_2024,
	address = {Yokohama, Japan},
	title = {Distilling and {Retrieving} {Generalizable} {Knowledge} for {Robot} {Manipulation} via {Language} {Corrections}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-8457-4},
	url = {https://ieeexplore.ieee.org/document/10610455/},
	doi = {10.1109/ICRA57147.2024.10610455},
	abstract = {Todayâ€™s robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via LLMs [1] by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations. We show further results and videos on our project website: https://sites.google.com/stanford.edu/droc.},
	language = {en},
	urldate = {2024-12-13},
	booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Zha, Lihan and Cui, Yuchen and Lin, Li-Heng and Kwon, Minae and Arenas, Montserrat Gonzalez and Zeng, Andy and Xia, Fei and Sadigh, Dorsa},
	month = may,
	year = {2024},
	note = {1 citations (Crossref/DOI) [2024-12-13]
TLDR: Distillation and Retrieval of Online Corrections (DROC) is presented, a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings.},
	keywords = {ccfInfo: CCF-B ICRA, citationNumber: 0, notion, â­â­â­},
	pages = {15172--15179},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-thought prompting elicits reasoning in large language models},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	language = {en},
	urldate = {2025-01-04},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ccfInfo: CCF-A NeurIPS, citationNumber: 8753, â­â­},
}

@inproceedings{brohan_rt-1_2023,
	title = {{RT}-1: robotics transformer for real-world control at scale},
	shorttitle = {Rt-1},
	doi = {10.15607/RSS.2023.XIX.025},
	language = {en},
	booktitle = {Robotics: {Science} and {Systems} {XIX}, {Daegu}, {Republic} of {Korea}, {July} 10-14, 2023},
	publisher = {arXiv},
	author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael S. and Salazar, Grecia and Sanketi, Pannag R. and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong T. and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
	editor = {Bekris, Kostas E. and Hauser, Kris and Herbert, Sylvia L. and Yu, Jingjin},
	year = {2023},
	note = {732 citations (Semantic Scholar/DOI) [2024-12-13]
TLDR: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¢«ç§°ä¸º Robotics Transformer çš„æ¨¡å‹ç±»ï¼Œå®ƒå±•ç°äº†æœ‰å‰é€”çš„å¯æ‰©å±•æ¨¡å‹å±æ€§ï¼Œå¹¶éªŒè¯äº†å¯¹ä¸åŒæ¨¡å‹ç±»çš„ç ”ç©¶ä¸­çš„ç»“è®ºä»¥åŠå®ƒä»¬åŸºäºæ•°æ®å¤§å°ã€æ¨¡å‹å¤§å°å’Œæ•°æ®å¤šæ ·æ€§çš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹æ‰§è¡Œç°å®ä¸–ç•Œä»»åŠ¡çš„çœŸå®æœºå™¨äººè¿›è¡Œå¤§è§„æ¨¡æ•°æ®æ”¶é›†ã€‚
titleTranslation: RT-1ï¼šç”¨äºå¤§è§„æ¨¡ç°å®ä¸–ç•Œæ§åˆ¶çš„æœºå™¨äººå˜å‹å™¨},
	keywords = {ccfInfo: CCF-None RSS, citationNumber: 587, notion, â­â­â­},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2025-01-05},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	note = {GSCC: 0251352},
	keywords = {ccfInfo: CCF-A CVPR, citationNumber: 13, â­â­},
	pages = {770--778},
}

@inproceedings{liu_libero_2023,
	title = {{LIBERO}: {Benchmarking} {Knowledge} {Transfer} for {Lifelong} {Robot} {Learning}},
	shorttitle = {{LIBERO}},
	url = {http://papers.nips.cc/paper\_files/paper/2023/hash/8c3c666820ea055a77726d66fc7d447f-Abstract-Datasets\_and\_Benchmarks.html},
	urldate = {2024-12-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 36: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2023, {NeurIPS} 2023, {New} {Orleans}, {LA}, {USA}, {December} 10 - 16, 2023},
	publisher = {arXiv},
	author = {Liu, Bo and Zhu, Yifeng and Gao, Chongkai and Feng, Yihao and Liu, Qiang and Zhu, Yuke and Stone, Peter},
	editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
	year = {2023},
	note = {TLDR: An extendible procedural generation pipeline that can in principle generate infinitely many tasks and highlight five key research topics in LLDM, including how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both.
titleTranslation: LIBEROï¼šç»ˆèº«æœºå™¨äººå­¦ä¹ çš„åŸºå‡†çŸ¥è¯†è½¬ç§»},
	keywords = {ccfInfo: CCF-A NeurIPS, citationNumber: 0, â­â­â­},
}

@inproceedings{zhang_remodiffuse_2023,
	address = {Paris, France},
	title = {{ReMoDiffuse}: {Retrieval}-{Augmented} {Motion} {Diffusion} {Model}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0718-4},
	shorttitle = {{ReMoDiffuse}},
	url = {https://ieeexplore.ieee.org/document/10378119/},
	doi = {10.1109/ICCV51070.2023.00040},
	language = {en},
	urldate = {2024-12-13},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhang, Mingyuan and Guo, Xinying and Pan, Liang and Cai, Zhongang and Hong, Fangzhou and Li, Huirong and Yang, Lei and Liu, Ziwei},
	month = oct,
	year = {2023},
	note = {GSCC: 0000130 
30 citations (Crossref/DOI) [2024-12-13]
TLDR: ReMoDiffuse is proposed, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process and outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality.},
	keywords = {ccfInfo: CCF-A ICCV, citationNumber: 0, notion},
	pages = {364--373},
}

@inproceedings{zhai_sigmoid_2023,
	address = {Paris, France},
	title = {Sigmoid loss for language image pre-training},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0718-4},
	url = {https://ieeexplore.ieee.org/document/10377550/},
	doi = {10.1109/ICCV51070.2023.01100},
	language = {en},
	urldate = {2025-01-05},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
	month = oct,
	year = {2023},
	note = {GSCC: 0000547 
TLDR: A simple pairwise sigmoid loss for imagetext pre-training operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization, which allows further scaling up the batch size, while also performing better at smaller batch sizes.},
	keywords = {ccfInfo: CCF-A ICCV, citationNumber: 47},
	pages = {11941--11952},
}

@misc{lin_showui_2024,
	title = {{ShowUI}: {One} {Vision}-{Language}-{Action} {Model} for {GUI} {Visual} {Agent}},
	shorttitle = {{ShowUI}},
	url = {http://arxiv.org/abs/2411.17465},
	doi = {10.48550/arXiv.2411.17465},
	abstract = {Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a visionlanguage-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale Highquality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1\% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33\% of redundant visual tokens during training and speeds up the performance by 1.4Ã—. Navigation experiments across web [12], mobile [36], and online [40] environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Lin, Kevin Qinghong and Li, Linjie and Gao, Difei and Yang, Zhengyuan and Wu, Shiwei and Bai, Zechen and Lei, Weixian and Wang, Lijuan and Shou, Mike Zheng},
	month = nov,
	year = {2024},
	note = {GSCC: 0000000 
TLDR: This work develops a vision-language-action model in digital world, namely ShowUI, which features the following innovations: UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks.
titleTranslation: ShowUIï¼šç”¨äº GUI å¯è§†åŒ–ä»£ç†çš„å•ä¸€è§†è§‰-è¯­è¨€-æ“ä½œæ¨¡å‹
abstractTranslation: æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ ï¼ˆGUIï¼‰ åŠ©æ‰‹åœ¨æé«˜äººç±»å·¥ä½œæµç¨‹ç”Ÿäº§åŠ›æ–¹é¢å…·æœ‰é‡è¦å‰æ™¯ã€‚è™½ç„¶å¤§å¤šæ•°ä»£ç†éƒ½æ˜¯åŸºäºè¯­è¨€çš„ï¼Œä¾èµ–äºå…·æœ‰æ–‡æœ¬ä¸°å¯Œå…ƒä¿¡æ¯ï¼ˆä¾‹å¦‚ HTML æˆ–è¾…åŠ©åŠŸèƒ½æ ‘ï¼‰çš„é—­æº APIï¼Œä½†å®ƒä»¬åœ¨åƒäººç±»ä¸€æ ·æ„ŸçŸ¥ UI è§†è§‰æ•ˆæœæ–¹é¢è¡¨ç°å‡ºå±€é™æ€§ï¼Œçªå‡ºäº†å¯¹ GUI å¯è§†åŒ–ä»£ç†çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ•°å­—ä¸–ç•Œä¸­çš„è§†è§‰è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼Œå³ ShowUIï¼Œå®ƒå…·æœ‰ä»¥ä¸‹åˆ›æ–°æ€§ï¼šï¼ˆiï¼‰ UI å¼•å¯¼çš„è§†è§‰ä»¤ç‰Œé€‰æ‹©ï¼Œé€šè¿‡å°†å±å¹•æˆªå›¾è¡¨è¿°ä¸º UI è¿æ¥å›¾æ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œè‡ªé€‚åº”åœ°è¯†åˆ«å®ƒä»¬çš„å†—ä½™å…³ç³»ï¼Œå¹¶ä½œä¸ºè‡ªæˆ‘æ³¨æ„å—æœŸé—´ä»¤ç‰Œé€‰æ‹©çš„æ ‡å‡†;ï¼ˆiiï¼‰ äº¤é”™è§†è§‰-è¯­è¨€-æ“ä½œæµï¼Œçµæ´»åœ°ç»Ÿä¸€ GUI ä»»åŠ¡ä¸­çš„ä¸åŒéœ€æ±‚ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆç®¡ç†å¯¼èˆªä¸­çš„è§†è§‰æ“ä½œå†å²è®°å½•ï¼Œæˆ–å°†æ¯ä¸ªå±å¹•æˆªå›¾çš„å¤šè½®æŸ¥è¯¢æ“ä½œåºåˆ—é…å¯¹ä»¥æé«˜è®­ç»ƒæ•ˆç‡;ï¼ˆiiiï¼‰ é€šè¿‡ä»”ç»†çš„æ•°æ®ç®¡ç†å’Œé‡‡ç”¨é‡é‡‡æ ·ç­–ç•¥æ¥è§£å†³é‡å¤§æ•°æ®ç±»å‹ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œä»è€Œæä¾›å°è§„æ¨¡çš„é«˜è´¨é‡ GUI æŒ‡ä»¤è·Ÿè¸ªæ•°æ®é›†ã€‚å€ŸåŠ©ä¸Šè¿°ç»„ä»¶ï¼ŒShowUI è¿™æ¬¾ä½¿ç”¨ 256K æ•°æ®çš„è½»é‡çº§ 2B æ¨¡å‹ï¼Œå®ç°äº† 75.1\% çš„æˆªå›¾é›¶ç‚¹æ¥åœ°å‡†ç¡®ç‡ã€‚å…¶ UI å¼•å¯¼çš„æ ‡è®°é€‰æ‹©è¿›ä¸€æ­¥å‡å°‘äº†è®­ç»ƒæœŸé—´ 33\% çš„å†—ä½™è§†è§‰æ ‡è®°ï¼Œå¹¶å°†æ€§èƒ½æé«˜äº† 1.4Ã—ã€‚è·¨ Web [12]ã€ç§»åŠ¨ [36] å’Œåœ¨çº¿ [40] ç¯å¢ƒçš„å¯¼èˆªå®éªŒè¿›ä¸€æ­¥å¼ºè°ƒäº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¨è¿› GUI å¯è§†åŒ–ä»£ç†æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚è¿™äº›æ¨¡å‹å¯åœ¨ https://github.com/showlab/ShowUI è´­ä¹°ã€‚},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, ccfInfo: Not Found, citationNumber: 0, notion},
}

@article{__2024,
	title = {åŸºäºå…·èº«æ™ºèƒ½çš„ç§»åŠ¨æ“ä½œæœºå™¨äººç³»ç»Ÿå‘å±•ç ”ç©¶},
	volume = {26},
	issn = {1009-1742},
	abstract = {å…·èº«æ™ºèƒ½æ˜¯æ–°ä¸€è½®ç§‘æŠ€é©å‘½ä¸äº§ä¸šå˜é©ä¸­çš„æˆ˜ç•¥æ€§æŠ€æœ¯ï¼Œæ˜¯å½“å‰ä¸–ç•Œå„å›½é‡ç‚¹ç«äº‰çš„å‰æ²¿é«˜åœ°ä¹‹ä¸€ï¼›ç§»åŠ¨æ“ä½œæœºå™¨äººç³»ç»Ÿå› å…¶ä¼˜ç§€çš„è¿åŠ¨ã€è§„åˆ’ã€æ‰§è¡Œèƒ½åŠ›æˆä¸ºå…·èº«æŠ€æœ¯é¦–é€‰çš„ç¡¬ä»¶è½½ä½“ï¼›åŸºäºå…·èº«æ™ºèƒ½çš„ç§»åŠ¨æ“ä½œæœºå™¨äººç³»ç»Ÿä½œä¸ºå®ç°è·¨é¢†åŸŸã€å¤šåœºæ™¯ã€å¤šåŠŸèƒ½çš„è‡ªä¸»å…·èº«æ™ºèƒ½å¹³å°ï¼Œå°†æˆä¸ºå¼•é¢†æœªæ¥æ–°ä¸€ä»£ä¿¡æ¯æŠ€æœ¯å’Œäººå·¥æ™ºèƒ½å‘å±•çš„å…³é”®ã€‚æœ¬æ–‡ä»åŸºäºå…·èº«æ™ºèƒ½çš„ç§»åŠ¨æ“ä½œæœºå™¨äººç³»ç»Ÿå‘å±•çš„éœ€æ±‚å‡ºå‘ï¼Œæ€»ç»“äº†åŸºäºå…·èº«æ™ºèƒ½çš„ç§»åŠ¨æ“ä½œæœºå™¨äººç³»ç»Ÿçš„å‘å±•ç°çŠ¶ï¼Œåˆ†æäº†è¯¥é¢†åŸŸå‘å±•é¢ä¸´çš„é—®é¢˜å’ŒæŒ‘æˆ˜ï¼Œæå‡ºäº†æ¶µç›–å¤šæ¨¡æ€æ„ŸçŸ¥æŠ€æœ¯ã€ä¸–ç•Œè®¤çŸ¥ä¸ç†è§£æŠ€æœ¯ã€æ™ºèƒ½è‡ªä¸»å†³ç­–æŠ€æœ¯ã€è¿åŠ¨ä¸æ“ä½œè”åˆè§„åˆ’æŠ€æœ¯ç­‰åŸºäºå…·èº«æ™ºèƒ½çš„ç§»åŠ¨æ“ä½œæœºå™¨äººç³»ç»Ÿçš„å…±æ€§å…³é”®æŠ€æœ¯ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡ä»å›½å®¶æ”¿ç­–å€¾æ–œã€å…±æ€§æŠ€æœ¯çªç ´ã€äº¤å‰å­¦ç§‘å»ºè®¾ä¸äººæ‰åŸ¹å…»ã€ç»¼åˆéªŒè¯å¹³å°æ„å»ºç­‰æ–¹é¢æå‡ºäº†å¯¹ç­–å»ºè®®ï¼Œä»¥æœŸåŠ©åŠ›å…·èº«æ™ºèƒ½å‘å±•æµªæ½®ä¸‹æˆ‘å›½ç§»åŠ¨æ“ä½œæœºå™¨äººé¢†åŸŸçš„é•¿è¶³å‘å±•ã€‚},
	language = {zh},
	number = {1},
	journal = {ä¸­å›½å·¥ç¨‹ç§‘å­¦},
	author = {å…°, æ²£åœ and èµµ, æ–‡åš and æœ±, å‡¯ and å¼ , æ¶›},
	year = {2024},
	keywords = {ccfInfo: Not Found, citationNumber: 0, â›” No DOI found, ä»»åŠ¡å’Œè¿åŠ¨è”åˆè§„åˆ’, å…·èº«æ™ºèƒ½, æ™ºèƒ½å†³ç­–, ç§»åŠ¨æ“ä½œæœºå™¨äºº},
	pages = {139--148},
}

@article{betker_improving_nodate,
	title = {Improving image generation with better captions},
	abstract = {We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.},
	language = {en},
	author = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
	keywords = {ccfInfo: Not Found, citationNumber: 418, â›” No DOI found},
}

@article{gupta_embodied_2021,
	title = {Embodied intelligence via learning and evolution},
	volume = {12},
	issn = {2041-1723},
	doi = {10.1038/s41467-021-25874-z},
	abstract = {Abstract The intertwined processes of learning and evolution in complex environmental niches have resulted in a remarkable diversity of morphological forms. Moreover, many aspects of animal intelligence are deeply embodied in these evolved morphologies. However, the principles governing relations between environmental complexity, evolved morphology, and the learnability of intelligent control, remain elusive, because performing large-scale in silico experiments on evolution and learning is challenging. Here, we introduce Deep Evolutionary Reinforcement Learning (DERL): a computational framework which can evolve diverse agent morphologies to learn challenging locomotion and manipulation tasks in complex environments. Leveraging DERL we demonstrate several relations between environmental complexity, morphological intelligence and the learnability of control. First, environmental complexity fosters the evolution of morphological intelligence as quantified by the ability of a morphology to facilitate the learning of novel tasks. Second, we demonstrate a morphological Baldwin effect i.e., in our simulations evolution rapidly selects morphologies that learn faster, thereby enabling behaviors learned late in the lifetime of early ancestors to be expressed early in the descendants lifetime. Third, we suggest a mechanistic basis for the above relationships through the evolution of morphologies that are more physically stable and energy efficient, and can therefore facilitate learning and control.},
	language = {en},
	number = {1},
	urldate = {2025-01-03},
	journal = {Nature Communications},
	author = {Gupta, Agrim and Savarese, Silvio and Ganguli, Surya and Fei-Fei, Li},
	month = oct,
	year = {2021},
	note = {TLDR: A new framework, deep evolutionary reinforcement learning, evolves agents with diverse morphologies to learn hard locomotion and manipulation tasks in complex environments, and reveals insights into relations between environmental physics, embodied intelligence, and the evolution of rapid learning.},
	keywords = {ccfInfo: CCF-None CORR, citationNumber: 211},
	pages = {5721},
}

@inproceedings{jia_scaling_2021,
	title = {Scaling up visual and vision-language representation learning with noisy text supervision},
	url = {https://proceedings.mlr.press/v139/jia21b.html},
	abstract = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
	language = {en},
	urldate = {2025-01-04},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498
shortConferenceName: ICML},
	keywords = {ccfInfo: CCF-A ICML, citationNumber: 3486},
	pages = {4904--4916},
}

@article{simpkins_cybernetics_2012,
	title = {Cybernetics: or control and communications in the animal and the machine (wiener, {N}.) [on the shelf]},
	volume = {19},
	issn = {1070-9932},
	shorttitle = {Cybernetics},
	doi = {10.1109/MRA.2012.2192815},
	language = {en},
	number = {2},
	urldate = {2025-01-03},
	journal = {IEEE Robotics and Automation Magazine},
	author = {Simpkins, C. and Simpkins, Annellen},
	month = jun,
	year = {2012},
	note = {TLDR: This book traces the core principles of this field during its process of discovery and inspires the contemporary roboticist to think broadly and be open to innovative applications.},
	keywords = {ccfInfo: Not Found, citationNumber: 4},
	pages = {94--95},
}

@incollection{turing_computing_2004,
	title = {Computing machinery and intelligence (1950)},
	isbn = {978-0-19-825079-1 978-0-19-191652-6},
	abstract = {Abstract Together with `On Computable Numbers', `Computing Machinery and Intelligence' forms Turing's best-known work. This elegant and sometimes amusing essay was originally published in 1950 in the leading philosophy journal Mind. Turing's friend Robin Gandy (like Turing a mathematical logician) said that `Computing Machinery and Intelligence'â€¦ was intended not so much as a penetrating contribution to philosophy but as propaganda. Turing thought the time had come for philosophers and mathematicians and scientists to take seriously the fact that computers were not merely calculating engines but were capable of behaviour which must be accounted as intelligent; he sought to persuade people that this was so. He wrote this paperâ€”unlike his mathematical papersâ€”quickly and with enjoyment. I can remember him reading aloud to me some of the passagesâ€” always with a smile, sometimes with a giggle. The quality and originality of `Computing Machinery and Intelligence' have earned it a place among the classics of philosophy of mind. `Computing Machinery and Intelligence' contains Turing's principal exposition of the famous `imitation game' or Turing test. The test first appeared, in a restricted form, in the closing paragraphs of `Intelligent Machinery' (Chapter 10). Chapters 13 and 14, dating from 1951 and 1952 respectively, contain further discussion and amplification; unpublished until 1999, this important additional material throws new light on how the Turing test is to be understood. The imitation game involves three participants: a computer, a human interrogator, and a human `foil'. The interrogator attempts to determine, by asking questions of the other two participants, which of them is the computer. All communication is via keyboard and screen, or an equivalent arrangement (Turing suggested a teleprinter link). The interrogator may ask questions as penetrating and wide-ranging as he or she likes, and the computer is permitted to do everything possible to force a wrong identification. (So the computer might answer `No' in response to `Are you a computer?' and might follow a request to multiply one large number by another with a long pause and a plausibly incorrect answer.) The foil must help the interrogator to make a correct identification.},
	language = {en},
	urldate = {2025-01-03},
	booktitle = {The {Essential} {Turing}},
	publisher = {Oxford University PressOxford},
	author = {Turing, Alan},
	editor = {Copeland, B J},
	month = sep,
	year = {2004},
	doi = {10.1093/oso/9780198250791.003.0017},
	keywords = {ccfInfo: Not Found, citationNumber: 0},
	pages = {433--464},
}

@article{__2024-1,
	title = {è‡ªä¸»æ— äººç³»ç»Ÿçš„å…·èº«è®¤çŸ¥æ™ºèƒ½æ¡†æ¶},
	volume = {42},
	issn = {1000-7857},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=VcTOyLYtvEyx5I76TaG4s_e0yh0cWR4m00t_X1bHVtOh9vh_fcS9L81i0sG8HRykshvtHF4sLnVt1_Vu1G5Frq5ulrwKKP_kxZXlafRgKNkAM3B0N0XSe2SFikZSPMzX67PF7wGfXM1fPxm34vzm8BZ3YDNKY8lf9oCYl9tZfntEbXf8cw9cAxD81qEreSAx&uniplatform=NZKPT&language=CHS},
	abstract = {è‡ªä¸»æ— äººç³»ç»Ÿæ˜¯ä¸€ç±»å…·æœ‰è‡ªä¸»è®¤çŸ¥ã€è¿åŠ¨è§„åˆ’ã€è‡ªä¸»å†³ç­–å’Œæ¨ç†èƒ½åŠ›çš„æ™ºèƒ½ç³»ç»Ÿï¼Œå…¶ç›®æ ‡æ˜¯åœ¨æœ‰é™ç”šè‡³æ²¡æœ‰äººå·¥å‚ä¸çš„æƒ…å†µä¸‹å®Œæˆå¤æ‚å¼€æ”¾åŠ¨æ€åœºæ™¯ä¸­çš„é€šç”¨ä»»åŠ¡ã€‚é’ˆå¯¹è‡ªä¸»æ— äººç³»ç»Ÿåœ¨è·¨åŸŸååŒä»»åŠ¡ä¸Šå¾€å¾€é¢ä¸´ååŒæ„ŸçŸ¥æ•ˆç‡ä½ã€è‡ªç»„ç½‘é€šä¿¡å¯é æ€§å·®ã€èµ„æºè°ƒåº¦æµç¨‹æ…¢ã€ä»»åŠ¡åˆ†é…æ˜“å†²çªç­‰ä¸€ç³»åˆ—é—®é¢˜ï¼Œæ¢è®¨äº†èåˆå¤§æ¨¡å‹å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ„å»ºäº†â€œå¤§æ¨¡å‹+è‡ªä¸»æ— äººç³»ç»Ÿ+äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹â€ä¸ºä¸€ä½“çš„è‡ªä¸»æ— äººç³»ç»Ÿâ€œç®—-æ§-æµ‹â€å…·èº«è®¤çŸ¥æ™ºèƒ½æ¶æ„ï¼Œä»¥æ¨åŠ¨è‡ªä¸»æ— äººç³»ç»Ÿå…·èº«è®¤çŸ¥æ™ºèƒ½åº”ç”¨è½åœ°ã€‚},
	language = {zh},
	number = {12},
	journal = {ç§‘æŠ€å¯¼æŠ¥},
	author = {å­™, é•¿é“¶ and ç©†, æœçµ® and æŸ³, æ–‡ç«  and ç‹, æ™“},
	year = {2024},
	note = {CNKICite: 2},
	keywords = {No DOI found, ccfInfo: Not Found, citationNumber: Not Found, â›” No DOI found, å…·èº«æ™ºèƒ½, å¤§æ¨¡å‹, ç”Ÿæˆå¼äººå·¥æ™ºèƒ½, è‡ªä¸»æ— äººç³»ç»Ÿ, é€šç”¨æ™ºèƒ½æœºå™¨},
	pages = {157--166},
}

@article{__nodate,
	title = {å…·èº«æ™ºèƒ½è‡ªä¸»æ— äººç³»ç»ŸæŠ€æœ¯},
	issn = {0254-4156},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=jEKy9Hq18MKSEMjVHV6twhTj-x-x9rho2eMdrVsbv__DTI9adcsMEMn98FFgh0creWJswBzE1foMmzl-3VmsoEBQwgIZevJKwvtsqVijl5UlIlueO3jMFrZJ0GP1Dfp6bAdlYsEw6u7va1LmTUNKx2Y50gI4Q-kDAy48DiKikICskY6UTyfnQMWhJWeSRGDx&uniplatform=NZKPT&language=CHS},
	doi = {10.16383/j.aas.c240456},
	abstract = {è‡ªä¸»æ— äººç³»ç»Ÿæ˜¯ä¸€ç±»å…·æœ‰è‡ªä¸»æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›çš„æ™ºèƒ½ç³»ç»Ÿ,åœ¨å›½é˜²å®‰å…¨ã€èˆªç©ºèˆªå¤©ã€é«˜æ€§èƒ½æœºå™¨äººç­‰æ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨.è¿‘å¹´æ¥,åŸºäºTransformeræ¶æ„çš„å„ç±»å¤§æ¨¡å‹å¿«é€Ÿé©æ–°,æå¤§åœ°æ¨åŠ¨äº†è‡ªä¸»æ— äººç³»ç»Ÿçš„å‘å±•.ç›®å‰,è‡ªä¸»æ— äººç³»ç»Ÿæ­£è¿æ¥ä¸€åœºä»¥â€œå…·èº«æ™ºèƒ½â€ä¸ºæ ¸å¿ƒçš„æ–°ä¸€ä»£æŠ€æœ¯é©å‘½.å¤§æ¨¡å‹éœ€è¦å€ŸåŠ©æ— äººç³»ç»Ÿçš„ç‰©ç†å®ä½“æ¥å®ç°â€œå…·èº«åŒ–â€,æ— äººç³»ç»Ÿå¯ä»¥åˆ©ç”¨å¤§æ¨¡å‹æŠ€æœ¯æ¥å®ç°â€œæ™ºèƒ½åŒ–â€.æœ¬æ–‡é˜è¿°äº†å…·èº«æ™ºèƒ½è‡ªä¸»æ— äººç³»ç»Ÿçš„å‘å±•ç°çŠ¶,è¯¦ç»†æ¢è®¨äº†åŒ…å«å¤§æ¨¡å‹é©±åŠ¨çš„å¤šæ¨¡æ€æ„ŸçŸ¥ã€é¢å‘å…·èº«ä»»åŠ¡çš„æ¨ç†ä¸å†³ç­–ã€åŸºäºåŠ¨æ€äº¤äº’çš„æœºå™¨äººå­¦ä¹ ä¸æ§åˆ¶ã€ä¸‰ç»´åœºæ™¯å…·èº«æ¨¡æ‹Ÿå™¨ç­‰å…·èº«æ™ºèƒ½é¢†åŸŸçš„å…³é”®æŠ€æœ¯.æœ€å,æŒ‡å‡ºäº†ç›®å‰å…·èº«æ™ºèƒ½æ— äººç³»ç»Ÿæ‰€é¢ä¸´çš„æŒ‘æˆ˜,å¹¶å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘.},
	language = {zh-CN},
	journal = {è‡ªåŠ¨åŒ–å­¦æŠ¥},
	author = {å­™, é•¿é“¶ and è¢, å¿ƒ and ç‹, è¿œå¤§ and æŸ³, æ–‡ç« },
	keywords = {ccfInfo: Net Error: 0, ccfInfo: Not Found, citationNumber: Not Found, notion, äººå·¥æ™ºèƒ½, å…·èº«æ™ºèƒ½, å¤§è¯­è¨€æ¨¡å‹, è‡ªä¸»æ— äººç³»ç»Ÿ},
	pages = {1--15},
}

@article{zhao_evaluating_nodate,
	title = {On {Evaluating} {Adversarial} {Robustness} of {Large} {Vision}-{Language} {Models}},
	abstract = {Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Our project page: yunqing-me.github.io/AttackVLM/.},
	language = {en},
	author = {Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man and Lin, Min},
	keywords = {ccfInfo: CCF-A NeurIPS, citationNumber: 35, notion},
}

@article{guo_solving_2020,
	title = {Solving {Partial} {Differential} {Equations} {Using} {Deep} {Learning} and {Physical} {Constraints}},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/17/5917},
	doi = {10.3390/app10175917},
	abstract = {The various studies of partial differential equations (PDEs) are hot topics of mathematical research. Among them, solving PDEs is a very important and difï¬cult task. Since many partial differential equations do not have analytical solutions, numerical methods are widely used to solve PDEs. Although numerical methods have been widely used with good performance, researchers are still searching for new methods for solving partial differential equations. In recent years, deep learning has achieved great success in many ï¬elds, such as image classiï¬cation and natural language processing. Studies have shown that deep neural networks have powerful function-ï¬tting capabilities and have great potential in the study of partial differential equations. In this paper, we introduce an improved Physics Informed Neural Network (PINN) for solving partial differential equations. PINN takes the physical information that is contained in partial differential equations as a regularization term, which improves the performance of neural networks. In this study, we use the method to study the wave equation, the KdVâ€“Burgers equation, and the KdV equation. The experimental results show that PINN is effective in solving partial differential equations and deserves further research.},
	language = {en},
	number = {17},
	urldate = {2024-12-29},
	journal = {Applied Sciences},
	author = {Guo, Yanan and Cao, Xiaoqun and Liu, Bainian and Gao, Mei},
	month = aug,
	year = {2020},
	note = {TLDR: An improved Physics Informed Neural Network (PINN) is introduced, which takes the physical information that is contained in partial differential equations as a regularization term, which improves the performance of neural networks and is used to study the wave equation, theKdVâ€“Burgers equation, and the KdV equation.},
	keywords = {ccfInfo: Not Found, citationNumber: 51},
	pages = {5917},
}

@article{cai_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Networks} for {Heat} {Transfer} {Problems}},
	volume = {143},
	issn = {0022-1481, 1528-8943},
	url = {https://asmedigitalcollection.asme.org/heattransfer/article/143/6/060801/1104439/Physics-Informed-Neural-Networks-for-Heat-Transfer},
	doi = {10.1115/1.4050542},
	abstract = {Abstract
            Physics-informed neural networks (PINNs) have gained popularity across different engineering fields due to their effectiveness in solving realistic problems with noisy data and often partially missing physics. In PINNs, automatic differentiation is leveraged to evaluate differential operators without discretization errors, and a multitask learning problem is defined in order to simultaneously fit observed data while respecting the underlying governing laws of physics. Here, we present applications of PINNs to various prototype heat transfer problems, targeting in particular realistic conditions not readily tackled with traditional computational methods. To this end, we first consider forced and mixed convection with unknown thermal boundary conditions on the heated surfaces and aim to obtain the temperature and velocity fields everywhere in the domain, including the boundaries, given some sparse temperature measurements. We also consider the prototype Stefan problem for two-phase flow, aiming to infer the moving interface, the velocity and temperature fields everywhere as well as the different conductivities of a solid and a liquid phase, given a few temperature measurements inside the domain. Finally, we present some realistic industrial applications related to power electronics to highlight the practicality of PINNs as well as the effective use of neural networks in solving general heat transfer problems of industrial complexity. Taken together, the results presented herein demonstrate that PINNs not only can solve ill-posed problems, which are beyond the reach of traditional computational methods, but they can also bridge the gap between computational and experimental heat transfer.},
	language = {en},
	number = {6},
	urldate = {2024-12-29},
	journal = {Journal of Heat Transfer},
	author = {Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George Em},
	month = jun,
	year = {2021},
	note = {TLDR: Applications of PINNs to various prototype heat transfer problems, targeting in particular realistic conditions not readily tackled with traditional computational methods, demonstrate that PINNs not only can solve ill-posed problems, which are beyond the reach of traditional computational Methods, but they can also bridge the gap between computational and experimental heat transfer.},
	keywords = {ccfInfo: Not Found, citationNumber: 248},
	pages = {060801},
}

@article{zhang_gw-pinn_2022,
	title = {{GW}-{PINN}: {A} deep learning algorithm for solving groundwater flow equations},
	volume = {165},
	issn = {03091708},
	shorttitle = {{GW}-{PINN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0309170822001142},
	doi = {10.1016/j.advwatres.2022.104243},
	abstract = {Machine learning methods provide new perspective for more convenient and efficient prediction of groundwater flow. In this study, a deep learning method â€œGW-PINNâ€ without labeled data for solving groundwater flow equations with wells was proposed. GW-PINN takes the physics inform neural network (PINN) as the backbone and uses either the hard or soft constraint in the loss function for training. A locally refined sampling strategy (LRS) is adopted to generate the consistent spatial sampling points for problems with strong hydraulic head change, and then combined with an appropriate temporal sampling scheme to obtain the final spatial-temporal sampling points. A snowball-style two-stage training strategy by dividing the temporal domain into two subdomains is designed to decrease the sampling points. Five cases were designed to test the training performance of GW-PINN under different sampling strategies and two constraints. The predicted results of GW-PINN were compared with MODFLOW and the analytical solution. The results demonstrate that GW-PINN possesses strong ability in capturing the hydraulic head change for both confined and un-confined aquifers. The hard constraint owns more robust learning ability than the soft constraint. The LRS strategy can generate more accurate results with much fewer sampling points than traditional sampling strategies, and the snowball-style two-stage training strategy is significantly efficient for problems with the drastic change of hydraulic head. Furthermore, the application of GW-PINN as a surrogate model for parameterized groundwater flow equations is illustrated. This study provides an option tool for efficient groundwater flow simulation, especially for those with local refinements are needed.},
	language = {en},
	urldate = {2024-12-29},
	journal = {Advances in Water Resources},
	author = {Zhang, Xiaoping and Zhu, Yan and Wang, Jing and Ju, Lili and Qian, Yingzhi and Ye, Ming and Yang, Jinzhong},
	month = jul,
	year = {2022},
	keywords = {ccfInfo: Not Found, citationNumber: 9},
	pages = {104243},
}

@article{chen_benchmarking_nodate,
	title = {Benchmarking {Robustness} of {Adaptation} {Methods} on {Pre}-trained {Vision}-{Language} {Models}},
	abstract = {Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. As test samples in real-world applications usually differ from adaptation data, studying the robustness of these adaptation methods against distribution shifts is essential. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead, it results in even lower robustness. We hope this study could benefit future research in developing robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at https://adarobustness.github.io.},
	language = {en},
	author = {Chen, Shuo and Gu, Jindong and Han, Zhen and Ma, Yunpu and Torr, Philip and Tresp, Volker},
	keywords = {ccfInfo: CCF-A NeurIPS, citationNumber: 0, notion},
}

@misc{huang_partial_2022,
	title = {Partial {Differential} {Equations} {Meet} {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Partial {Differential} {Equations} {Meet} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2211.05567},
	doi = {10.48550/arXiv.2211.05567},
	abstract = {Many problems in science and engineering can be modeled mathematically by a group of Partial Differential Equations (PDEs). Mechanism-based Scientiï¬c Computing following PDEs has long been an essential paradigm for studying topics such as Computational Fluid Dynamics, multiphysics simulation, molecular dynamics, or even dynamical systems. It is a vibrant multi-disciplinary ï¬eld of increasing importance and with extraordinary potential. At the same time, solving PDEs efï¬ciently has been a long-standing challenge. Generally, apart from a few Differential Equations where analytical solutions are directly available, more equations must rely on numerical approaches to be solved approximately, e.g., the Finite Difference Method and Finite Element Method. These numerical methods usually divide a continuous problem domain into discrete grids and then concentrate on solving the system at each of those points or elements. Although these traditional numerical methods show effectiveness in solving PDEs, the vast number of iterative operations accompanying each step forward greatly reduces the efï¬ciency. Recently, another equally important paradigm, data-based computation represented by Deep Learning, has emerged as an effective means of solving PDEs. Surprisingly, a comprehensive review of this interesting subdivision of AI for Science is still lacking. This survey aims to categorize and review the current progress on Deep Neural Networks for PDEs. We discuss the literature published in this subï¬eld over the past decades and present them in a common taxonomy, followed by an overview and classiï¬cation of applications of these related methods in scientiï¬c research and engineering/medical scenarios. The origin, developing history, character, and sort, as well as the future trends in each potential direction of this area are also introduced.},
	language = {en},
	urldate = {2024-12-29},
	publisher = {arXiv},
	author = {Huang, Shudong and Feng, Wentao and Tang, Chenwei and Lv, Jiancheng},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05567 [cs]
TLDR: This survey aims to categorize and review the current progress on Deep Neural Networks for PDEs, and discusses the literature published in this subfield over the past decades and presents them in a common taxonomy.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{jiang_phygnnet_2023,
	address = {Shanghai China},
	title = {{PhyGNNet}: {Solving} spatiotemporal {PDEs} with {Physics}-informed {Graph} {Neural} {Network}},
	isbn = {978-1-4503-9944-9},
	shorttitle = {{PhyGNNet}},
	url = {https://dl.acm.org/doi/10.1145/3590003.3590029},
	doi = {10.1145/3590003.3590029},
	abstract = {Partial differential equations (PDEs) are a common means of describing physical processes. Solving PDEs can obtain simulated results of physical evolution. Currently, the mainstream neural network method is to minimize the loss of PDEs thus constraining neural networks to fit the solution mappings. By the implementation of differentiation, the methods can be divided into PINN methods based on automatic differentiation and other methods based on discrete differentiation. PINN methods rely on automatic backpropagation, and the computation step is time-consuming, for iterative training, the complexity of the neural network and the number of collocation points are limited to a small condition, thus abating accuracy. The discrete differentiation is more efficient in computation, following the regular computational domain assumption. However, in practice, the assumption does not necessarily hold. In this paper, we propose a PhyGNNet method to solve PDEs based on graph neural network and discrete differentiation on irregular domain. Meanwhile, to verify the validity of the method, we solve Burgers equation and conduct a numerical comparison with PINN. The results show that the proposed method performs better both in fit ability and time extrapolation than PINN. Code is available at https://github.com/echowve/phygnnet.},
	language = {en},
	urldate = {2024-12-29},
	booktitle = {Proceedings of the 2023 2nd {Asia} {Conference} on {Algorithms}, {Computing} and {Machine} {Learning}},
	publisher = {ACM},
	author = {Jiang, Longxiang and Wang, Liyuan and Chu, Xinkun and Xiao, Yonghao and Zhang, Hao},
	month = mar,
	year = {2023},
	note = {TLDR: A PhyGNNet method to solve PDEs based on graph neural network and discrete differentiation on irregular domain is proposed and shows that the proposed method performs better both in fit ability and time extrapolation than PINN.},
	keywords = {ccfInfo: CCF-None CACML, citationNumber: 3},
	pages = {143--147},
}

@inproceedings{richter_robust_2022,
	title = {Robust {SDE}-{Based} {Variational} {Formulations} for {Solving} {Linear} {PDEs} via {Deep} {Learning}},
	url = {https://proceedings.mlr.press/v162/richter22a.html},
	abstract = {The combination of Monte Carlo methods and deep learning has recently led to efficient algorithms for solving partial differential equations (PDEs) in high dimensions. Related learning problems are often stated as variational formulations based on associated stochastic differential equations (SDEs), which allow the minimization of corresponding losses using gradient-based optimization methods. In respective numerical implementations it is therefore crucial to rely on adequate gradient estimators that exhibit low variance in order to reach convergence accurately and swiftly. In this article, we rigorously investigate corresponding numerical aspects that appear in the context of linear Kolmogorov PDEs. In particular, we systematically compare existing deep learning approaches and provide theoretical explanations for their performances. Subsequently, we suggest novel methods that can be shown to be more robust both theoretically and numerically, leading to substantial performance improvements.},
	language = {en},
	urldate = {2024-12-27},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Richter, Lorenz and Berner, Julius},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {ccfInfo: CCF-A ICML, citationNumber: 11},
	pages = {18649--18666},
}

@inproceedings{liang_code_2023,
	title = {Code as {Policies}: {Language} {Model} {Programs} for {Embodied} {Control}},
	shorttitle = {Code as {Policies}},
	url = {https://ieeexplore.ieee.org/document/10160591/},
	doi = {10.1109/ICRA48891.2023.10160591},
	abstract = {Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (â€˜fasterâ€™) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
	language = {en-US},
	urldate = {2024-12-13},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
	month = may,
	year = {2023},
	note = {95 citations (Crossref/DOI) [2024-12-13]
TLDR: Code as Policies is presented, a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms.},
	keywords = {Codes, Detectors, Feedback loop, Impedance, Libraries, Natural languages, Process control, ccfInfo: CCF-B ICRA, citationNumber: 721, notion},
	pages = {9493--9500},
}

@misc{hao_physics-informed_2023,
	title = {Physics-{Informed} {Machine} {Learning}: {A} {Survey} on {Problems}, {Methods} and {Applications}},
	shorttitle = {Physics-{Informed} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2211.08064},
	doi = {10.48550/arXiv.2211.08064},
	abstract = {Recent advances of data-driven machine learning have revolutionized ï¬elds like computer vision, reinforcement learning, and many scientiï¬c and engineering domains. In many real-world and scientiï¬c problems, systems that generate data are governed by physical laws. Recent work shows that it provides potential beneï¬ts for machine learning models by incorporating the physical prior and collected data, which makes the intersection of machine learning and physics become a prevailing paradigm. By integrating the data and mathematical physics models seamlessly, it can guide the machine learning model towards solutions that are physically plausible, improving accuracy and efï¬ciency even in uncertain and high-dimensional contexts. In this survey, we present this learning paradigm called Physics-Informed Machine Learning (PIML) which is to build a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism. We systematically review the recent development of physics-informed machine learning from three perspectives of machine learning tasks, representation of physical prior, and methods for incorporating physical prior. We also propose several important open research problems based on the current trends in the ï¬eld. We argue that encoding different forms of physical prior into model architectures, optimizers, inference algorithms, and signiï¬cant domain-speciï¬c applications like inverse engineering design and robotic control is far from being fully explored in the ï¬eld of physics-informed machine learning. We believe that the interdisciplinary research of physics-informed machine learning will signiï¬cantly propel research progress, foster the creation of more effective machine learning models, and also offer invaluable assistance in addressing long-standing problems in related disciplines.},
	language = {en},
	urldate = {2024-12-29},
	publisher = {arXiv},
	author = {Hao, Zhongkai and Liu, Songming and Zhang, Yichi and Ying, Chengyang and Feng, Yao and Su, Hang and Zhu, Jun},
	month = mar,
	year = {2023},
	note = {arXiv:2211.08064 [cs]
TLDR: This survey presents a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism and argues that encoding different forms of physical prior into model architectures, optimizers, inference algorithms, and significant domain-specific applications is far from being fully explored.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, ccfInfo: CCF-None CORR, citationNumber: 133},
}

@misc{sarch_vlm_2024,
	title = {{VLM} {Agents} {Generate} {Their} {Own} {Memories}: {Distilling} {Experience} into {Embodied} {Programs} of {Thought}},
	shorttitle = {{VLM} {Agents} {Generate} {Their} {Own} {Memories}},
	url = {http://arxiv.org/abs/2406.14596},
	doi = {10.48550/arXiv.2406.14596},
	abstract = {Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. In this work, we ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal demonstrations? We propose In-Context Abstraction Learning (ICAL), a method that builds a memory of multimodal experience from sub-optimal demonstrations and human feedback. Given a task demonstration that may contain inefficiencies or mistakes, a VLM abstracts the trajectory into a generalized program of thoughts by correcting inefficient actions and annotating cognitive abstractions: causal relationships, object state changes, temporal subgoals, and task-relevant visual elements. These programs of thought are iteratively improved and adapted through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting examples, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. Moreover, as the agentâ€™s library of examples grows, it becomes more efficient, relying less on human feedback and requiring fewer environment interactions per demonstration. Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6\% improvement in goal-condition success. In VisualWebArena, our task success rate improves over the SOTA from 14.3\% to 22.7\% using GPT4V. In Ego4D action forecasting, we improve over few-shot GPT-4V and remain competitive with supervised models. We show finetuning our retrieval-augmented in-context agent yields additional improvements. Our approach significantly reduces reliance on manual prompt engineering and consistently outperforms in-context learning from action plans that lack such programs of thought.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Sarch, Gabriel and Jang, Lawrence and Tarr, Michael J. and Cohen, William W. and Marino, Kenneth and Fragkiadaki, Katerina},
	month = nov,
	year = {2024},
	note = {TLDR: This work proposes In-Context Abstraction Learning (ICAL), a method that builds a memory of multi-modal experience insights from sub-optimal demonstrations and human feedback and consistently outperforms in-context learning from action plans that lack such insights.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ccfInfo: Not Found, citationNumber: 1, notion},
}

@misc{wang_physics-guided_2023,
	title = {Physics-{Guided} {Deep} {Learning} for {Dynamical} {Systems}: {A} {Survey}},
	shorttitle = {Physics-{Guided} {Deep} {Learning} for {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/2107.01272},
	doi = {10.48550/arXiv.2107.01272},
	abstract = {Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are sample efï¬cient, and interpretable but often rely on rigid assumptions. Furthermore, direct numerical approximation is usually computationally intensive, requiring signiï¬cant computational resources and expertise, and many real-world systems do not have fully-known governing laws. While deep learning (DL) provides novel alternatives for efï¬ciently recognizing complex patterns and emulating nonlinear dynamics, its predictions do not necessarily obey the governing laws of physical systems, nor do they generalize well across different systems. Thus, the study of physics-guided DL emerged and has gained great progress. Physics-guided DL aims to take the best from both physics-based modeling and state-of-the-art DL models to better solve scientiï¬c problems. In this paper, we provide a structured overview of existing methodologies of integrating prior physical knowledge or physics-based modeling into DL, with a special emphasis on learning dynamical systems. We also discuss the fundamental challenges and emerging opportunities in the area.},
	language = {en},
	urldate = {2024-12-29},
	publisher = {arXiv},
	author = {Wang, Rui and Yu, Rose},
	month = feb,
	year = {2023},
	note = {arXiv:2107.01272 [cs]},
	keywords = {Computer Science - Machine Learning, ccfInfo: CCF-None CORR, citationNumber: 2},
}

@article{__nodate,
	title = {é¢å‘å…·èº«äººå·¥æ™ºèƒ½çš„ç‰©ä½“ç›®æ ‡å¯¼èˆªç»¼è¿°},
	issn = {1000-9825},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=jEKy9Hq18MIc2K9TO2803ttUcVqvF7L-N4QB3Qt_zthVsu3AUkyPSfNzd1qv9gk2JgTkWn-3LpEgOROMb9JTZiKsP67F8quUoOx_tMbZtqB-5XmpUxuTp4y7oJPpRX431STedSSTY8BRWw1lTxkz3xx5kzJqcRHiOO1WAeRb-gRxoAPDS25s8Oia2yG_zcLO&uniplatform=NZKPT&language=CHS},
	doi = {10.13328/j.cnki.jos.007250},
	abstract = {è¿‘å¹´æ¥éšç€è®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸æ–­å‘å±•,å…·èº«äººå·¥æ™ºèƒ½(embodied AI)å—åˆ°å›½å†…å¤–å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„å¹¿æ³›å…³æ³¨.å…·èº«äººå·¥æ™ºèƒ½å¼ºè°ƒå…·èº«æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒè¿›è¡Œæƒ…æ™¯åŒ–çš„äº¤äº’æ¥ä¸»åŠ¨è·å–ç‰©ç†ä¸–ç•Œçš„çœŸå®åé¦ˆ,å¹¶é€šè¿‡å¯¹åé¦ˆè¿›è¡Œå­¦ä¹ ä½¿å…·èº«æ™ºèƒ½ä½“æ›´åŠ æ™ºèƒ½.ä½œä¸ºå…·èº«äººå·¥æ™ºèƒ½å…·ä½“åŒ–çš„ä»»åŠ¡ä¹‹ä¸€,ç‰©ä½“ç›®æ ‡å¯¼èˆªè¦æ±‚å…·èº«æ™ºèƒ½ä½“åœ¨äº‹å…ˆæœªçŸ¥çš„ã€å¤æ‚ä¸”è¯­ä¹‰ä¸°å¯Œçš„åœºæ™¯ä¸­æœå¯»å¹¶å¯¼èˆªè‡³æŒ‡å®šçš„ç‰©ä½“ç›®æ ‡(ä¾‹å¦‚:æ‰¾åˆ°æ°´æ§½).ç‰©ä½“ç›®æ ‡å¯¼èˆªåœ¨è¾…åŠ©äººç±»æ—¥å¸¸æ´»åŠ¨çš„æ™ºèƒ½åŠ©æ‰‹æ–¹é¢æœ‰ç€å·¨å¤§çš„åº”ç”¨æ½œåŠ›,æ˜¯å…¶ä»–åŸºäºäº¤äº’çš„å…·èº«æ™ºèƒ½ç ”ç©¶çš„åŸºç¡€å’Œå‰ç½®ä»»åŠ¡.ç³»ç»Ÿåœ°åˆ†ç±»å’Œæ¢³ç†å½“å‰ç‰©ä½“ç›®æ ‡å¯¼èˆªç›¸å…³å·¥ä½œ,é¦–å…ˆä»‹ç»ç¯å¢ƒè¡¨ç¤ºå’Œè§†è§‰è‡ªä¸»æ¢ç´¢ç›¸å…³çŸ¥è¯†,ä»3ç§ä¸åŒçš„è§’åº¦å¯¹ç°æœ‰çš„ç‰©ä½“ç›®æ ‡å¯¼èˆªæ–¹æ³•è¿›è¡Œåˆ†ç±»å’Œåˆ†æ,å…¶æ¬¡ä»‹ç»ä¸¤ç±»æ›´é«˜å±‚æ¬¡çš„ç‰©ä½“é‡æ’å¸ƒä»»åŠ¡,æè¿°é€¼çœŸçš„å®¤å†…ä»¿çœŸç¯å¢ƒæ•°æ®é›†ã€è¯„ä»·æŒ‡æ ‡å’Œé€šç”¨çš„å¯¼èˆªç­–ç•¥è®­ç»ƒèŒƒå¼,æœ€åæ¯”è¾ƒå’Œåˆ†æç°æœ‰çš„ç‰©ä½“ç›®æ ‡å¯¼èˆªç­–ç•¥åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½,æ€»ç»“è¯¥é¢†åŸŸæ‰€é¢ä¸´çš„æŒ‘æˆ˜,å¹¶å¯¹å‘å±•å‰æ™¯ä½œå‡ºå±•æœ›.},
	language = {en-US},
	journal = {è½¯ä»¶å­¦æŠ¥},
	author = {é™ˆ, é“‚å’ and åº·, å˜‰ç»ª and é’Ÿ, è and å´”, æ°¸æ­£ and å¢, æ€æ€¡ and æ¨, æ˜Šæ¥  and ç‹, å»ºæ–°},
	keywords = {/done, ccfInfo: Not Found, citationNumber: Not Found, notion, å…·èº«äººå·¥æ™ºèƒ½, ç‰©ä½“ç›®æ ‡å¯¼èˆª, è§†è§‰ç‰©ä½“é‡æ’å¸ƒ, è§†è§‰è‡ªä¸»æ¢ç´¢},
	pages = {1--43},
}

@article{__2023,
	title = {ç‰©ç†é©±åŠ¨æ·±åº¦å­¦ä¹ æ³¢åŠ¨æ•°å€¼æ¨¡æ‹Ÿæ–¹æ³•åŠåº”ç”¨},
	volume = {55},
	issn = {0459-1879},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=upp0mjVwiKW48_AFoLXx141WxgLhzfZPfqLOsOBEuaDUlGz3yC40XL2Xxv8bDZn4v192Fmo_Qe69dc_CDAxZqR791UMTCJxkjw5eYfZrK4S4Vu7Q9Jwbo3q8zyfotWxHX7dEKrbdIf0Hlw22-EnBVSQtQzJfGL8KnkaNqvzevBw-tt77vak2gcZPA-oWalBb&uniplatform=NZKPT&language=CHS},
	abstract = {è¿‘å¹´æ¥,ç‰©ç†å…ˆéªŒèåˆæ•°æ®çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ±‚è§£ä»¥åå¾®åˆ†æ–¹ç¨‹ä¸ºç†è®ºåŸºç¡€çš„æ­£åæ¼”é—®é¢˜å·²æˆä¸ºäº¤å‰å­¦ç§‘çƒ­ç‚¹.é’ˆå¯¹åœ°éœ‡å·¥ç¨‹æ³¢åŠ¨æ•°å€¼æ¨¡æ‹Ÿ,æœ¬æ–‡é˜æ˜äº†ç‰©ç†é©±åŠ¨æ·±åº¦å­¦ä¹ æ–¹æ³• PINNçš„æ•°å­¦æ¦‚å¿µåŠå®ç°æ–¹å¼,ä»¥æ— æºé¡¹ä¸€ç»´æ³¢åŠ¨ä¸ºä¾‹,å¼€å±•äº†ç›¸å…³ç†è®ºæ¨¡å‹æ„å»º,å¹¶ä¸è§£æè§£åŠæœ‰é™å·®åˆ†æ–¹æ³•è¿›è¡Œå¯¹æ¯”,åˆ†æäº†PINNæ–¹æ³•ä¸å…¶ä»–æ•°å€¼ç®—æ³•æ¨¡æ‹Ÿæ³¢åœºçš„ç›¸å¯¹èŒƒæ•°è¯¯å·®,éªŒè¯äº†ç‰©ç†é©±åŠ¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ±‚è§£æ³¢åŠ¨é—®é¢˜çš„å¯è¡Œæ€§.é‡‡ç”¨ç‰©ç†é©±åŠ¨æ·±åº¦å­¦ä¹ æ–¹æ³•å¹¶ç»“åˆè°±å…ƒæ³•å½¢æˆçš„ç¨€ç–åˆå§‹æ³¢åœºæ•°æ®,å¼€å±•äº†äºŒç»´æ³¢åŠ¨æ•°å€¼æ¨¡æ‹Ÿ,å®ç°äº†è‡ªç”±è¾¹ç•Œæ¡ä»¶åŠèµ·ä¼åœ°è¡¨ç­‰å…¸å‹å·¥å†µçš„æ¨¡æ‹Ÿ,å¹¶ç»™å‡ºäº†æ—¶åºæ³¢åœºåˆ†å¸ƒç‰¹æ€§.æ›´æ¢ä¸åŒçš„åˆå§‹æ¡ä»¶,æµ‹è¯•äº†ç¥ç»ç½‘ç»œçš„æ³›åŒ–ç²¾åº¦,æå‡ºå¯æ˜¾è‘—æé«˜ç½‘ç»œè®­ç»ƒæ•ˆç‡çš„è¿ç§»å­¦ä¹ æ–¹æ³•.é€šè¿‡ä¸è°±å…ƒæ³•çš„ç»“æœå¯¹æ¯”,éªŒè¯äº†æœ¬æ–‡æ–¹æ³•æ¨¡æ‹Ÿå‡è´¨åœºåœ°ã€ç©ºé—´ä¸å‡åŒ€åŠå¤æ‚åœ°å½¢åœºåœ°æ³¢åŠ¨é—®é¢˜çš„å¯é æ€§.ç»“æœè¡¨æ˜,ç‰©ç†é©±åŠ¨æ·±åº¦å­¦ä¹ æ–¹æ³•å…·å¤‡æ— ç½‘æ ¼ã€ç²¾ç»†åŒ–æ¨¡æ‹Ÿç­‰ä¼˜åŠ¿,å¹¶å¯å®ç°è‡ªç”±åœ°è¡¨åŠä¾§è¾¹ç•Œæ³¢åœºé€å°„ç­‰æ•°å€¼æ¨¡æ‹Ÿæ¡ä»¶.},
	language = {zh-CN},
	number = {1},
	journal = {åŠ›å­¦å­¦æŠ¥},
	author = {é™ˆ, è‹ and ä¸, æ¯… and å­™, æµ© and èµµ, å¯† and ç‹, è¿›å»· and æ, å°å†›},
	year = {2023},
	note = {CNKICite: 7},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found, â›” No DOI found, å†…æºæ³¢åŠ¨, æ³¢åŠ¨æ•°å€¼æ¨¡æ‹Ÿ, ç‰©ç†ä¿¡æ¯æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, è¾¹ç•Œæ¡ä»¶},
	pages = {272--282},
}

@misc{zhang_motiondiffuse_2022,
	title = {{MotionDiffuse}: {Text}-{Driven} {Human} {Motion} {Generation} with {Diffusion} {Model}},
	shorttitle = {{MotionDiffuse}},
	url = {http://arxiv.org/abs/2208.15001},
	doi = {10.48550/arXiv.2208.15001},
	abstract = {Human motion modeling is important for many modern graphics applications, which typically require professional skills. In order to remove the skill barriers for laymen, recent motion generation methods can directly generate human motions conditioned on natural languages. However, it remains challenging to achieve diverse and fine-grained motion generation with various text inputs. To address this problem, we propose MotionDiffuse, the first diffusion model-based text-driven motion generation framework, which demonstrates several desired properties over existing methods. 1) Probabilistic Mapping. Instead of a deterministic language-motion mapping, MotionDiffuse generates motions through a series of denoising steps in which variations are injected. 2) Realistic Synthesis. MotionDiffuse excels at modeling complicated data distribution and generating vivid motion sequences. 3) Multi-Level Manipulation. MotionDiffuse responds to fine-grained instructions on body parts, and arbitrary-length motion synthesis with time-varied text prompts. Our experiments show MotionDiffuse outperforms existing SoTA methods by convincing margins on text-driven motion generation and action-conditioned motion generation. A qualitative analysis further demonstrates MotionDiffuse's controllability for comprehensive motion generation. Homepage: https://mingyuan-zhang.github.io/projects/MotionDiffuse.html},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
	month = aug,
	year = {2022},
	note = {titleTranslation: MotionSdiffuseï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬é©±åŠ¨äººä½“è¿åŠ¨ç”Ÿæˆ},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ccfInfo: CCF-A TPAMI, citationNumber: 118, notion},
}

@article{wang_nas-pinn_2024,
	title = {{NAS}-{PINN}: {Neural} architecture search-guided physics-informed neural network for solving {PDEs}},
	volume = {496},
	issn = {00219991},
	shorttitle = {{NAS}-{PINN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999123006988},
	doi = {10.1016/j.jcp.2023.112603},
	abstract = {Physics-informed neural network (PINN) has been a prevalent framework for solving PDEs since proposed. By incorporating the physical information into the neural network through loss functions, it can predict solutions to PDEs in an unsupervised manner. However, the design of the neural network structure basically relies on prior knowledge and experience, which has caused great trouble and high computational overhead. Therefore, we propose a neural architecture search-guided method, namely NAS-PINN, to automatically search the optimum neural architecture for solving certain PDEs. By relaxing the search space into a continuous one and utilizing masks to realize the addition of tensors in different shapes, NAS-PINN can be trained through a bilevel optimization, where the inner loop optimizes the weights and bias of neural networks and the outer loop the architecture parameters. We verify the ability of NAS-PINN by several numerical experiments including Poisson, Burgers, and Advection equations. The characteristics of effective neural architectures for solving different PDEs are summarized, which can be used to guide the design of neural networks in PINN. It is found that more hidden layers do not necessarily mean better performance and sometimes can be harmful. Especially for Poisson and Advection, a shallow neural network with more neurons is more appropriate in PINNs. It is also indicated that for complex problems, neural networks with residual connection can improve the performance of PINNs.},
	language = {en},
	urldate = {2024-12-29},
	journal = {Journal of Computational Physics},
	author = {Wang, Yifan and Zhong, Linlin},
	month = jan,
	year = {2024},
	note = {TLDR: A neural architecture search-guided method, namely NAS-PINN, to automatically search the optimum neural architecture for solving certain PDEs and it is indicated that for complex problems, neural networks with residual connection can improve the performance of PINNs.},
	keywords = {ccfInfo: CCF-None JCPHY, citationNumber: 0},
	pages = {112603},
}

@phdthesis{__2024,
	type = {ç¡•å£«},
	title = {åŸºäºæ·±åº¦å­¦ä¹ çš„åå¾®åˆ†æ–¹ç¨‹æ±‚è§£},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=7ivGe_dCHDr4WFzABybh_xCaYeC9py2Xx5JGX1unuA5Ls4nRhM3qan9sF-Q4k3Yds8p5xozSlYudCTjhBJKel8OWTOyglLrwFQivGQJ-iWlsmLpgK5UHxk7PcArPkZxbdcLBcXAwE82Yt1UrG4zzBuTY_DjpPzVDI_VCblGnp1VJD-89rLY3XnHF37hjwwdm&uniplatform=NZKPT&language=CHS},
	abstract = {ç”±äºäººå·¥æ™ºèƒ½å’Œæ·±åº¦å­¦ä¹ çš„å¿«é€Ÿè¿›å±•,å…¶åº”ç”¨ä¹Ÿä¸å†ä»…é™äºå•ä¸€çš„è§†è§‰ã€å£°éŸ³å’Œå›¾åƒä»»åŠ¡ã€‚å¦‚ä»Š,è®¡ç®—æœºåœ¨ç§‘å­¦æŠ€æœ¯é¢†åŸŸä¸­çš„åº”ç”¨ä¹Ÿè¶Šæ¥è¶Šçªæ˜¾,å…¶ä¸­,äººå·¥æ™ºèƒ½åœ¨è‡ªç„¶ç§‘å­¦é¢†åŸŸçš„è¿ç”¨å·²ç»å½¢æˆäº†ä¸€ç§æ–°çš„å‘å±•æ–¹å‘ã€‚å°¤å…¶æ˜¯åœ¨æ±‚è§£åå¾®åˆ†æ–¹ç¨‹çš„é—®é¢˜ä¸­,äººå·¥æ™ºèƒ½çš„è¿ç”¨å€å—é‡è§†,å®ƒä¹Ÿæœ‰ç€æ›´å¹¿é˜”çš„ä½¿ç”¨é¢†åŸŸ,æ¶‰åŠè€Œä¸ä»…é™äºæµä½“åŠ›å­¦å’Œçƒ­åŠ›å­¦çš„ç ”ç©¶ç­‰é¢†åŸŸã€‚ åœ¨åº”ç”¨ä¼ ç»Ÿæ•°å­¦æŠ€å·§è§£å†³åå¾®åˆ†æ–¹ç¨‹æ—¶,é€šå¸¸éœ€é€šè¿‡ç½‘æ ¼åˆ’åˆ†å¹¶è¿›è¡Œè¿­ä»£è®¡ç®—,è¿™ä¸ä»…è®¡ç®—æˆæœ¬é«˜æ˜‚,å­˜å‚¨æˆæœ¬ä¹Ÿä¸è²,ç‰¹åˆ«æ˜¯å¯¹äºä¸è§„åˆ™å½¢çŠ¶çš„åŒºåŸŸ,ä¸ºäº†è·å¾—ç²¾ç¡®è§£,å¾€å¾€éœ€è¦ä»˜å‡ºæ›´å¤§çš„ä»£ä»·ã€‚è€Œä¸”ä¼ ç»Ÿæ•°å­¦æŠ€å·§åœ¨é¢å¯¹åŒºåŸŸå½¢æ€è½¬å˜æ—¶,å¾€å¾€éœ€è¦é’ˆå¯¹æ–°è¾¹ç•Œæ¡ä»¶é‡æ–°æ‰§è¡Œè¿­ä»£è®¡ç®—ã€‚é’ˆå¯¹è¿™ä¸€éš¾é¢˜,äººå·¥æ™ºèƒ½é¢†åŸŸé‡‡çº³äº†ç®—å­ç¥ç»ç½‘ç»œçš„æ–°ç†å¿µã€‚è¯¥æŠ€æœ¯èƒ½å¤ŸæŒæ¡è¾¹ç•Œå½¢æ€ä¸è§£å‡½æ•°ä¹‹é—´çš„æ˜ å°„å…³ç³»,æœ‰æ•ˆé¿å…äº†å¯¹åŠ¨æ€å˜åŒ–åŒºåŸŸè¿›è¡Œåå¤çš„æ±‚è§£è¿‡ç¨‹ã€‚å°½ç®¡å¦‚æ­¤,å®é™…è§£å†³é—®é¢˜çš„è¿‡ç¨‹ä¸­,ä»ç„¶å­˜åœ¨ç€ä¸€äº›æŒ‘æˆ˜,æ¯”å¦‚æ¯æ¬¡è®­ç»ƒåªé’ˆå¯¹ç‰¹å®šæ¡ä»¶ä¸‹çš„æ–¹ç¨‹æ±‚è§£ã€å¤šå°ºåº¦ä¸‹ç²¾åº¦ä¸‹é™ä»¥åŠæ¨ç†é€Ÿåº¦ç¼“æ…¢ç­‰é—®é¢˜ã€‚ æœ¬ç ”ç©¶é€šè¿‡é‡‡ç”¨æ·±åº¦ç®—å­ç¥ç»ç½‘ç»œæ¥åº”å¯¹å‰è¿°æŒ‘æˆ˜,å¹¶è®¾è®¡äº†é€šç”¨çš„è¾“å…¥åè®®å¯¹ä¸åŒç±»å‹çš„åå¾®åˆ†æ–¹ç¨‹ç¼–ç ,ä½¿å¾—ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒæ—¶ä¸å†éœ€è¦ä¸ºæ¯ä¸€ç§ä¸åŒç±»å‹çš„åå¾®åˆ†æ–¹ç¨‹è®¾è®¡æ¶æ„ã€‚åŒæ—¶,å‘ç°åœ¨å…·ä½“é—®é¢˜å»ºæ¨¡ä¸­,ç”±äºä¸åŒçš„åå¾®åˆ†æ–¹ç¨‹å…³æ³¨çš„é¢†åŸŸä¸åŒ,å¦‚æœå¯¹äºæ‰€æœ‰çš„æ–¹ç¨‹éƒ½åº”ç”¨åŒä¸€ç§æ¶æ„,ä¾¿å¯èƒ½åªåœ¨æŸç§é¢†åŸŸçš„æ–¹ç¨‹ä¸­æ•ˆæœè¾ƒå¥½,æœ¬æ–‡è®¾è®¡äº†ä¸€ç§é—¨æ§ç½‘ç»œæœºåˆ¶,å¯ä»¥æ ¹æ®ä¸åŒçš„åå¾®åˆ†æ–¹ç¨‹é¢†åŸŸé€‰æ‹©ä¸åŒçš„å‰é¦ˆç½‘ç»œå±‚,ä»¥ä½¿ä¸åŒé¢†åŸŸçš„æ–¹ç¨‹è·å¾—æ›´å¥½çš„é¢„æµ‹æ•ˆæœã€‚å…¶æ¬¡,ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æå–ç½‘æ ¼ç‚¹å’Œè¾“å…¥å‡½æ•°çš„ç›¸å…³ä¿¡æ¯,æ ¹æ®ä¸åŒçš„è¾“å…¥å‡½æ•°,è¿ç”¨å¤šå¤´æ³¨æ„åŠ›æå–ä¸åŒçš„ä¿¡æ¯ã€‚ç„¶å,å¯¹äºç‰¹å¾æå–æ–¹é¢,å¼•å…¥äº†ç»å…¸çš„å‚…é‡Œå¶ç®—å­å¤„ç†ç‰¹å¾ä¿¡æ¯,å°†ç‰¹å¾è½¬æ¢åˆ°é¢‘åŸŸè¿›è¡Œå¤„ç†,ä½¿å¾—æ•°æ®å¤„ç†çš„é€Ÿåº¦ä¸è¾“å…¥çš„ç»´åº¦æ— å…³,å¹¶ä¸”å¯¹äºç‰¹å¾ä¿¡æ¯çš„æå–æ›´åŠ å‡†ç¡®ã€‚æœ€å,æœ¬æ–‡é’ˆå¯¹æ‹¬æµä½“ã€å¼¹æ€§åŠ›å­¦ã€ç”µç£å­¦ã€çƒ­ä¼ å¯¼ç­‰å¤šä¸ªé¢†åŸŸçš„æ•°æ®é›†è¿›è¡Œäº†å®éªŒ,å¹¶æ¨ªå‘å¯¹æ¯”äº†ä¸åŒçš„æ·±åº¦ç®—å­ç¥ç»ç½‘ç»œã€‚ä¸ºè¯æ˜æœ¬ç ”ç©¶æ‰€ææ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå„æ„æˆæ¨¡å—çš„é‡è¦æ€§,æˆ‘ä»¬å¯¹æ¨¡å‹å¼€å±•äº†è§„æ¨¡å®éªŒå’Œæ¶ˆèå®éªŒã€‚},
	language = {zh},
	school = {æµ™æ±Ÿå¤§å­¦},
	author = {é™ˆ, ç…œè°¦},
	collaborator = {èµ–, ä¿Š},
	year = {2024},
	keywords = {DeepONet, FNO, PINN, ccfInfo: Not Found, citationNumber: 0, åå¾®åˆ†æ–¹ç¨‹, æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ},
}

@article{__2024-1,
	title = {æ±‚è§£åå¾®åˆ†æ–¹ç¨‹çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ¦‚è¿°},
	issn = {2096-5036},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=7ivGe_dCHDrTgwhlEOqif3D4I1wyl32ESOaBhEcZYOZ8kc0H7bpxRYwEloiBktWjP7of26ZKOVsfM_MwfjAimQu7M3FIc3vjbP3vmMs7mM6Gemqu8z38fTSjxD4TpzRlr-JeelZFoTIAyjzuP9vKoPcUBAqrtadS_3DqlkVpB2qBRyMV-Q-D730ILicTn7HT&uniplatform=NZKPT&language=CHS},
	doi = {10.16453/j.2096-5036.202445},
	abstract = {æ•°å€¼æ±‚è§£åå¾®åˆ†æ–¹ç¨‹æ˜¯å¤šå­¦ç§‘è®¡ç®—é¢†åŸŸå…±åŒå…³æ³¨çš„ä¸€é¡¹åŸºç¡€ç§‘å­¦è¯¾é¢˜ã€‚è¿‘å¹´æ¥,ç¥ç»ç½‘ç»œçš„æ·±åº¦å­¦ä¹ æ–¹æ³•å¾—åˆ°äº†å¤§åŠ›çš„å‘å±•,è¿™ä¸€æ–¹æ³•ä¹Ÿè¢«å¼•å…¥æ±‚è§£åå¾®åˆ†æ–¹ç¨‹è¿™ä¸€é¢†åŸŸ,é€æ¸æˆä¸ºäº†ç§‘å­¦è®¡ç®—é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹ä¹‹ä¸€ã€‚ä¸ºæ­¤,æœ¬æ–‡ä»‹ç»äº†è¿‘äº›å¹´æ±‚è§£åå¾®åˆ†æ–¹ç¨‹çš„ä¸€äº›æ·±åº¦å­¦ä¹ æ–¹æ³•,å°†è¿™äº›æ·±åº¦å­¦ä¹ æ–¹æ³•ä»æ•°å­¦è§’åº¦åˆ†ä¸ºå‡½æ•°å­¦ä¹ ç±»æ–¹æ³•å’Œç®—å­ç±»å­¦ä¹ æ–¹æ³•ã€‚æœ¬æ–‡å°†å¯¹è¿™ä¸¤ç±»æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç ”ç©¶ç°çŠ¶è¿›è¡Œæ¢³ç†ä»¥åŠæ€»ç»“,åŒæ—¶è¿˜ç®€è¦ä»‹ç»äº†ç›®å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ±‚è§£åå¾®åˆ†æ–¹ç¨‹ä¸­é‡åˆ°çš„ä¸€äº›å›°éš¾å’ŒæŒ‘æˆ˜,å¹¶å¯¹å¯èƒ½çš„ç ”ç©¶è¶‹åŠ¿å’Œæ–¹å‘è¿›è¡Œäº†é¢„æµ‹ã€‚},
	language = {zh},
	number = {5},
	journal = {äººå·¥æ™ºèƒ½},
	author = {é‚¹, é’æ¾ and éŸ¦, å¹³ and é™ˆ, è¾½ and å²‘, é‰´ç„• and è°­, å‡¯è€€},
	year = {2024},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found, åå¾®åˆ†æ–¹ç¨‹, å‡½æ•°ç±»å­¦ä¹ æ–¹æ³•, æ·±åº¦å­¦ä¹ æ–¹æ³•, ç®—å­ç±»å­¦ä¹ æ–¹æ³•},
	pages = {17--34},
}

@article{raissi_deep_nodate,
	title = {Deep {Hidden} {Physics} {Models}: {Deep} {Learning} of {Nonlinear} {Partial} {Diï¬€erential} {Equations}},
	abstract = {We put forth a deep learning approach for discovering nonlinear partial diï¬€erential equations from scattered and potentially noisy observations in space and time. Speciï¬cally, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The ï¬rst network acts as a prior on the unknown solution and essentially enables us to avoid numerical diï¬€erentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the eï¬€ectiveness of our approach for several benchmark problems spanning a number of scientiï¬c domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgersâ€™, Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear SchrÂ¨odinger, and Navier-Stokes equations.},
	language = {en},
	author = {Raissi, Maziar},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found, â›” No DOI found},
}

@article{majumdar_where_nodate,
	title = {Where are we in the search for an {Artificial} {Visual} {Cortex} for {Embodied} {Intelligence}?},
	abstract = {We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual â€˜foundation modelsâ€™ for Embodied AI. First, we curate CORTEXBENCH, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train differentsized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CORTEXBENCH. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.},
	language = {en},
	author = {Majumdar, Arjun and Yadav, Karmesh and Arnaud, Sergio and Ma, Yecheng Jason and Chen, Claire and Silwal, Sneha and Jain, Aryan and Berges, Vincent-Pierre and Wu, Tingfan and Vakil, Jay and Abbeel, Pieter and Malik, Jitendra and Batra, Dhruv and Lin, Yixin and Maksymets, Oleksandr and Rajeswaran, Aravind and Meier, Franziska},
	keywords = {ccfInfo: CCF-A NeurIPS, citationNumber: 45, notion},
}

@inproceedings{long_pde-net_2018,
	title = {{PDE}-{Net}: {Learning} {PDEs} from {Data}},
	shorttitle = {{PDE}-{Net}},
	url = {https://proceedings.mlr.press/v80/long18a.html},
	abstract = {Partial differential equations (PDEs) play a prominent role in many disciplines of science and engineering. PDEs are commonly derived based on empirical observations. However, with the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. Comparing with existing approaches, our approach has the most flexibility by learning both differential operators and the nonlinear response function of the underlying PDE model. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.
  åå¾®åˆ†æ–¹ç¨‹ ï¼ˆPDEï¼‰ åœ¨ç§‘å­¦å’Œå·¥ç¨‹çš„è®¸å¤šå­¦ç§‘ä¸­éƒ½å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚åå¾®åˆ†æ–¹ç¨‹é€šå¸¸æ˜¯æ ¹æ®å®è¯è§‚å¯Ÿå¾—å‡ºçš„ã€‚ç„¶è€Œï¼Œéšç€è¿‡å»åå¹´ä¼ æ„Ÿå™¨ã€è®¡ç®—èƒ½åŠ›å’Œæ•°æ®å­˜å‚¨çš„å¿«é€Ÿå‘å±•ï¼Œæµ·é‡æ•°æ®å¯ä»¥è½»æ¾æ”¶é›†å’Œé«˜æ•ˆå­˜å‚¨ã€‚å¦‚æ­¤å¤§é‡çš„æ•°æ®ä¸ºæ•°æ®é©±åŠ¨çš„ç‰©ç†å®šå¾‹å‘ç°æä¾›äº†æ–°çš„æœºä¼šã€‚å—æ·±åº¦å­¦ä¹ ä¸­ç¥ç»ç½‘ç»œè®¾è®¡æœ€æ–°å‘å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å‰é¦ˆæ·±åº¦ç½‘ç»œï¼Œç§°ä¸º PDE-Netï¼Œä»¥åŒæ—¶å®ç°ä¸¤ä¸ªç›®æ ‡ï¼šå‡†ç¡®é¢„æµ‹å¤æ‚ç³»ç»Ÿçš„åŠ¨åŠ›å­¦å¹¶æ­ç¤ºæ½œåœ¨çš„éšè— PDE æ¨¡å‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å­¦ä¹ å¾®åˆ†ç®—å­å’Œåº•å±‚åå¾®åˆ†æ–¹ç¨‹æ¨¡å‹çš„éçº¿æ€§å“åº”å‡½æ•°ï¼Œå…·æœ‰æœ€å¤§çš„çµæ´»æ€§ã€‚æ‰€æå‡ºçš„ PDE-Net çš„ä¸€ä¸ªç‰¹æ®ŠåŠŸèƒ½æ˜¯æ‰€æœ‰æ»¤æ³¢å™¨éƒ½å—åˆ°é€‚å½“çº¦æŸï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾è¯†åˆ«æ§åˆ¶ PDE æ¨¡å‹ï¼ŒåŒæ—¶ä»ç„¶ä¿æŒç½‘ç»œçš„è¡¨è¾¾å’Œé¢„æµ‹èƒ½åŠ›ã€‚è¿™äº›çº¦æŸæ˜¯é€šè¿‡å……åˆ†åˆ©ç”¨å¾®åˆ†ç®—å­çš„é˜¶æ•°å’Œæ»¤æ³¢å™¨çš„å’Œè§„åˆ™çš„é˜¶æ•°ä¹‹é—´çš„å…³ç³»ï¼ˆä¸€ä¸ªæºè‡ªå°æ³¢ç†è®ºçš„é‡è¦æ¦‚å¿µï¼‰æ¥ç²¾å¿ƒè®¾è®¡çš„ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒPDE-Net æœ‰å¯èƒ½æ­ç¤ºè§‚å¯Ÿåˆ°çš„åŠ¨åŠ›å­¦çš„éšè— PDEï¼Œå¹¶åœ¨ç›¸å¯¹è¾ƒé•¿çš„æ—¶é—´å†…é¢„æµ‹åŠ¨åŠ›å­¦è¡Œä¸ºï¼Œå³ä½¿åœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚},
	language = {en},
	urldate = {2024-12-27},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Long, Zichao and Lu, Yiping and Ma, Xianzhong and Dong, Bin},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {ccfInfo: CCF-A ICML, citationNumber: 785},
	pages = {3208--3216},
}

@article{lu_deepxde_2021,
	title = {{DeepXDE}: {A} {Deep} {Learning} {Library} for {Solving} {Differential} {Equations}},
	volume = {63},
	issn = {0036-1445, 1095-7200},
	shorttitle = {{DeepXDE}},
	url = {https://epubs.siam.org/doi/10.1137/19M1274067},
	doi = {10.1137/19M1274067},
	abstract = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from an implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residualbased adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an educational tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the userfriendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging scientific machine learning field.},
	language = {en},
	number = {1},
	urldate = {2024-12-29},
	journal = {SIAM Review},
	author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
	month = jan,
	year = {2021},
	note = {TLDR: An overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation, and a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs.},
	keywords = {ccfInfo: CCF-None SIAMREV, citationNumber: 938},
	pages = {208--228},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {00219991},
	shorttitle = {Physics-informed neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	language = {en},
	urldate = {2024-12-29},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
	month = feb,
	year = {2019},
	keywords = {ccfInfo: CCF-None JCPHY, citationNumber: 11621},
	pages = {686--707},
}

@article{peng_pinn_2022,
	title = {{PINN} deep learning method for the {Chen}â€“{Lee}â€“{Liu} equation: {Rogue} wave on the periodic background},
	volume = {105},
	issn = {10075704},
	shorttitle = {{PINN} deep learning method for the {Chen}â€“{Lee}â€“{Liu} equation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1007570421003798},
	doi = {10.1016/j.cnsns.2021.106067},
	abstract = {We consider the exact rogue periodic wave (rogue wave on the periodic background) and periodic wave solutions for the Chenâ€“Leeâ€“Liu equation via the odd-th order Darboux transformation. Then, the multi-layer physics-informed neural networks (PINNs) deep learning method is applied to research the data-driven rogue periodic wave, breather wave, soliton wave and periodic wave solutions of well-known Chenâ€“Leeâ€“Liu equation. Especially, the data-driven rogue periodic wave is learned for the first time to solve the partial differential equation. In addition, using image simulation, the relevant dynamical behaviors and error analysis for there solutions are presented. The numerical results indicate that the rogue periodic wave, breather wave, soliton wave and periodic wave solutions for Chenâ€“Leeâ€“Liu equation can be generated well by PINNs deep learning method.},
	language = {en},
	urldate = {2024-12-29},
	journal = {Communications in Nonlinear Science and Numerical Simulation},
	author = {Peng, Wei-Qi and Pu, Jun-Cai and Chen, Yong},
	month = feb,
	year = {2022},
	note = {TLDR: Numerical results indicate that the rogue periodic wave, breather wave, soliton wave and periodic wave solutions for Chen-Lee-Liu equation can be generated well by PINNs deep learning method.},
	keywords = {ccfInfo: CCF-None CNSNS, citationNumber: 51},
	pages = {106067},
}

@article{noauthor_dgm_2018,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {0021-9991},
	shorttitle = {{DGM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with aâ€¦},
	language = {en-US},
	urldate = {2024-12-27},
	journal = {Journal of Computational Physics},
	month = dec,
	year = {2018},
	note = {Publisher: Academic Press
TLDR: A deep learning algorithm similar in spirit to Galerkin methods, using a deep neural network instead of linear combinations of basis functions is proposed, and is implemented for American options in up to 100 dimensions.},
	keywords = {ccfInfo: CCF-None JCPHY, citationNumber: 1427},
	pages = {1339--1364},
}
