\section{绪论}
\subsection{研究背景与意义}
\subsubsection{研究背景}

随着人工智能技术的快速发展，具身智能作为一个融合了感知、决策和执行的新兴领域，正逐渐从实验室迈向实际应用场景。具身智能旨在赋予智能体在真实环境中感知周围世界的能力，使其能够基于多模态信息作出合理决策，并采取相应的行动，实现类人的环境交互能力。

早期的具身智能研究主要集中在简单环境下的基础任务，如路径导航、物体抓取等[1]。这些研究通常依赖于特定的算法，缺乏通用性和灵活性。近年来，得益于深度学习和强化学习的重大突破，具身智能研究取得了突破性进展。现代具身智能系统已经能够执行更为复杂的多步骤长期任务，展现出更为广阔的应用前景[1]。

大型视觉语言模型（Large Vision-Language Models，LVLM）的兴起，如ViT[2]、CLIP[3]和PaLI[4]等模型，极大地推动了具身智能的技术发展。这类模型通过大规模的预训练，学习了视觉和语言的统一表示，获得了强大的多模态理解能力，能够同时处理图像和文本的输入，实现跨模态的信息融合和理解，提供了更为丰富的感知能力。在LVLM的基础上，RT-2[5]、Octo[6]、OpenVLA[7]以及TinyVLA[8]等VLA模型应运而生，进一步将动作执行能力整合到多模态框架中，能够同时处理视觉图像信息和自然语言指令，并直接作出预测动作，实现从感知到动作的端到端学习。VLA模型的强大能力大幅降低了具身智能系统的开发复杂度，提高了系统的通用性，扩展了具身智能的应用范围。

然而，尽管VLA模型为具身智能领域做出了巨大的贡献，但其复杂的模型架构可能引入了大量潜在的安全脆弱性[9]。由于具身智能系统通常与物理世界直接交互，其安全性不仅关系到信息安全，更直接影响到物理安全。因此，深入研究具身智能的安全脆弱性并提出有效的评估方法，对确保这类系统在实际应用中的安全可靠运行具有重要意义。

\subsubsection{研究意义}

近期研究表明，虽然VLA模型展现出令人印象深刻的能力，但是它们在面对对抗攻击时展现了明显的脆弱性[10]。对抗攻击是一种针对深度学习模型安全性的评估方法，通过向模型的输入注入精心设计的微小扰动，诱导模型产生非预期的输出，来评估模型的安全性[11]。这种攻击方法最初应用于图像分类模型，后来逐渐扩展到其他视觉任务和模型架构[12]。对于具身智能系统，通过向VLA模型的视觉输入中添加对抗扰动，攻击者可以有效干扰其动作预测过程，使具身智能系统执行错误的操作，甚至完全失效[10]。在机械臂操作、无人机控制或自动驾驶等安全关键场景中，这种脆弱性可能导致严重的安全事故[9]，对依赖VLA模型的具身智能系统构成了严重的安全威胁。

尽管现有的对抗攻击方法能够有效干扰VLA模型，但它们普遍缺乏视觉上的隐蔽性[10]。这些攻击方法产生的扰动在视觉上往往十分明显，呈现出不自然的噪声模式或异常的色彩分布，非常容易被人类识别。考虑到具身智能系统通常部署在真实环境中，这种缺乏隐蔽性的对抗攻击很容易被人类发现并移除，从而失去攻击效果。这种局限性大大降低了其在实际应用场景中的威胁能力和实用价值，无法真实反映具身智能系统在现实环境中可能面临的安全挑战。

因此，本文提出了一种面向具身智能的创新隐蔽对抗补丁攻击算法。该算法采用基于图像补丁的对抗攻击，同时适用于数字和物理两种形式的部署[13]，并结合了定向位姿攻击和多层次扰动约束机制，实现了兼具攻击性和隐蔽性的对抗补丁生成，为未来构建更安全、更鲁棒的具身智能系统提供了新的思路。


\cite{huangResearchU2019}	%参考文献引用示例